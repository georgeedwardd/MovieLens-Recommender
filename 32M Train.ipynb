{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb7af0-54db-4c49-ba78-1d062ff96a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8f187-3a5e-4b47-a30d-644d698b16b5",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26003c36-6972-4197-8fba-c152e7a89081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./ratings.csv\")\n",
    "data = ratings.iloc[:, : -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1b3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract year from movie title ---\n",
    "def extract_year(title):\n",
    "    \"\"\"Extract 4-digit year from movie title in parentheses.\"\"\"\n",
    "    if m := re.search(r'\\((\\d{4})\\)', title):\n",
    "        return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Load movies data ---\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "# Extract year and compute decade\n",
    "movies['year'] = movies['title'].map(extract_year)\n",
    "movies['decade'] = ((movies['year'] // 10) * 10).astype('Int64')\n",
    "movies['decade'] = movies['decade'].astype('string') + \"s\"\n",
    "movies.loc[movies['year'] < 1950, 'decade'] = 'pre 1950s'\n",
    "\n",
    "# Map movie IDs to titles\n",
    "movieid_to_title = dict(zip(movies['movieId'], movies['title']))\n",
    "\n",
    "# Clean genres\n",
    "movies['genres'] = movies['genres'].str.split('|').apply(\n",
    "    lambda lst: [g for g in lst if g != '(no genres listed)']\n",
    ")\n",
    "\n",
    "# Ensure decade is string and handle missing values\n",
    "movies['decade'] = movies['decade'].astype('string')\n",
    "movies['decade'] = movies['decade'].where(movies['decade'].notna(), None)\n",
    "\n",
    "# Select relevant columns\n",
    "movies = movies[['movieId', 'title', 'genres', 'decade']]\n",
    "\n",
    "# Fill missing decade for encoding\n",
    "movies['decade_filled'] = movies['decade'].fillna('SKIP_THIS')\n",
    "\n",
    "\n",
    "# --- Encode features ---\n",
    "# Multi-hot encode genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_features = mlb.fit_transform(movies['genres'])\n",
    "\n",
    "# One-hot encode decades\n",
    "valid_decades = sorted([d for d in movies['decade_filled'].dropna().unique() if d != 'SKIP_THIS'])\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', categories=[valid_decades])\n",
    "decade_features = ohe.fit_transform(movies[['decade_filled']])\n",
    "\n",
    "# Combine genre and decade features\n",
    "item_features = np.hstack([genre_features, decade_features])\n",
    "\n",
    "# Feature names\n",
    "decade_feature_names = [f\"decade_{d}\" for d in valid_decades]\n",
    "genre_feature_names = [f\"genre_{g}\" for g in mlb.classes_]\n",
    "feature_names = genre_feature_names + decade_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86926135-085d-4e40-a4ac-4b8a713462e6",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1463788c-eaa6-422d-9bc6-a036d45d6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 200948\n",
      "Number of items: 84432\n",
      "Number of ratings: 32000204\n",
      "Number of features: 28\n"
     ]
    }
   ],
   "source": [
    "# --- Users ---\n",
    "max_users = np.max(data.iloc[:, 0])\n",
    "unique_users_ordered = pd.unique(data.iloc[:, 0])[:max_users]\n",
    "\n",
    "# Filter data to only include these users\n",
    "filtered_data = data[data.iloc[:, 0].isin(unique_users_ordered)].copy()\n",
    "\n",
    "# Remap user IDs to 0-based indexing\n",
    "user_id_map = {old: new for new, old in enumerate(unique_users_ordered)}\n",
    "filtered_data.iloc[:, 0] = filtered_data.iloc[:, 0].map(user_id_map)\n",
    "\n",
    "# --- Items ---\n",
    "unique_items = filtered_data.iloc[:, 1].unique()\n",
    "item_id_map = {old: new for new, old in enumerate(unique_items)}\n",
    "\n",
    "# Filter movies to include only these items and map new item indices\n",
    "movies = movies[movies['movieId'].isin(item_id_map)].copy()\n",
    "movies['item_idx'] = movies['movieId'].map(item_id_map)\n",
    "\n",
    "# Reorder item features according to new item indices\n",
    "item_features = item_features[movies['item_idx'].argsort()]\n",
    "\n",
    "# Remap item IDs in filtered data to 0-based indexing\n",
    "filtered_data.iloc[:, 1] = filtered_data.iloc[:, 1].map(item_id_map)\n",
    "\n",
    "# --- Shuffle data ---\n",
    "filtered_data = filtered_data.sample(frac=1, random_state=99).reset_index(drop=True)\n",
    "\n",
    "# --- Summary statistics ---\n",
    "n_users = filtered_data.iloc[:, 0].nunique()\n",
    "n_items = filtered_data.iloc[:, 1].nunique()\n",
    "n_ratings = len(filtered_data)\n",
    "n_features = item_features.shape[1]\n",
    "\n",
    "print(f'Number of users: {n_users}')\n",
    "print(f'Number of items: {n_items}')\n",
    "print(f'Number of ratings: {n_ratings}')\n",
    "print(f'Number of features: {n_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978a748f-ac80-4aaf-a13d-49c26d2eebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000204/32000204 [01:15<00:00, 421961.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Create sparse matrices ---\n",
    "data_by_user_train = lil_matrix((n_users, n_items))\n",
    "data_by_user_test = lil_matrix((n_users, n_items))\n",
    "data_by_movie_train = lil_matrix((n_items, n_users))\n",
    "data_by_movie_test = lil_matrix((n_items, n_users))\n",
    "\n",
    "# --- Initialize user and item rating lists ---\n",
    "user_ratings = [[] for _ in range(n_users)]\n",
    "item_ratings = [[] for _ in range(n_items)]\n",
    "\n",
    "# Populate user and item ratings\n",
    "for user, item, rating in tqdm(filtered_data.iloc[:n_ratings, :].values):\n",
    "    user_ratings[int(user)].append((int(item), float(rating)))\n",
    "    item_ratings[int(item)].append((int(user), float(rating)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9c1bb4-c7a8-42f8-8def-5c74e908c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting train/test: 100%|██████████| 200948/200948 [03:30<00:00, 955.22it/s] \n"
     ]
    }
   ],
   "source": [
    "# --- Split data into train and test sets ---\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for user in tqdm(range(n_users), desc=\"Splitting train/test\"):\n",
    "    ratings = user_ratings[user]\n",
    "\n",
    "    if len(ratings) == 0:\n",
    "        continue\n",
    "    elif len(ratings) == 1:\n",
    "        # If only one rating, assign to train\n",
    "        train_data.append((user, ratings[0][0], ratings[0][1]))\n",
    "        continue\n",
    "\n",
    "    # Split 20% of user's ratings as test\n",
    "    train_split, test_split = train_test_split(\n",
    "        ratings,\n",
    "        test_size=0.2,\n",
    "        random_state=99\n",
    "    )\n",
    "\n",
    "    # Append train ratings\n",
    "    for item, rating in train_split:\n",
    "        train_data.append((user, item, rating))\n",
    "\n",
    "    # Append test ratings\n",
    "    for item, rating in test_split:\n",
    "        test_data.append((user, item, rating))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00524f7-dffa-4361-a9a7-a6635dacf72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# Re-initialise for train\n",
    "user_ratings = [[] for _ in range(n_users)]\n",
    "item_ratings = [[] for _ in range(n_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e23848-2b8e-4da0-b5fb-bee4f733428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling train matrices: 100%|██████████| 25520897/25520897 [01:20<00:00, 318265.20it/s]\n",
      "Filling test matrices: 100%|██████████| 6479307/6479307 [00:14<00:00, 446463.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Populate user and item ratings for training data ---\n",
    "for user, item, rating in tqdm(train_data, desc=\"Filling train matrices\"):\n",
    "    data_by_user_train[int(user), int(item)] = rating\n",
    "    user_ratings[int(user)].append((int(item), float(rating)))\n",
    "    item_ratings[int(item)].append((int(user), float(rating)))\n",
    "\n",
    "# --- Populate user-item ratings for test data ---\n",
    "for user, item, rating in tqdm(test_data, desc=\"Filling test matrices\"):\n",
    "    data_by_user_test[int(user), int(item)] = rating\n",
    "\n",
    "# --- Convert to sparse CSR format ---\n",
    "data_by_user_train = data_by_user_train.tocsr()\n",
    "data_by_movie_train = data_by_user_train.transpose().tocsr()\n",
    "data_by_user_test = data_by_user_test.tocsr()\n",
    "data_by_movie_test = data_by_user_test.transpose().tocsr()\n",
    "\n",
    "# --- COO format for convenience ---\n",
    "train_coo = data_by_user_train.tocoo()\n",
    "test_coo = data_by_user_test.tocoo()\n",
    "\n",
    "# --- Compute global mean for training data ---\n",
    "mu_train = np.mean([r for _, _, r in train_data])\n",
    "\n",
    "# --- Initialize biases ---\n",
    "user_bias = np.zeros(n_users)\n",
    "item_bias = np.zeros(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a55a0b-4fe1-4164-859f-956f5f7020f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- User Updates --------------------\n",
    "def update_user_bias(user, mu, data_by_user_train, lam, gamma, tau,\n",
    "                     item_bias, user_bias, item_embedding, user_embedding):\n",
    "    \"\"\"Update user bias given training data.\"\"\"\n",
    "    start_ptr = data_by_user_train.indptr[user]\n",
    "    end_ptr = data_by_user_train.indptr[user + 1]\n",
    "    items = data_by_user_train.indices[start_ptr:end_ptr]\n",
    "    ratings = data_by_user_train.data[start_ptr:end_ptr]\n",
    "\n",
    "    if len(ratings) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    users = np.full_like(items, user)\n",
    "    uv = np.sum(user_embedding[users] * item_embedding[items], axis=1)\n",
    "    bias = np.sum(ratings - uv - item_bias[items] - mu)\n",
    "    return lam * bias / (lam * len(ratings) + gamma)\n",
    "\n",
    "\n",
    "def update_user_embedding(user, mu, data_by_user_train, lam, gamma, tau,\n",
    "                          item_bias, user_bias, item_embedding, user_embedding):\n",
    "    \"\"\"Update user embedding vector.\"\"\"\n",
    "    start_ptr = data_by_user_train.indptr[user]\n",
    "    end_ptr = data_by_user_train.indptr[user + 1]\n",
    "    items = data_by_user_train.indices[start_ptr:end_ptr]\n",
    "    ratings = data_by_user_train.data[start_ptr:end_ptr]\n",
    "\n",
    "    k = item_embedding.shape[1]\n",
    "    if len(ratings) == 0:\n",
    "        return np.zeros(k)\n",
    "\n",
    "    users = np.full_like(items, user)\n",
    "    sum1 = item_embedding[items].T @ item_embedding[items]\n",
    "    sum2 = item_embedding[items].T @ (ratings - user_bias[users] - item_bias[items] - mu)\n",
    "    p1 = lam * sum1 + tau * np.eye(k)\n",
    "    p2 = lam * sum2\n",
    "    return np.linalg.solve(p1, p2)\n",
    "\n",
    "\n",
    "# -------------------- Item Updates --------------------\n",
    "def update_item_bias(item, mu, data_by_movie_train, lam, gamma, tau,\n",
    "                     item_bias, user_bias, item_embedding, user_embedding):\n",
    "    \"\"\"Update item bias given training data.\"\"\"\n",
    "    start_ptr = data_by_movie_train.indptr[item]\n",
    "    end_ptr = data_by_movie_train.indptr[item + 1]\n",
    "    users = data_by_movie_train.indices[start_ptr:end_ptr]\n",
    "    ratings = data_by_movie_train.data[start_ptr:end_ptr]\n",
    "\n",
    "    if len(ratings) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    items = np.full_like(users, item)\n",
    "    uv = np.sum(user_embedding[users] * item_embedding[items], axis=1)\n",
    "    bias = np.sum(ratings - uv - user_bias[users] - mu)\n",
    "    return lam * bias / (lam * len(ratings) + gamma)\n",
    "\n",
    "\n",
    "def update_item_embedding(item, mu, data_by_movie_train, lam, gamma, tau,\n",
    "                          item_bias, user_bias, item_embedding, user_embedding,\n",
    "                          item_features, feature_embedding):\n",
    "    \"\"\"Update item embedding vector including feature contribution.\"\"\"\n",
    "    start_ptr = data_by_movie_train.indptr[item]\n",
    "    end_ptr = data_by_movie_train.indptr[item + 1]\n",
    "    users = data_by_movie_train.indices[start_ptr:end_ptr]\n",
    "    ratings = data_by_movie_train.data[start_ptr:end_ptr]\n",
    "\n",
    "    k = user_embedding.shape[1]\n",
    "    if len(ratings) == 0:\n",
    "        return np.zeros(k)\n",
    "\n",
    "    items = np.full_like(users, item)\n",
    "    sum1 = user_embedding[users].T @ user_embedding[users]\n",
    "    sum2 = user_embedding[users].T @ (ratings - item_bias[items] - user_bias[users] - mu)\n",
    "    p1 = lam * sum1 + tau * np.eye(k)\n",
    "    p2 = lam * sum2\n",
    "\n",
    "    # Feature contribution\n",
    "    feats = item_features[item]\n",
    "    F_n = np.sum(feats)\n",
    "    s_n = np.zeros(k) if F_n == 0 else (feats @ feature_embedding) / np.sqrt(F_n)\n",
    "\n",
    "    p2 = lam * sum2 + tau * s_n\n",
    "    return np.linalg.solve(p1, p2)\n",
    "\n",
    "\n",
    "# -------------------- Feature Updates --------------------\n",
    "def update_feature_embedding(item_features, item_embedding, tau, lam):\n",
    "    \"\"\"Update feature embeddings based on item embeddings.\"\"\"\n",
    "    n_items, n_features = item_features.shape\n",
    "    k = item_embedding.shape[1]\n",
    "\n",
    "    # Compute scaling for items with multiple features\n",
    "    F_n = item_features.sum(axis=1, keepdims=True)\n",
    "    scaling = np.where(F_n > 0, 1.0 / np.sqrt(F_n), 0.0)\n",
    "\n",
    "    # Weighted features\n",
    "    weighted_features = item_features * scaling\n",
    "\n",
    "    # Linear system: A F = B\n",
    "    A = lam * (weighted_features.T @ weighted_features) + tau * np.eye(n_features)\n",
    "    B = lam * (weighted_features.T @ item_embedding)\n",
    "\n",
    "    # Solve for feature embedding\n",
    "    feature_embedding = np.linalg.solve(A, B)\n",
    "    return feature_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a096ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-01 22:51:05,471] A new study created in memory with name: no-name-62d20800-6066-4164-bd0b-cef2a1bb5cde\n",
      "C:\\Users\\gmedw\\AppData\\Local\\Temp\\ipykernel_17620\\703367911.py:83: RuntimeWarning: divide by zero encountered in divide\n",
      "  scaling = np.where(F_n > 0, 1.0/np.sqrt(F_n), 0.0)\n",
      "[I 2025-10-01 22:53:43,365] Trial 0 finished with value: 0.7904330962114431 and parameters: {'lam': 0.023482306305590867, 'gamma': 0.07933592532767576, 'tau': 0.28404768177419687}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 22:55:55,334] Trial 1 finished with value: 0.804548941087428 and parameters: {'lam': 0.04249334080448625, 'gamma': 0.15401749178226593, 'tau': 0.05116451213395118}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 22:58:23,098] Trial 2 finished with value: 0.8010385258907208 and parameters: {'lam': 0.06276439946633211, 'gamma': 0.27623247449192906, 'tau': 0.12106358294296984}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:00:50,225] Trial 3 finished with value: 0.802900636600541 and parameters: {'lam': 0.09306676851497558, 'gamma': 0.2399631556638269, 'tau': 0.12163934963227784}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:03:19,091] Trial 4 finished with value: 0.7953776644251863 and parameters: {'lam': 0.05191725259487747, 'gamma': 0.26982651474394875, 'tau': 0.29566940437775974}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:05:44,098] Trial 5 finished with value: 0.7950342947964867 and parameters: {'lam': 0.06360531084713116, 'gamma': 0.19350691432884132, 'tau': 0.2463780120654678}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:08:06,302] Trial 6 finished with value: 0.7945269751606165 and parameters: {'lam': 0.03453508713102026, 'gamma': 0.017104692485574613, 'tau': 0.2522937360123565}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:10:15,725] Trial 7 finished with value: 0.7941176243161637 and parameters: {'lam': 0.03151220863840327, 'gamma': 0.12196491540179641, 'tau': 0.15518988003404485}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:12:42,033] Trial 8 finished with value: 0.8167324534294242 and parameters: {'lam': 0.08596356398873607, 'gamma': 0.08073297588962496, 'tau': 0.0357382734985385}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:15:09,697] Trial 9 finished with value: 0.8072443489330661 and parameters: {'lam': 0.01476911815527721, 'gamma': 0.07033440207927844, 'tau': 0.015390599245013452}. Best is trial 0 with value: 0.7904330962114431.\n",
      "[I 2025-10-01 23:17:37,895] Trial 10 finished with value: 0.789698005396268 and parameters: {'lam': 0.012856381116269135, 'gamma': 0.011006176372416132, 'tau': 0.19926860710780542}. Best is trial 10 with value: 0.789698005396268.\n",
      "[I 2025-10-01 23:20:03,155] Trial 11 finished with value: 0.7902105269949792 and parameters: {'lam': 0.010419094699580083, 'gamma': 0.02551874017243124, 'tau': 0.1936622837540527}. Best is trial 10 with value: 0.789698005396268.\n",
      "[I 2025-10-01 23:22:27,472] Trial 12 finished with value: 0.7903205857678007 and parameters: {'lam': 0.011947011370161086, 'gamma': 0.014170214037514978, 'tau': 0.19302330707028148}. Best is trial 10 with value: 0.789698005396268.\n",
      "[I 2025-10-01 23:24:50,980] Trial 13 finished with value: 0.7894546562510912 and parameters: {'lam': 0.010007044852275812, 'gamma': 0.054344153382144836, 'tau': 0.19454070317306127}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:27:14,498] Trial 14 finished with value: 0.7928934326526866 and parameters: {'lam': 0.02465558528576935, 'gamma': 0.04827344256180977, 'tau': 0.20114996742688615}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:29:35,829] Trial 15 finished with value: 0.7965955694108738 and parameters: {'lam': 0.04351393418910865, 'gamma': 0.11923380957142077, 'tau': 0.1441267141027528}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:31:53,434] Trial 16 finished with value: 0.7911464082980294 and parameters: {'lam': 0.023421863970933203, 'gamma': 0.049697202120778994, 'tau': 0.23283283005187885}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:34:13,161] Trial 17 finished with value: 0.8075584325186056 and parameters: {'lam': 0.07779161191410312, 'gamma': 0.11131904388305325, 'tau': 0.07975665248318142}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:36:34,807] Trial 18 finished with value: 0.7909492828877063 and parameters: {'lam': 0.019111821641868813, 'gamma': 0.049596765331896775, 'tau': 0.166430800047093}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:38:54,572] Trial 19 finished with value: 0.7956484490254756 and parameters: {'lam': 0.03478468301931997, 'gamma': 0.20669098477788506, 'tau': 0.21842368997547165}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:41:14,437] Trial 20 finished with value: 0.8003887594096102 and parameters: {'lam': 0.046866221028751154, 'gamma': 0.18070043567276228, 'tau': 0.08522321815725253}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:43:36,063] Trial 21 finished with value: 0.792160152901855 and parameters: {'lam': 0.011674506405251812, 'gamma': 0.015742297249009888, 'tau': 0.18586634419552855}. Best is trial 13 with value: 0.7894546562510912.\n",
      "[I 2025-10-01 23:45:57,031] Trial 22 finished with value: 0.7889689432267974 and parameters: {'lam': 0.010204938529903694, 'gamma': 0.04062028171136078, 'tau': 0.17830755435152928}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-01 23:48:21,065] Trial 23 finished with value: 0.7954588593222811 and parameters: {'lam': 0.029201511443799816, 'gamma': 0.09279270956842739, 'tau': 0.17092547466484057}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-01 23:50:45,010] Trial 24 finished with value: 0.7913063108216079 and parameters: {'lam': 0.019449701485580513, 'gamma': 0.03853574083812651, 'tau': 0.22005961234426147}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-01 23:53:02,522] Trial 25 finished with value: 0.792457692105506 and parameters: {'lam': 0.017971740710461453, 'gamma': 0.15074294655339912, 'tau': 0.26899656458089444}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-01 23:55:19,636] Trial 26 finished with value: 0.8000808168753473 and parameters: {'lam': 0.037376810242969946, 'gamma': 0.06063494920019724, 'tau': 0.13645610685631795}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-01 23:57:37,251] Trial 27 finished with value: 0.7927808129323091 and parameters: {'lam': 0.0271653973898675, 'gamma': 0.10065283191100427, 'tau': 0.20934961894259685}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-01 23:59:47,459] Trial 28 finished with value: 0.794047886234892 and parameters: {'lam': 0.017461631276174273, 'gamma': 0.033845672955137136, 'tau': 0.17655831139331724}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:02:05,164] Trial 29 finished with value: 0.7892940215620291 and parameters: {'lam': 0.02215659225011264, 'gamma': 0.06312426594572662, 'tau': 0.2637470458279584}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:04:29,079] Trial 30 finished with value: 0.7893582085083752 and parameters: {'lam': 0.02117018109705244, 'gamma': 0.13173649415578065, 'tau': 0.2727969888869238}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:06:53,722] Trial 31 finished with value: 0.7936338564584773 and parameters: {'lam': 0.023612013771245558, 'gamma': 0.08464027500123694, 'tau': 0.27183656298089387}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:09:19,210] Trial 32 finished with value: 0.7896455999777092 and parameters: {'lam': 0.021325945219691514, 'gamma': 0.13716836554050627, 'tau': 0.29872363451703693}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:11:43,137] Trial 33 finished with value: 0.7908043530496024 and parameters: {'lam': 0.039168137987767554, 'gamma': 0.0686548120592001, 'tau': 0.25096660413211486}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:14:07,519] Trial 34 finished with value: 0.7955837855983836 and parameters: {'lam': 0.059137445823322664, 'gamma': 0.0970733292337928, 'tau': 0.2756504824734911}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:16:14,133] Trial 35 finished with value: 0.7895746801231339 and parameters: {'lam': 0.01623971239455617, 'gamma': 0.1670082385652081, 'tau': 0.239380658765247}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:18:35,031] Trial 36 finished with value: 0.7971116404270105 and parameters: {'lam': 0.02982180978591385, 'gamma': 0.22183290618896845, 'tau': 0.10744993089895709}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:20:50,638] Trial 37 finished with value: 0.7938512811529606 and parameters: {'lam': 0.02580259364511539, 'gamma': 0.056888156731288775, 'tau': 0.26165298376644275}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:23:20,592] Trial 38 finished with value: 0.7959460996179788 and parameters: {'lam': 0.09977951524446341, 'gamma': 0.1350863589839765, 'tau': 0.28283843677206955}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:25:41,760] Trial 39 finished with value: 0.798830967834103 and parameters: {'lam': 0.07303177970160908, 'gamma': 0.28972417524975935, 'tau': 0.22609225102046548}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:28:05,255] Trial 40 finished with value: 0.7956170269006817 and parameters: {'lam': 0.0507229825318051, 'gamma': 0.0768295143556766, 'tau': 0.2517423483630523}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:30:27,316] Trial 41 finished with value: 0.7906728886471502 and parameters: {'lam': 0.015071376334278455, 'gamma': 0.17750972926408537, 'tau': 0.237711593835186}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:32:46,486] Trial 42 finished with value: 0.7911397142178486 and parameters: {'lam': 0.016324453409139975, 'gamma': 0.1763126108110051, 'tau': 0.28860357691867417}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:35:06,119] Trial 43 finished with value: 0.7907105387577801 and parameters: {'lam': 0.0100744370828373, 'gamma': 0.25413863307146645, 'tau': 0.24093930172727834}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:37:27,497] Trial 44 finished with value: 0.7935056320478104 and parameters: {'lam': 0.03154046094884421, 'gamma': 0.16260492263712611, 'tau': 0.2611435971741412}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:39:54,510] Trial 45 finished with value: 0.789924098709763 and parameters: {'lam': 0.015108673346862372, 'gamma': 0.0308300224628062, 'tau': 0.21312272386343983}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:42:14,858] Trial 46 finished with value: 0.7922565067861629 and parameters: {'lam': 0.020864543906644807, 'gamma': 0.1378678621588446, 'tau': 0.1826963275671117}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:44:36,888] Trial 47 finished with value: 0.7917373421587671 and parameters: {'lam': 0.01046804333284999, 'gamma': 0.2078594256996652, 'tau': 0.15594472842223495}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:47:04,189] Trial 48 finished with value: 0.7921129496994912 and parameters: {'lam': 0.021743349737722593, 'gamma': 0.16202267395919653, 'tau': 0.23302846846086897}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:49:25,186] Trial 49 finished with value: 0.7901530240972259 and parameters: {'lam': 0.014588777477408797, 'gamma': 0.11393777629291253, 'tau': 0.20341578456107057}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:51:47,067] Trial 50 finished with value: 0.789165791448482 and parameters: {'lam': 0.02637596025648093, 'gamma': 0.06486745850756567, 'tau': 0.28414938923618094}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:54:10,265] Trial 51 finished with value: 0.7916064271822796 and parameters: {'lam': 0.026567515778942712, 'gamma': 0.06581646084802517, 'tau': 0.28398216610742205}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:56:32,996] Trial 52 finished with value: 0.7931724648445572 and parameters: {'lam': 0.033192669938759314, 'gamma': 0.041182214103407576, 'tau': 0.2598557759208107}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 00:58:54,062] Trial 53 finished with value: 0.7890292667858494 and parameters: {'lam': 0.018772582917316805, 'gamma': 0.08095174478432618, 'tau': 0.2945840319740703}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:01:15,889] Trial 54 finished with value: 0.7897894455604499 and parameters: {'lam': 0.02378177032702395, 'gamma': 0.08568482532439478, 'tau': 0.2953820088103417}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:03:35,925] Trial 55 finished with value: 0.7904841949521993 and parameters: {'lam': 0.013323294225705008, 'gamma': 0.05332197472667614, 'tau': 0.2762574833517793}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:05:55,911] Trial 56 finished with value: 0.7930172975304114 and parameters: {'lam': 0.03930871742256229, 'gamma': 0.0251319696946838, 'tau': 0.286445601691101}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:08:21,017] Trial 57 finished with value: 0.795350352652737 and parameters: {'lam': 0.028268115467327652, 'gamma': 0.10397596298094144, 'tau': 0.13796974148357005}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:10:42,099] Trial 58 finished with value: 0.7905521596734718 and parameters: {'lam': 0.019400307906690048, 'gamma': 0.0750628123727357, 'tau': 0.26673382617893404}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:13:01,828] Trial 59 finished with value: 0.7950643260146396 and parameters: {'lam': 0.06766367557599653, 'gamma': 0.0447090940800631, 'tau': 0.2986845590315277}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:15:20,847] Trial 60 finished with value: 0.8058592266535667 and parameters: {'lam': 0.08446663940107843, 'gamma': 0.02388045378910414, 'tau': 0.11437545693045437}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:17:40,106] Trial 61 finished with value: 0.7898251037187816 and parameters: {'lam': 0.017320449425838044, 'gamma': 0.06129628384829377, 'tau': 0.24582424560929034}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:20:01,487] Trial 62 finished with value: 0.7907328650726599 and parameters: {'lam': 0.013011561412766123, 'gamma': 0.1268893329637319, 'tau': 0.2575177658144212}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:22:19,769] Trial 63 finished with value: 0.7903888136206852 and parameters: {'lam': 0.022163840806538434, 'gamma': 0.14959585424402339, 'tau': 0.2249426706825779}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:24:36,018] Trial 64 finished with value: 0.7895697029535913 and parameters: {'lam': 0.01808105500520709, 'gamma': 0.08793507038700858, 'tau': 0.1929141520327186}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:26:53,515] Trial 65 finished with value: 0.7933914211007217 and parameters: {'lam': 0.018499622796210115, 'gamma': 0.08920834438275137, 'tau': 0.16381488385486498}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:29:12,397] Trial 66 finished with value: 0.7912740883738636 and parameters: {'lam': 0.024737290027365435, 'gamma': 0.10501376342530544, 'tau': 0.19117453161211695}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:31:33,545] Trial 67 finished with value: 0.7941563477448327 and parameters: {'lam': 0.035277200343017186, 'gamma': 0.07074031343036251, 'tau': 0.15212975477882854}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:33:53,824] Trial 68 finished with value: 0.7912704301372904 and parameters: {'lam': 0.012742180329060857, 'gamma': 0.07841325258536802, 'tau': 0.2785527521898402}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:36:14,822] Trial 69 finished with value: 0.8046128462366107 and parameters: {'lam': 0.030331023171435628, 'gamma': 0.05366329140824953, 'tau': 0.04005625284592154}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:38:38,151] Trial 70 finished with value: 0.7940482064758322 and parameters: {'lam': 0.01977935716414349, 'gamma': 0.09352795326515606, 'tau': 0.17567451382056376}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:41:00,953] Trial 71 finished with value: 0.7894110804671841 and parameters: {'lam': 0.01648387830415452, 'gamma': 0.03980914968434347, 'tau': 0.2043871287941099}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:43:20,982] Trial 72 finished with value: 0.7898976571406267 and parameters: {'lam': 0.010119711898982909, 'gamma': 0.040274378534289425, 'tau': 0.2030094863532238}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:45:39,414] Trial 73 finished with value: 0.7904912616893897 and parameters: {'lam': 0.016234181461596428, 'gamma': 0.06289942921579558, 'tau': 0.18923572902511007}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:47:59,944] Trial 74 finished with value: 0.7939780447843336 and parameters: {'lam': 0.026782696230484383, 'gamma': 0.030419814137005938, 'tau': 0.21119517568560484}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:50:16,879] Trial 75 finished with value: 0.7919238446960818 and parameters: {'lam': 0.021668898768617995, 'gamma': 0.04718515733718361, 'tau': 0.18233460624760026}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:52:33,551] Trial 76 finished with value: 0.7918146866492176 and parameters: {'lam': 0.0132144667818358, 'gamma': 0.08273382432604248, 'tau': 0.2906967135088444}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:54:50,937] Trial 77 finished with value: 0.7927175419773649 and parameters: {'lam': 0.01815560865645187, 'gamma': 0.018006396807132726, 'tau': 0.2702565549590827}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:57:10,935] Trial 78 finished with value: 0.7921685711101838 and parameters: {'lam': 0.024691407681618798, 'gamma': 0.035916701604564664, 'tau': 0.19966119740919624}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 01:59:30,918] Trial 79 finished with value: 0.7950014038437199 and parameters: {'lam': 0.046195208040199554, 'gamma': 0.06820814573685101, 'tau': 0.16479411947466255}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 02:01:48,755] Trial 80 finished with value: 0.7931627974257612 and parameters: {'lam': 0.014752510247065045, 'gamma': 0.011111304950244927, 'tau': 0.2212998802193761}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 02:04:08,446] Trial 81 finished with value: 0.7932046221848914 and parameters: {'lam': 0.017045561519944057, 'gamma': 0.054514893457432156, 'tau': 0.24522127868307528}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 02:06:29,185] Trial 82 finished with value: 0.7902208044846952 and parameters: {'lam': 0.012190239527248205, 'gamma': 0.18699103333723693, 'tau': 0.23646827901718503}. Best is trial 22 with value: 0.7889689432267974.\n",
      "[I 2025-10-02 02:08:51,857] Trial 83 finished with value: 0.7886477760934653 and parameters: {'lam': 0.020771165296281063, 'gamma': 0.16882362875035972, 'tau': 0.2535922194021547}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:11:10,690] Trial 84 finished with value: 0.7916581081533319 and parameters: {'lam': 0.020330950003362387, 'gamma': 0.12548284342912813, 'tau': 0.2672155406409717}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:13:30,087] Trial 85 finished with value: 0.7901453346717984 and parameters: {'lam': 0.022596656690705767, 'gamma': 0.14273701872417324, 'tau': 0.2934121914118083}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:15:50,173] Trial 86 finished with value: 0.7933763518856581 and parameters: {'lam': 0.02819139855115868, 'gamma': 0.073378441473623, 'tau': 0.28089132641356396}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:18:06,201] Trial 87 finished with value: 0.7919062126054658 and parameters: {'lam': 0.03239138876811208, 'gamma': 0.04774535505961706, 'tau': 0.25588253629796426}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:20:23,699] Trial 88 finished with value: 0.7902781875506713 and parameters: {'lam': 0.015674573156010854, 'gamma': 0.1941340959972081, 'tau': 0.19585087895179945}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:22:42,766] Trial 89 finished with value: 0.7902384267999581 and parameters: {'lam': 0.01914488218295488, 'gamma': 0.10958122988535635, 'tau': 0.1765971811419527}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:25:00,958] Trial 90 finished with value: 0.7911671648745379 and parameters: {'lam': 0.011411285421023709, 'gamma': 0.059865775579065084, 'tau': 0.22948494391200683}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:27:18,307] Trial 91 finished with value: 0.7916975836806841 and parameters: {'lam': 0.01668457033491242, 'gamma': 0.1680650323831823, 'tau': 0.2491825166242554}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:29:35,854] Trial 92 finished with value: 0.808145526391974 and parameters: {'lam': 0.01403147631473635, 'gamma': 0.2022061113366133, 'tau': 0.010709626506785863}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:31:52,167] Trial 93 finished with value: 0.7915599908295832 and parameters: {'lam': 0.020672407497950386, 'gamma': 0.16625735962647942, 'tau': 0.21356008172727747}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:34:09,549] Trial 94 finished with value: 0.7927706721281358 and parameters: {'lam': 0.023254935016846834, 'gamma': 0.1840227214629205, 'tau': 0.2737603676976367}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:36:28,133] Trial 95 finished with value: 0.791960036986251 and parameters: {'lam': 0.025074481273978065, 'gamma': 0.1718504703245413, 'tau': 0.20757083085394062}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:39:02,194] Trial 96 finished with value: 0.7898422801058448 and parameters: {'lam': 0.01505802708198515, 'gamma': 0.15098177670567775, 'tau': 0.2614355348983971}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:41:25,597] Trial 97 finished with value: 0.7945702624240669 and parameters: {'lam': 0.05563571757333456, 'gamma': 0.11802341481479986, 'tau': 0.28853478585859305}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:43:45,173] Trial 98 finished with value: 0.7892630111126632 and parameters: {'lam': 0.017753946807057396, 'gamma': 0.09743555579584945, 'tau': 0.24273568691287412}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:46:05,689] Trial 99 finished with value: 0.791417710333227 and parameters: {'lam': 0.0184848899312655, 'gamma': 0.09544100262272054, 'tau': 0.17219561871062722}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:48:23,537] Trial 100 finished with value: 0.7914616281777569 and parameters: {'lam': 0.010024873330735987, 'gamma': 0.0870473923975153, 'tau': 0.26588110755965716}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:50:43,868] Trial 101 finished with value: 0.791474827520004 and parameters: {'lam': 0.012525284937091104, 'gamma': 0.15652771782031158, 'tau': 0.25536323113660336}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:53:03,187] Trial 102 finished with value: 0.7907167900009154 and parameters: {'lam': 0.01650871818953421, 'gamma': 0.08025178926899412, 'tau': 0.2427895459250798}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:55:19,994] Trial 103 finished with value: 0.7913050420858251 and parameters: {'lam': 0.022112564386012785, 'gamma': 0.13150000756783914, 'tau': 0.21794164722980502}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 02:57:38,893] Trial 104 finished with value: 0.7890008258087557 and parameters: {'lam': 0.02076154557794091, 'gamma': 0.06467016406133401, 'tau': 0.27982259793873676}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 03:00:02,103] Trial 105 finished with value: 0.7890641857870447 and parameters: {'lam': 0.026750043363385705, 'gamma': 0.10005085957532961, 'tau': 0.2811142677122119}. Best is trial 83 with value: 0.7886477760934653.\n",
      "[I 2025-10-02 03:02:21,423] Trial 106 finished with value: 0.7885688975766115 and parameters: {'lam': 0.026454062875325157, 'gamma': 0.06539161815004611, 'tau': 0.29944608054518657}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:04:38,824] Trial 107 finished with value: 0.7904526804042845 and parameters: {'lam': 0.02974904507096737, 'gamma': 0.09886522911818597, 'tau': 0.29989088901971334}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:06:58,800] Trial 108 finished with value: 0.7894500735446511 and parameters: {'lam': 0.02805595344066172, 'gamma': 0.06684370658099449, 'tau': 0.28405963373130483}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:09:22,865] Trial 109 finished with value: 0.7894094906355076 and parameters: {'lam': 0.03380938088204108, 'gamma': 0.07620582843547698, 'tau': 0.2808575903815289}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:11:40,629] Trial 110 finished with value: 0.7920766089087645 and parameters: {'lam': 0.041500223058108486, 'gamma': 0.07504552601303217, 'tau': 0.2777759184687444}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:13:56,863] Trial 111 finished with value: 0.7910537199334627 and parameters: {'lam': 0.025733793323899416, 'gamma': 0.059859921481231496, 'tau': 0.2931631353840002}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:16:13,741] Trial 112 finished with value: 0.7909406945511714 and parameters: {'lam': 0.03500090573033613, 'gamma': 0.10447048944032708, 'tau': 0.2723254062815329}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:18:40,994] Trial 113 finished with value: 0.7906802346320766 and parameters: {'lam': 0.030676773933663283, 'gamma': 0.08053543865059536, 'tau': 0.28529810664345095}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:21:06,458] Trial 114 finished with value: 0.7928612877500668 and parameters: {'lam': 0.02361171319595711, 'gamma': 0.0510684487612522, 'tau': 0.27936664623736096}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:23:24,504] Trial 115 finished with value: 0.7936674333735492 and parameters: {'lam': 0.02640536964306945, 'gamma': 0.09100905134285388, 'tau': 0.264643430398602}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:25:42,502] Trial 116 finished with value: 0.7933875480147831 and parameters: {'lam': 0.02068020701281023, 'gamma': 0.04102536095027689, 'tau': 0.2753505253471822}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:27:58,648] Trial 117 finished with value: 0.7920850984481659 and parameters: {'lam': 0.02875084360541231, 'gamma': 0.07010638923567139, 'tau': 0.288953257320531}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:30:15,746] Trial 118 finished with value: 0.7898187480194236 and parameters: {'lam': 0.03248085651880415, 'gamma': 0.06486393245398708, 'tau': 0.2948786790695955}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:32:32,125] Trial 119 finished with value: 0.7896130305002451 and parameters: {'lam': 0.023457885766594008, 'gamma': 0.030424402036534244, 'tau': 0.282492084266858}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:34:48,440] Trial 120 finished with value: 0.7943304745881492 and parameters: {'lam': 0.036254707621366414, 'gamma': 0.058989273561332045, 'tau': 0.272964541370757}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:37:11,139] Trial 121 finished with value: 0.7936388765489809 and parameters: {'lam': 0.026122986264109242, 'gamma': 0.06685289885131696, 'tau': 0.28492699344108957}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:39:30,768] Trial 122 finished with value: 0.7952776951379313 and parameters: {'lam': 0.028836835785453773, 'gamma': 0.07425870822868003, 'tau': 0.2993758261320564}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:41:46,603] Trial 123 finished with value: 0.7890748287824902 and parameters: {'lam': 0.026937897370691292, 'gamma': 0.08226077555669427, 'tau': 0.25241799097906537}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:44:03,947] Trial 124 finished with value: 0.7898586195980829 and parameters: {'lam': 0.02004101557278907, 'gamma': 0.08157181388572562, 'tau': 0.2542296863009826}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:46:25,735] Trial 125 finished with value: 0.7910854374963785 and parameters: {'lam': 0.021867133268686827, 'gamma': 0.10012889059303161, 'tau': 0.2684307092760249}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:48:43,295] Trial 126 finished with value: 0.7942167754324871 and parameters: {'lam': 0.023992698456758395, 'gamma': 0.11062223494015665, 'tau': 0.24924187511284873}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:51:01,692] Trial 127 finished with value: 0.7927298373716166 and parameters: {'lam': 0.03350754161254398, 'gamma': 0.043309160331787666, 'tau': 0.25867007861783714}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:53:20,359] Trial 128 finished with value: 0.7906695891314754 and parameters: {'lam': 0.018329417191106055, 'gamma': 0.09283196410811598, 'tau': 0.29007527403700284}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:55:39,163] Trial 129 finished with value: 0.7905796818341971 and parameters: {'lam': 0.025377456140702855, 'gamma': 0.05468232991615689, 'tau': 0.26326096043079994}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 03:57:57,243] Trial 130 finished with value: 0.7993476764913383 and parameters: {'lam': 0.026994973370594713, 'gamma': 0.07595638036840877, 'tau': 0.06334948935218677}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:00:19,741] Trial 131 finished with value: 0.789920338645877 and parameters: {'lam': 0.02822430707700893, 'gamma': 0.06612158044180061, 'tau': 0.28036906114230503}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:02:36,439] Trial 132 finished with value: 0.7910600531490956 and parameters: {'lam': 0.03126764173694228, 'gamma': 0.0852797899131028, 'tau': 0.2849796221716442}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:04:53,560] Trial 133 finished with value: 0.7890277611276172 and parameters: {'lam': 0.02153663385116762, 'gamma': 0.04938882489958007, 'tau': 0.27018142660705924}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:07:12,711] Trial 134 finished with value: 0.7897041477855321 and parameters: {'lam': 0.02152300935437208, 'gamma': 0.049560591579940985, 'tau': 0.27201422446412443}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:09:33,793] Trial 135 finished with value: 0.7927670557106291 and parameters: {'lam': 0.019600215804129885, 'gamma': 0.035934022877342015, 'tau': 0.2941946701039829}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:11:55,608] Trial 136 finished with value: 0.7932069090650351 and parameters: {'lam': 0.0229480768641525, 'gamma': 0.0591755114563644, 'tau': 0.2508596279749295}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:14:12,220] Trial 137 finished with value: 0.790193056290305 and parameters: {'lam': 0.017408483470466116, 'gamma': 0.019824278357777298, 'tau': 0.23481528304873062}. Best is trial 106 with value: 0.7885688975766115.\n",
      "[I 2025-10-02 04:16:31,366] Trial 138 finished with value: 0.7878340814135439 and parameters: {'lam': 0.01469563546854434, 'gamma': 0.14325177827923363, 'tau': 0.2677502570525854}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:18:47,967] Trial 139 finished with value: 0.7889753127396769 and parameters: {'lam': 0.015216097045746654, 'gamma': 0.14331501285546114, 'tau': 0.26656531136573014}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:21:08,270] Trial 140 finished with value: 0.7932060351119967 and parameters: {'lam': 0.01409454808890273, 'gamma': 0.15454302713118687, 'tau': 0.268746330838835}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:23:27,776] Trial 141 finished with value: 0.7889219458352942 and parameters: {'lam': 0.01961448483657748, 'gamma': 0.14043369612429293, 'tau': 0.2631923736136623}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:25:44,357] Trial 142 finished with value: 0.7898393404015781 and parameters: {'lam': 0.015085140227558889, 'gamma': 0.14204170109872266, 'tau': 0.2613426761003812}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:28:01,756] Trial 143 finished with value: 0.7894655676901416 and parameters: {'lam': 0.019114862307238543, 'gamma': 0.14512174525137503, 'tau': 0.24258870480700276}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:30:17,137] Trial 144 finished with value: 0.790207471797271 and parameters: {'lam': 0.011763321433488278, 'gamma': 0.1269484770818208, 'tau': 0.2567408433043339}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:32:35,849] Trial 145 finished with value: 0.7931771580006782 and parameters: {'lam': 0.02079434306138853, 'gamma': 0.13381466509511436, 'tau': 0.27609970908355436}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:34:50,317] Trial 146 finished with value: 0.7904295001833622 and parameters: {'lam': 0.017902963402493425, 'gamma': 0.12998100072617286, 'tau': 0.1247827605561288}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:37:22,156] Trial 147 finished with value: 0.7930676579551738 and parameters: {'lam': 0.024642791422018414, 'gamma': 0.14727697406841908, 'tau': 0.26592524996836714}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:39:53,177] Trial 148 finished with value: 0.7921936694880624 and parameters: {'lam': 0.01394538013235218, 'gamma': 0.11967874777939218, 'tau': 0.25169346515552626}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:42:12,096] Trial 149 finished with value: 0.7891437344534838 and parameters: {'lam': 0.01611328216349621, 'gamma': 0.13896894056452574, 'tau': 0.2614599223323779}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:44:31,817] Trial 150 finished with value: 0.7907523017660557 and parameters: {'lam': 0.015691791167704517, 'gamma': 0.15994507753669326, 'tau': 0.2596131824935748}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:46:53,213] Trial 151 finished with value: 0.789284549659821 and parameters: {'lam': 0.019855807431370533, 'gamma': 0.11502906886717143, 'tau': 0.2692522970937575}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:49:10,525] Trial 152 finished with value: 0.7917738848793886 and parameters: {'lam': 0.016984445754604154, 'gamma': 0.1394542995742648, 'tau': 0.2712660121165859}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:51:32,852] Trial 153 finished with value: 0.7953515535222321 and parameters: {'lam': 0.06523571276602545, 'gamma': 0.10561926304735346, 'tau': 0.24717576612341027}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:53:51,553] Trial 154 finished with value: 0.7914160698228512 and parameters: {'lam': 0.019487745779030778, 'gamma': 0.12031323289311405, 'tau': 0.27550327214210235}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:56:10,017] Trial 155 finished with value: 0.7917801402970257 and parameters: {'lam': 0.011810050112049712, 'gamma': 0.13663361417136177, 'tau': 0.2609022149226081}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 04:58:33,126] Trial 156 finished with value: 0.7901227453048584 and parameters: {'lam': 0.02254607866962865, 'gamma': 0.156467421003865, 'tau': 0.2651797093279657}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:00:55,296] Trial 157 finished with value: 0.791809890121018 and parameters: {'lam': 0.014295451606007445, 'gamma': 0.17377653569610574, 'tau': 0.2898704560375015}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:03:12,722] Trial 158 finished with value: 0.7925729932788782 and parameters: {'lam': 0.02153504369530474, 'gamma': 0.11241647428489192, 'tau': 0.25525790958069794}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:05:29,765] Trial 159 finished with value: 0.7991394765570825 and parameters: {'lam': 0.08517985809461133, 'gamma': 0.11581797997211021, 'tau': 0.2790289369426992}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:07:53,654] Trial 160 finished with value: 0.7913008841471169 and parameters: {'lam': 0.016298041043830973, 'gamma': 0.0962277775438358, 'tau': 0.10205555549478894}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:10:14,982] Trial 161 finished with value: 0.7932962444205287 and parameters: {'lam': 0.020858747313092063, 'gamma': 0.14709009622029515, 'tau': 0.2667196411427062}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:12:34,195] Trial 162 finished with value: 0.7896437012051147 and parameters: {'lam': 0.018803969671532365, 'gamma': 0.12388119817885164, 'tau': 0.27108199655242904}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:14:53,001] Trial 163 finished with value: 0.791519545525421 and parameters: {'lam': 0.02401298525878238, 'gamma': 0.13517495916140834, 'tau': 0.28687986359544365}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:17:07,841] Trial 164 finished with value: 0.7924201978391323 and parameters: {'lam': 0.01770988523388132, 'gamma': 0.12971068337696512, 'tau': 0.2762098394818051}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:19:28,144] Trial 165 finished with value: 0.7949090668545592 and parameters: {'lam': 0.0731253680225756, 'gamma': 0.1434812150894561, 'tau': 0.2953015309005681}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:21:43,441] Trial 166 finished with value: 0.7922448744450542 and parameters: {'lam': 0.02636519181920831, 'gamma': 0.1515285059027736, 'tau': 0.23869626853206238}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:24:01,804] Trial 167 finished with value: 0.7901709523974887 and parameters: {'lam': 0.012956129455987236, 'gamma': 0.07151166109569514, 'tau': 0.2536022153144732}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:26:21,350] Trial 168 finished with value: 0.7913884342998079 and parameters: {'lam': 0.02268420794335719, 'gamma': 0.08860180852397195, 'tau': 0.2616329403802538}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:28:57,614] Trial 169 finished with value: 0.7974958019222456 and parameters: {'lam': 0.09486917660323976, 'gamma': 0.10709649595882252, 'tau': 0.28306387640335307}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:31:50,029] Trial 170 finished with value: 0.7892536418424264 and parameters: {'lam': 0.019701452908825867, 'gamma': 0.2625748668995264, 'tau': 0.2688323171495164}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:34:17,693] Trial 171 finished with value: 0.7894653194586652 and parameters: {'lam': 0.020676773769118813, 'gamma': 0.2356567479137068, 'tau': 0.2702680513001478}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:36:47,065] Trial 172 finished with value: 0.7907201189984456 and parameters: {'lam': 0.019401149738084114, 'gamma': 0.21414020761201139, 'tau': 0.26485502580903725}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:39:19,217] Trial 173 finished with value: 0.7932354013062269 and parameters: {'lam': 0.015851453343202464, 'gamma': 0.26242791375207386, 'tau': 0.278827101080647}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:41:35,190] Trial 174 finished with value: 0.7910701022807071 and parameters: {'lam': 0.024767069861829638, 'gamma': 0.28742645290644253, 'tau': 0.24427500634780702}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:43:52,604] Trial 175 finished with value: 0.7911585846849826 and parameters: {'lam': 0.02249503180161182, 'gamma': 0.14026087027194192, 'tau': 0.25716518099103747}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:46:09,596] Trial 176 finished with value: 0.7939169819767656 and parameters: {'lam': 0.01916228263709822, 'gamma': 0.05471588679966902, 'tau': 0.2736293163241088}. Best is trial 138 with value: 0.7878340814135439.\n",
      "[I 2025-10-02 05:48:27,896] Trial 177 finished with value: 0.7875698372914338 and parameters: {'lam': 0.017684422019387584, 'gamma': 0.06224262766516858, 'tau': 0.29037492246228525}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 05:50:47,724] Trial 178 finished with value: 0.7949840217917009 and parameters: {'lam': 0.017112876934799258, 'gamma': 0.2996470974069394, 'tau': 0.2898717366164234}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 05:53:07,380] Trial 179 finished with value: 0.7892586478480792 and parameters: {'lam': 0.014205023155696814, 'gamma': 0.06139903591775234, 'tau': 0.2808069685398102}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 05:55:22,211] Trial 180 finished with value: 0.7916296929463241 and parameters: {'lam': 0.013909714632450994, 'gamma': 0.047650285690519684, 'tau': 0.29536080687706817}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 05:57:41,418] Trial 181 finished with value: 0.7923378831977463 and parameters: {'lam': 0.011260394315469876, 'gamma': 0.06314031337810727, 'tau': 0.2996686063931067}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:00:03,773] Trial 182 finished with value: 0.7914194817975363 and parameters: {'lam': 0.0156334900782125, 'gamma': 0.24652566547717442, 'tau': 0.28343265751191116}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:02:20,057] Trial 183 finished with value: 0.7903358973334724 and parameters: {'lam': 0.017995034946353437, 'gamma': 0.05805976610644701, 'tau': 0.2904377584317453}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:04:35,595] Trial 184 finished with value: 0.7910421315973294 and parameters: {'lam': 0.012918065776532527, 'gamma': 0.06815733632545884, 'tau': 0.26862425666947964}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:06:56,878] Trial 185 finished with value: 0.7902976127721296 and parameters: {'lam': 0.015584154208099957, 'gamma': 0.0828337702653976, 'tau': 0.2819193954142662}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:09:17,318] Trial 186 finished with value: 0.790803827505335 and parameters: {'lam': 0.020644719211266614, 'gamma': 0.06312335858609658, 'tau': 0.27678307743697295}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:11:32,273] Trial 187 finished with value: 0.7909612580208998 and parameters: {'lam': 0.01045342535213058, 'gamma': 0.05249933386085255, 'tau': 0.2613348642601979}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:13:47,617] Trial 188 finished with value: 0.7915381251365994 and parameters: {'lam': 0.018279357833180396, 'gamma': 0.07730438789684772, 'tau': 0.2500520415748073}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:16:04,856] Trial 189 finished with value: 0.7917110596350666 and parameters: {'lam': 0.014148576010831682, 'gamma': 0.044744863081158825, 'tau': 0.28641908717148895}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:18:23,713] Trial 190 finished with value: 0.7901530078104717 and parameters: {'lam': 0.023368902418452155, 'gamma': 0.05929268036314186, 'tau': 0.2686978740218887}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:20:43,928] Trial 191 finished with value: 0.789258586130324 and parameters: {'lam': 0.020612968584778135, 'gamma': 0.07202046187309728, 'tau': 0.27504379510753035}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:23:02,145] Trial 192 finished with value: 0.7895491213595291 and parameters: {'lam': 0.020269984669097015, 'gamma': 0.06813594887349118, 'tau': 0.27528672653907893}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:25:21,472] Trial 193 finished with value: 0.7905506268988992 and parameters: {'lam': 0.01694532478191299, 'gamma': 0.07082271562265464, 'tau': 0.2628105413020933}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:27:37,571] Trial 194 finished with value: 0.7923345703642765 and parameters: {'lam': 0.025237675518012117, 'gamma': 0.07758913950831063, 'tau': 0.29229098331915687}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:29:54,055] Trial 195 finished with value: 0.7888687169517321 and parameters: {'lam': 0.021558597506629383, 'gamma': 0.09195741766289875, 'tau': 0.27998402404332196}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:32:10,713] Trial 196 finished with value: 0.7924301730753225 and parameters: {'lam': 0.021604511215971754, 'gamma': 0.09779449223532542, 'tau': 0.2825151063303315}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:34:26,834] Trial 197 finished with value: 0.7892914639207927 and parameters: {'lam': 0.019134652139110225, 'gamma': 0.2273426050472187, 'tau': 0.2777285530291694}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:36:47,519] Trial 198 finished with value: 0.7925178460399271 and parameters: {'lam': 0.04925928270453485, 'gamma': 0.08809676087117205, 'tau': 0.2871190724779091}. Best is trial 177 with value: 0.7875698372914338.\n",
      "[I 2025-10-02 06:39:05,628] Trial 199 finished with value: 0.79226078765413 and parameters: {'lam': 0.015924898457500464, 'gamma': 0.09190256672669889, 'tau': 0.2707808629915243}. Best is trial 177 with value: 0.7875698372914338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'lam': 0.017684422019387584, 'gamma': 0.06224262766516858, 'tau': 0.29037492246228525}\n",
      "Best RMSE: 0.7876\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    lam = trial.suggest_float(\"lam\", 0.01, 0.1)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.01, 0.3)\n",
    "    tau = trial.suggest_float(\"tau\", 0.01, 0.3)\n",
    "    k = 12\n",
    "\n",
    "    user_bias = np.zeros(n_users)\n",
    "    item_bias = np.zeros(n_items)\n",
    "    user_embedding = np.random.normal(scale=0.15, size=(n_users, k))\n",
    "    item_embedding = np.random.normal(scale=0.15, size=(n_items, k))\n",
    "    feature_embedding = np.random.normal(scale=0.15, size=(n_features, k))\n",
    "    mu_train = train_data[:, 2].mean()\n",
    "\n",
    "    iterations = 4\n",
    "\n",
    "    for iteration in range(1, iterations + 1):\n",
    "        # Update user biases\n",
    "        user_bias = Parallel(n_jobs=-1)(\n",
    "            delayed(update_user_bias)(\n",
    "                user,\n",
    "                mu_train,\n",
    "                data_by_user_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "            ) for user in range(n_users)\n",
    "        )\n",
    "        user_bias = np.array(user_bias)\n",
    "\n",
    "        # Update user embeddings\n",
    "        user_embedding = Parallel(n_jobs=-1)(\n",
    "            delayed(update_user_embedding)(\n",
    "                user,\n",
    "                mu_train,\n",
    "                data_by_user_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "            ) for user in range(n_users)\n",
    "        )\n",
    "        user_embedding = np.vstack(user_embedding)\n",
    "\n",
    "        # Update item biases\n",
    "        item_bias = Parallel(n_jobs=-1)(\n",
    "            delayed(update_item_bias)(\n",
    "                item,\n",
    "                mu_train,\n",
    "                data_by_movie_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "            ) for item in range(n_items)\n",
    "        )\n",
    "        item_bias = np.array(item_bias)\n",
    "\n",
    "        # Update item embeddings\n",
    "        item_embedding = Parallel(n_jobs=-1)(\n",
    "            delayed(update_item_embedding)(\n",
    "                item,\n",
    "                mu_train,\n",
    "                data_by_movie_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "                item_features,\n",
    "                feature_embedding,\n",
    "            ) for item in range(n_items)\n",
    "        )\n",
    "        item_embedding = np.vstack(item_embedding)\n",
    "\n",
    "        # Update feature embeddings\n",
    "        feature_embedding = update_feature_embedding(\n",
    "            item_features, item_embedding, tau, lam=lam\n",
    "        )\n",
    "\n",
    "    # Compute test RMSE\n",
    "    uv_test = np.sum(\n",
    "        user_embedding[test_coo.row] * item_embedding[test_coo.col], axis=1\n",
    "    )\n",
    "    pred_test = user_bias[test_coo.row] + item_bias[test_coo.col] + uv_test + mu_train\n",
    "    diff_test = test_coo.data - pred_test\n",
    "    test_rmse = np.sqrt(np.mean(diff_test**2))\n",
    "\n",
    "    return test_rmse\n",
    "\n",
    "\n",
    "# Run study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200, n_jobs=1)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(study.best_params)\n",
    "print(f\"Best RMSE: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gmedw\\AppData\\Local\\Temp\\ipykernel_23304\\703367911.py:83: RuntimeWarning: divide by zero encountered in divide\n",
      "  scaling = np.where(F_n > 0, 1.0/np.sqrt(F_n), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, NLL = 166924.6327, Train RMSE = 0.8533, Test RMSE = 0.8707\n",
      "Iteration 2, NLL = 150307.0708, Train RMSE = 0.8032, Test RMSE = 0.8315\n",
      "Iteration 3, NLL = 144955.7412, Train RMSE = 0.7857, Test RMSE = 0.8152\n",
      "Iteration 4, NLL = 143166.3527, Train RMSE = 0.7796, Test RMSE = 0.8096\n",
      "Iteration 5, NLL = 142184.1914, Train RMSE = 0.7763, Test RMSE = 0.8065\n",
      "Iteration 6, NLL = 141461.8987, Train RMSE = 0.7738, Test RMSE = 0.8043\n",
      "Iteration 7, NLL = 140870.6040, Train RMSE = 0.7719, Test RMSE = 0.8025\n",
      "Iteration 8, NLL = 140401.3094, Train RMSE = 0.7703, Test RMSE = 0.8011\n",
      "Iteration 9, NLL = 140053.2182, Train RMSE = 0.7692, Test RMSE = 0.8000\n",
      "Iteration 10, NLL = 139805.2859, Train RMSE = 0.7683, Test RMSE = 0.7993\n",
      "Iteration 11, NLL = 139630.2913, Train RMSE = 0.7678, Test RMSE = 0.7987\n",
      "Iteration 12, NLL = 139505.9223, Train RMSE = 0.7674, Test RMSE = 0.7984\n",
      "Iteration 13, NLL = 139416.3932, Train RMSE = 0.7671, Test RMSE = 0.7981\n",
      "Iteration 14, NLL = 139350.9675, Train RMSE = 0.7669, Test RMSE = 0.7979\n",
      "Iteration 15, NLL = 139302.3802, Train RMSE = 0.7668, Test RMSE = 0.7978\n",
      "Iteration 1, NLL = 166313.4254, Train RMSE = 0.8510, Test RMSE = 0.8698\n",
      "Iteration 2, NLL = 147669.2201, Train RMSE = 0.7935, Test RMSE = 0.8260\n",
      "Iteration 3, NLL = 141689.1585, Train RMSE = 0.7736, Test RMSE = 0.8076\n",
      "Iteration 4, NLL = 139406.9449, Train RMSE = 0.7658, Test RMSE = 0.8004\n",
      "Iteration 5, NLL = 138417.4995, Train RMSE = 0.7624, Test RMSE = 0.7974\n",
      "Iteration 6, NLL = 137919.1565, Train RMSE = 0.7608, Test RMSE = 0.7960\n",
      "Iteration 7, NLL = 137623.2251, Train RMSE = 0.7598, Test RMSE = 0.7952\n",
      "Iteration 8, NLL = 137420.6191, Train RMSE = 0.7592, Test RMSE = 0.7946\n",
      "Iteration 9, NLL = 137266.3509, Train RMSE = 0.7588, Test RMSE = 0.7943\n",
      "Iteration 10, NLL = 137140.3643, Train RMSE = 0.7584, Test RMSE = 0.7940\n",
      "Iteration 11, NLL = 137033.1651, Train RMSE = 0.7581, Test RMSE = 0.7937\n",
      "Iteration 12, NLL = 136939.9253, Train RMSE = 0.7578, Test RMSE = 0.7935\n",
      "Iteration 13, NLL = 136857.9397, Train RMSE = 0.7576, Test RMSE = 0.7934\n",
      "Iteration 14, NLL = 136785.5725, Train RMSE = 0.7574, Test RMSE = 0.7932\n",
      "Iteration 15, NLL = 136721.8137, Train RMSE = 0.7572, Test RMSE = 0.7931\n",
      "Iteration 1, NLL = 165652.4327, Train RMSE = 0.8487, Test RMSE = 0.8694\n",
      "Iteration 2, NLL = 145727.6150, Train RMSE = 0.7865, Test RMSE = 0.8237\n",
      "Iteration 3, NLL = 139914.4099, Train RMSE = 0.7669, Test RMSE = 0.8057\n",
      "Iteration 4, NLL = 137593.5432, Train RMSE = 0.7590, Test RMSE = 0.7984\n",
      "Iteration 5, NLL = 136261.2076, Train RMSE = 0.7545, Test RMSE = 0.7942\n",
      "Iteration 6, NLL = 135451.1246, Train RMSE = 0.7518, Test RMSE = 0.7917\n",
      "Iteration 7, NLL = 134962.1898, Train RMSE = 0.7502, Test RMSE = 0.7902\n",
      "Iteration 8, NLL = 134657.0179, Train RMSE = 0.7492, Test RMSE = 0.7893\n",
      "Iteration 9, NLL = 134453.9473, Train RMSE = 0.7485, Test RMSE = 0.7888\n",
      "Iteration 10, NLL = 134309.1132, Train RMSE = 0.7481, Test RMSE = 0.7884\n",
      "Iteration 11, NLL = 134199.6911, Train RMSE = 0.7478, Test RMSE = 0.7881\n",
      "Iteration 12, NLL = 134114.0750, Train RMSE = 0.7475, Test RMSE = 0.7879\n",
      "Iteration 13, NLL = 134046.2872, Train RMSE = 0.7473, Test RMSE = 0.7878\n",
      "Iteration 14, NLL = 133992.6569, Train RMSE = 0.7472, Test RMSE = 0.7877\n",
      "Iteration 15, NLL = 133950.3063, Train RMSE = 0.7470, Test RMSE = 0.7876\n",
      "Iteration 1, NLL = 165078.4616, Train RMSE = 0.8465, Test RMSE = 0.8697\n",
      "Iteration 2, NLL = 143918.7249, Train RMSE = 0.7798, Test RMSE = 0.8216\n",
      "Iteration 3, NLL = 137595.6477, Train RMSE = 0.7584, Test RMSE = 0.8020\n",
      "Iteration 4, NLL = 135165.6492, Train RMSE = 0.7499, Test RMSE = 0.7941\n",
      "Iteration 5, NLL = 133947.4446, Train RMSE = 0.7457, Test RMSE = 0.7901\n",
      "Iteration 6, NLL = 133261.9339, Train RMSE = 0.7434, Test RMSE = 0.7880\n",
      "Iteration 7, NLL = 132852.8023, Train RMSE = 0.7420, Test RMSE = 0.7868\n",
      "Iteration 8, NLL = 132593.3611, Train RMSE = 0.7412, Test RMSE = 0.7861\n",
      "Iteration 9, NLL = 132417.9788, Train RMSE = 0.7406, Test RMSE = 0.7856\n",
      "Iteration 10, NLL = 132291.8230, Train RMSE = 0.7402, Test RMSE = 0.7853\n",
      "Iteration 11, NLL = 132195.8127, Train RMSE = 0.7399, Test RMSE = 0.7851\n",
      "Iteration 12, NLL = 132119.3379, Train RMSE = 0.7397, Test RMSE = 0.7849\n",
      "Iteration 13, NLL = 132056.6235, Train RMSE = 0.7395, Test RMSE = 0.7848\n",
      "Iteration 14, NLL = 132004.5731, Train RMSE = 0.7394, Test RMSE = 0.7847\n",
      "Iteration 15, NLL = 131961.2902, Train RMSE = 0.7393, Test RMSE = 0.7846\n",
      "Iteration 1, NLL = 164969.5148, Train RMSE = 0.8458, Test RMSE = 0.8711\n",
      "Iteration 2, NLL = 143496.0701, Train RMSE = 0.7776, Test RMSE = 0.8235\n",
      "Iteration 3, NLL = 136002.5411, Train RMSE = 0.7522, Test RMSE = 0.8003\n",
      "Iteration 4, NLL = 133374.7122, Train RMSE = 0.7430, Test RMSE = 0.7918\n",
      "Iteration 5, NLL = 132232.5497, Train RMSE = 0.7390, Test RMSE = 0.7881\n",
      "Iteration 6, NLL = 131640.7051, Train RMSE = 0.7370, Test RMSE = 0.7863\n",
      "Iteration 7, NLL = 131293.8834, Train RMSE = 0.7358, Test RMSE = 0.7853\n",
      "Iteration 8, NLL = 131070.5979, Train RMSE = 0.7351, Test RMSE = 0.7847\n",
      "Iteration 9, NLL = 130916.1313, Train RMSE = 0.7346, Test RMSE = 0.7843\n",
      "Iteration 10, NLL = 130803.1916, Train RMSE = 0.7343, Test RMSE = 0.7841\n",
      "Iteration 11, NLL = 130716.9422, Train RMSE = 0.7340, Test RMSE = 0.7839\n",
      "Iteration 12, NLL = 130648.6699, Train RMSE = 0.7338, Test RMSE = 0.7837\n",
      "Iteration 13, NLL = 130592.8908, Train RMSE = 0.7337, Test RMSE = 0.7836\n",
      "Iteration 14, NLL = 130545.9529, Train RMSE = 0.7336, Test RMSE = 0.7835\n",
      "Iteration 15, NLL = 130505.3153, Train RMSE = 0.7334, Test RMSE = 0.7834\n",
      "Iteration 1, NLL = 164516.0345, Train RMSE = 0.8442, Test RMSE = 0.8709\n",
      "Iteration 2, NLL = 142126.3351, Train RMSE = 0.7723, Test RMSE = 0.8228\n",
      "Iteration 3, NLL = 135047.5242, Train RMSE = 0.7482, Test RMSE = 0.8006\n",
      "Iteration 4, NLL = 132024.2842, Train RMSE = 0.7376, Test RMSE = 0.7905\n",
      "Iteration 5, NLL = 130517.5925, Train RMSE = 0.7323, Test RMSE = 0.7856\n",
      "Iteration 6, NLL = 129708.3533, Train RMSE = 0.7295, Test RMSE = 0.7830\n",
      "Iteration 7, NLL = 129234.9853, Train RMSE = 0.7279, Test RMSE = 0.7816\n",
      "Iteration 8, NLL = 128936.2067, Train RMSE = 0.7269, Test RMSE = 0.7808\n",
      "Iteration 9, NLL = 128735.7167, Train RMSE = 0.7263, Test RMSE = 0.7802\n",
      "Iteration 10, NLL = 128594.6150, Train RMSE = 0.7258, Test RMSE = 0.7799\n",
      "Iteration 11, NLL = 128491.5646, Train RMSE = 0.7255, Test RMSE = 0.7796\n",
      "Iteration 12, NLL = 128414.0585, Train RMSE = 0.7253, Test RMSE = 0.7795\n",
      "Iteration 13, NLL = 128354.3462, Train RMSE = 0.7251, Test RMSE = 0.7794\n",
      "Iteration 14, NLL = 128307.4056, Train RMSE = 0.7250, Test RMSE = 0.7793\n",
      "Iteration 15, NLL = 128269.8638, Train RMSE = 0.7249, Test RMSE = 0.7792\n",
      "Iteration 1, NLL = 163586.7411, Train RMSE = 0.8411, Test RMSE = 0.8699\n",
      "Iteration 2, NLL = 140243.4406, Train RMSE = 0.7654, Test RMSE = 0.8203\n",
      "Iteration 3, NLL = 133083.6209, Train RMSE = 0.7407, Test RMSE = 0.7980\n",
      "Iteration 4, NLL = 130420.1559, Train RMSE = 0.7311, Test RMSE = 0.7891\n",
      "Iteration 5, NLL = 129219.0850, Train RMSE = 0.7268, Test RMSE = 0.7851\n",
      "Iteration 6, NLL = 128575.0998, Train RMSE = 0.7245, Test RMSE = 0.7830\n",
      "Iteration 7, NLL = 128182.8515, Train RMSE = 0.7232, Test RMSE = 0.7818\n",
      "Iteration 8, NLL = 127920.1595, Train RMSE = 0.7223, Test RMSE = 0.7810\n",
      "Iteration 9, NLL = 127731.3004, Train RMSE = 0.7217, Test RMSE = 0.7804\n",
      "Iteration 10, NLL = 127587.9212, Train RMSE = 0.7213, Test RMSE = 0.7800\n",
      "Iteration 11, NLL = 127474.2810, Train RMSE = 0.7209, Test RMSE = 0.7797\n",
      "Iteration 12, NLL = 127381.0751, Train RMSE = 0.7207, Test RMSE = 0.7795\n",
      "Iteration 13, NLL = 127302.5626, Train RMSE = 0.7204, Test RMSE = 0.7793\n",
      "Iteration 14, NLL = 127235.0880, Train RMSE = 0.7202, Test RMSE = 0.7791\n",
      "Iteration 15, NLL = 127176.2537, Train RMSE = 0.7200, Test RMSE = 0.7789\n",
      "Iteration 1, NLL = 163029.9309, Train RMSE = 0.8390, Test RMSE = 0.8696\n",
      "Iteration 2, NLL = 138796.8784, Train RMSE = 0.7597, Test RMSE = 0.8190\n",
      "Iteration 3, NLL = 131862.6655, Train RMSE = 0.7356, Test RMSE = 0.7973\n",
      "Iteration 4, NLL = 129213.6638, Train RMSE = 0.7261, Test RMSE = 0.7885\n",
      "Iteration 5, NLL = 127882.5770, Train RMSE = 0.7213, Test RMSE = 0.7840\n",
      "Iteration 6, NLL = 127113.5628, Train RMSE = 0.7186, Test RMSE = 0.7814\n",
      "Iteration 7, NLL = 126632.7381, Train RMSE = 0.7169, Test RMSE = 0.7799\n",
      "Iteration 8, NLL = 126314.2131, Train RMSE = 0.7159, Test RMSE = 0.7789\n",
      "Iteration 9, NLL = 126092.5776, Train RMSE = 0.7151, Test RMSE = 0.7782\n",
      "Iteration 10, NLL = 125931.3787, Train RMSE = 0.7146, Test RMSE = 0.7777\n",
      "Iteration 11, NLL = 125809.6321, Train RMSE = 0.7142, Test RMSE = 0.7774\n",
      "Iteration 12, NLL = 125714.8617, Train RMSE = 0.7140, Test RMSE = 0.7772\n",
      "Iteration 13, NLL = 125639.3100, Train RMSE = 0.7137, Test RMSE = 0.7770\n",
      "Iteration 14, NLL = 125577.9065, Train RMSE = 0.7135, Test RMSE = 0.7768\n",
      "Iteration 15, NLL = 125527.1808, Train RMSE = 0.7134, Test RMSE = 0.7767\n",
      "Iteration 1, NLL = 162634.9549, Train RMSE = 0.8375, Test RMSE = 0.8698\n",
      "Iteration 2, NLL = 138451.4014, Train RMSE = 0.7576, Test RMSE = 0.8209\n",
      "Iteration 3, NLL = 131300.7167, Train RMSE = 0.7328, Test RMSE = 0.7984\n",
      "Iteration 4, NLL = 128359.2637, Train RMSE = 0.7223, Test RMSE = 0.7887\n",
      "Iteration 5, NLL = 126931.9677, Train RMSE = 0.7172, Test RMSE = 0.7839\n",
      "Iteration 6, NLL = 126119.7675, Train RMSE = 0.7143, Test RMSE = 0.7812\n",
      "Iteration 7, NLL = 125600.1755, Train RMSE = 0.7125, Test RMSE = 0.7794\n",
      "Iteration 8, NLL = 125239.9323, Train RMSE = 0.7113, Test RMSE = 0.7783\n",
      "Iteration 9, NLL = 124974.9520, Train RMSE = 0.7104, Test RMSE = 0.7775\n",
      "Iteration 10, NLL = 124771.4104, Train RMSE = 0.7098, Test RMSE = 0.7768\n",
      "Iteration 11, NLL = 124610.5562, Train RMSE = 0.7092, Test RMSE = 0.7764\n",
      "Iteration 12, NLL = 124481.3383, Train RMSE = 0.7088, Test RMSE = 0.7760\n",
      "Iteration 13, NLL = 124376.6002, Train RMSE = 0.7085, Test RMSE = 0.7757\n",
      "Iteration 14, NLL = 124291.2496, Train RMSE = 0.7082, Test RMSE = 0.7755\n",
      "Iteration 15, NLL = 124221.4372, Train RMSE = 0.7080, Test RMSE = 0.7753\n",
      "Iteration 1, NLL = 162933.1996, Train RMSE = 0.8379, Test RMSE = 0.8724\n",
      "Iteration 2, NLL = 137683.1452, Train RMSE = 0.7542, Test RMSE = 0.8214\n",
      "Iteration 3, NLL = 130283.8058, Train RMSE = 0.7283, Test RMSE = 0.7980\n",
      "Iteration 4, NLL = 127463.8900, Train RMSE = 0.7182, Test RMSE = 0.7887\n",
      "Iteration 5, NLL = 125994.2540, Train RMSE = 0.7130, Test RMSE = 0.7837\n",
      "Iteration 6, NLL = 125101.4746, Train RMSE = 0.7099, Test RMSE = 0.7808\n",
      "Iteration 7, NLL = 124509.8743, Train RMSE = 0.7078, Test RMSE = 0.7789\n",
      "Iteration 8, NLL = 124094.8094, Train RMSE = 0.7064, Test RMSE = 0.7775\n",
      "Iteration 9, NLL = 123795.3282, Train RMSE = 0.7054, Test RMSE = 0.7766\n",
      "Iteration 10, NLL = 123575.5504, Train RMSE = 0.7047, Test RMSE = 0.7760\n",
      "Iteration 11, NLL = 123410.8020, Train RMSE = 0.7041, Test RMSE = 0.7755\n",
      "Iteration 12, NLL = 123284.1207, Train RMSE = 0.7037, Test RMSE = 0.7752\n",
      "Iteration 13, NLL = 123184.2558, Train RMSE = 0.7034, Test RMSE = 0.7749\n",
      "Iteration 14, NLL = 123103.8121, Train RMSE = 0.7031, Test RMSE = 0.7747\n",
      "Iteration 15, NLL = 123037.8533, Train RMSE = 0.7029, Test RMSE = 0.7746\n",
      "Iteration 1, NLL = 162109.6480, Train RMSE = 0.8352, Test RMSE = 0.8709\n",
      "Iteration 2, NLL = 137073.5100, Train RMSE = 0.7513, Test RMSE = 0.8223\n",
      "Iteration 3, NLL = 129682.3541, Train RMSE = 0.7254, Test RMSE = 0.7990\n",
      "Iteration 4, NLL = 126540.2319, Train RMSE = 0.7142, Test RMSE = 0.7885\n",
      "Iteration 5, NLL = 124940.0744, Train RMSE = 0.7085, Test RMSE = 0.7831\n",
      "Iteration 6, NLL = 124016.3407, Train RMSE = 0.7052, Test RMSE = 0.7801\n",
      "Iteration 7, NLL = 123430.0774, Train RMSE = 0.7032, Test RMSE = 0.7783\n",
      "Iteration 8, NLL = 123029.0503, Train RMSE = 0.7018, Test RMSE = 0.7770\n",
      "Iteration 9, NLL = 122738.9810, Train RMSE = 0.7008, Test RMSE = 0.7761\n",
      "Iteration 10, NLL = 122520.4704, Train RMSE = 0.7001, Test RMSE = 0.7755\n",
      "Iteration 11, NLL = 122350.6329, Train RMSE = 0.6996, Test RMSE = 0.7750\n",
      "Iteration 12, NLL = 122215.2477, Train RMSE = 0.6991, Test RMSE = 0.7746\n",
      "Iteration 13, NLL = 122105.1388, Train RMSE = 0.6988, Test RMSE = 0.7743\n",
      "Iteration 14, NLL = 122014.2314, Train RMSE = 0.6985, Test RMSE = 0.7740\n",
      "Iteration 15, NLL = 121938.3863, Train RMSE = 0.6982, Test RMSE = 0.7738\n",
      "Iteration 1, NLL = 161840.0953, Train RMSE = 0.8340, Test RMSE = 0.8719\n",
      "Iteration 2, NLL = 136217.6035, Train RMSE = 0.7476, Test RMSE = 0.8231\n",
      "Iteration 3, NLL = 128600.4983, Train RMSE = 0.7210, Test RMSE = 0.7991\n",
      "Iteration 4, NLL = 125366.9539, Train RMSE = 0.7094, Test RMSE = 0.7881\n",
      "Iteration 5, NLL = 123763.3584, Train RMSE = 0.7036, Test RMSE = 0.7825\n",
      "Iteration 6, NLL = 122863.3815, Train RMSE = 0.7004, Test RMSE = 0.7794\n",
      "Iteration 7, NLL = 122300.1036, Train RMSE = 0.6984, Test RMSE = 0.7776\n",
      "Iteration 8, NLL = 121917.0072, Train RMSE = 0.6971, Test RMSE = 0.7764\n",
      "Iteration 9, NLL = 121640.2963, Train RMSE = 0.6961, Test RMSE = 0.7755\n",
      "Iteration 10, NLL = 121431.5686, Train RMSE = 0.6954, Test RMSE = 0.7749\n",
      "Iteration 11, NLL = 121269.1304, Train RMSE = 0.6949, Test RMSE = 0.7744\n",
      "Iteration 12, NLL = 121139.8144, Train RMSE = 0.6945, Test RMSE = 0.7740\n",
      "Iteration 13, NLL = 121035.0958, Train RMSE = 0.6941, Test RMSE = 0.7737\n",
      "Iteration 14, NLL = 120949.1555, Train RMSE = 0.6938, Test RMSE = 0.7734\n",
      "Iteration 15, NLL = 120877.8573, Train RMSE = 0.6936, Test RMSE = 0.7733\n",
      "Iteration 1, NLL = 161029.6684, Train RMSE = 0.8311, Test RMSE = 0.8704\n",
      "Iteration 2, NLL = 134049.7463, Train RMSE = 0.7395, Test RMSE = 0.8179\n",
      "Iteration 3, NLL = 127109.0043, Train RMSE = 0.7148, Test RMSE = 0.7962\n",
      "Iteration 4, NLL = 124395.2532, Train RMSE = 0.7050, Test RMSE = 0.7873\n",
      "Iteration 5, NLL = 122970.9225, Train RMSE = 0.6999, Test RMSE = 0.7826\n",
      "Iteration 6, NLL = 122097.2646, Train RMSE = 0.6968, Test RMSE = 0.7798\n",
      "Iteration 7, NLL = 121518.3196, Train RMSE = 0.6948, Test RMSE = 0.7780\n",
      "Iteration 8, NLL = 121116.2804, Train RMSE = 0.6934, Test RMSE = 0.7768\n",
      "Iteration 9, NLL = 120826.0721, Train RMSE = 0.6925, Test RMSE = 0.7759\n",
      "Iteration 10, NLL = 120609.2308, Train RMSE = 0.6917, Test RMSE = 0.7753\n",
      "Iteration 11, NLL = 120442.4013, Train RMSE = 0.6912, Test RMSE = 0.7748\n",
      "Iteration 12, NLL = 120310.9846, Train RMSE = 0.6908, Test RMSE = 0.7745\n",
      "Iteration 13, NLL = 120205.4871, Train RMSE = 0.6904, Test RMSE = 0.7742\n",
      "Iteration 14, NLL = 120119.4657, Train RMSE = 0.6901, Test RMSE = 0.7740\n",
      "Iteration 15, NLL = 120048.3885, Train RMSE = 0.6899, Test RMSE = 0.7738\n",
      "Iteration 1, NLL = 161267.6944, Train RMSE = 0.8316, Test RMSE = 0.8739\n",
      "Iteration 2, NLL = 135023.2550, Train RMSE = 0.7420, Test RMSE = 0.8252\n",
      "Iteration 3, NLL = 127098.8154, Train RMSE = 0.7141, Test RMSE = 0.8000\n",
      "Iteration 4, NLL = 123849.4586, Train RMSE = 0.7023, Test RMSE = 0.7890\n",
      "Iteration 5, NLL = 122237.8236, Train RMSE = 0.6965, Test RMSE = 0.7835\n",
      "Iteration 6, NLL = 121295.1402, Train RMSE = 0.6931, Test RMSE = 0.7803\n",
      "Iteration 7, NLL = 120680.4496, Train RMSE = 0.6910, Test RMSE = 0.7783\n",
      "Iteration 8, NLL = 120250.2202, Train RMSE = 0.6895, Test RMSE = 0.7769\n",
      "Iteration 9, NLL = 119935.7392, Train RMSE = 0.6884, Test RMSE = 0.7759\n",
      "Iteration 10, NLL = 119699.4923, Train RMSE = 0.6876, Test RMSE = 0.7752\n",
      "Iteration 11, NLL = 119518.2744, Train RMSE = 0.6870, Test RMSE = 0.7747\n",
      "Iteration 12, NLL = 119376.6497, Train RMSE = 0.6866, Test RMSE = 0.7742\n",
      "Iteration 13, NLL = 119264.0740, Train RMSE = 0.6862, Test RMSE = 0.7739\n",
      "Iteration 14, NLL = 119173.2535, Train RMSE = 0.6859, Test RMSE = 0.7737\n",
      "Iteration 15, NLL = 119099.0562, Train RMSE = 0.6857, Test RMSE = 0.7735\n",
      "Iteration 1, NLL = 160498.4208, Train RMSE = 0.8288, Test RMSE = 0.8723\n",
      "Iteration 2, NLL = 133440.1971, Train RMSE = 0.7359, Test RMSE = 0.8225\n",
      "Iteration 3, NLL = 125932.1475, Train RMSE = 0.7091, Test RMSE = 0.7989\n",
      "Iteration 4, NLL = 122861.8164, Train RMSE = 0.6979, Test RMSE = 0.7885\n",
      "Iteration 5, NLL = 121309.8024, Train RMSE = 0.6923, Test RMSE = 0.7833\n",
      "Iteration 6, NLL = 120396.7067, Train RMSE = 0.6890, Test RMSE = 0.7802\n",
      "Iteration 7, NLL = 119804.9349, Train RMSE = 0.6870, Test RMSE = 0.7783\n",
      "Iteration 8, NLL = 119394.1282, Train RMSE = 0.6856, Test RMSE = 0.7770\n",
      "Iteration 9, NLL = 119093.7775, Train RMSE = 0.6846, Test RMSE = 0.7761\n",
      "Iteration 10, NLL = 118865.4801, Train RMSE = 0.6838, Test RMSE = 0.7754\n",
      "Iteration 11, NLL = 118686.8726, Train RMSE = 0.6832, Test RMSE = 0.7749\n",
      "Iteration 12, NLL = 118544.0314, Train RMSE = 0.6827, Test RMSE = 0.7745\n",
      "Iteration 13, NLL = 118427.7371, Train RMSE = 0.6824, Test RMSE = 0.7741\n",
      "Iteration 14, NLL = 118331.6141, Train RMSE = 0.6820, Test RMSE = 0.7738\n",
      "Iteration 15, NLL = 118251.1432, Train RMSE = 0.6818, Test RMSE = 0.7736\n",
      "Iteration 1, NLL = 159933.1746, Train RMSE = 0.8268, Test RMSE = 0.8713\n",
      "Iteration 2, NLL = 131803.9569, Train RMSE = 0.7294, Test RMSE = 0.8192\n",
      "Iteration 3, NLL = 124918.5462, Train RMSE = 0.7046, Test RMSE = 0.7981\n",
      "Iteration 4, NLL = 122062.9687, Train RMSE = 0.6942, Test RMSE = 0.7887\n",
      "Iteration 5, NLL = 120534.3962, Train RMSE = 0.6887, Test RMSE = 0.7835\n",
      "Iteration 6, NLL = 119595.9660, Train RMSE = 0.6854, Test RMSE = 0.7804\n",
      "Iteration 7, NLL = 118970.1329, Train RMSE = 0.6832, Test RMSE = 0.7783\n",
      "Iteration 8, NLL = 118529.3121, Train RMSE = 0.6817, Test RMSE = 0.7768\n",
      "Iteration 9, NLL = 118206.6717, Train RMSE = 0.6806, Test RMSE = 0.7758\n",
      "Iteration 10, NLL = 117963.6536, Train RMSE = 0.6798, Test RMSE = 0.7750\n",
      "Iteration 11, NLL = 117776.3201, Train RMSE = 0.6792, Test RMSE = 0.7744\n",
      "Iteration 12, NLL = 117629.1061, Train RMSE = 0.6787, Test RMSE = 0.7740\n",
      "Iteration 13, NLL = 117511.5627, Train RMSE = 0.6783, Test RMSE = 0.7736\n",
      "Iteration 14, NLL = 117416.4599, Train RMSE = 0.6780, Test RMSE = 0.7734\n",
      "Iteration 15, NLL = 117338.6354, Train RMSE = 0.6778, Test RMSE = 0.7732\n",
      "Iteration 1, NLL = 159383.2542, Train RMSE = 0.8247, Test RMSE = 0.8708\n",
      "Iteration 2, NLL = 130994.4015, Train RMSE = 0.7260, Test RMSE = 0.8184\n",
      "Iteration 3, NLL = 123930.3534, Train RMSE = 0.7004, Test RMSE = 0.7968\n",
      "Iteration 4, NLL = 121084.4045, Train RMSE = 0.6900, Test RMSE = 0.7876\n",
      "Iteration 5, NLL = 119619.4625, Train RMSE = 0.6847, Test RMSE = 0.7829\n",
      "Iteration 6, NLL = 118744.9757, Train RMSE = 0.6816, Test RMSE = 0.7801\n",
      "Iteration 7, NLL = 118171.3785, Train RMSE = 0.6796, Test RMSE = 0.7784\n",
      "Iteration 8, NLL = 117770.7377, Train RMSE = 0.6782, Test RMSE = 0.7772\n",
      "Iteration 9, NLL = 117477.9707, Train RMSE = 0.6772, Test RMSE = 0.7764\n",
      "Iteration 10, NLL = 117256.4613, Train RMSE = 0.6765, Test RMSE = 0.7757\n",
      "Iteration 11, NLL = 117084.0950, Train RMSE = 0.6759, Test RMSE = 0.7753\n",
      "Iteration 12, NLL = 116946.7873, Train RMSE = 0.6754, Test RMSE = 0.7749\n",
      "Iteration 13, NLL = 116835.2055, Train RMSE = 0.6751, Test RMSE = 0.7746\n",
      "Iteration 14, NLL = 116742.9823, Train RMSE = 0.6748, Test RMSE = 0.7743\n",
      "Iteration 15, NLL = 116665.6706, Train RMSE = 0.6745, Test RMSE = 0.7741\n",
      "Iteration 1, NLL = 160153.9264, Train RMSE = 0.8268, Test RMSE = 0.8761\n",
      "Iteration 2, NLL = 131596.4303, Train RMSE = 0.7273, Test RMSE = 0.8248\n",
      "Iteration 3, NLL = 123779.6656, Train RMSE = 0.6992, Test RMSE = 0.8005\n",
      "Iteration 4, NLL = 120732.3364, Train RMSE = 0.6880, Test RMSE = 0.7903\n",
      "Iteration 5, NLL = 119119.3415, Train RMSE = 0.6821, Test RMSE = 0.7847\n",
      "Iteration 6, NLL = 118136.1883, Train RMSE = 0.6786, Test RMSE = 0.7813\n",
      "Iteration 7, NLL = 117491.0975, Train RMSE = 0.6763, Test RMSE = 0.7792\n",
      "Iteration 8, NLL = 117044.0403, Train RMSE = 0.6747, Test RMSE = 0.7777\n",
      "Iteration 9, NLL = 116720.3874, Train RMSE = 0.6736, Test RMSE = 0.7767\n",
      "Iteration 10, NLL = 116477.8737, Train RMSE = 0.6728, Test RMSE = 0.7759\n",
      "Iteration 11, NLL = 116291.1495, Train RMSE = 0.6722, Test RMSE = 0.7753\n",
      "Iteration 12, NLL = 116144.1444, Train RMSE = 0.6717, Test RMSE = 0.7749\n",
      "Iteration 13, NLL = 116026.1854, Train RMSE = 0.6713, Test RMSE = 0.7745\n",
      "Iteration 14, NLL = 115929.9482, Train RMSE = 0.6710, Test RMSE = 0.7743\n",
      "Iteration 15, NLL = 115850.2945, Train RMSE = 0.6707, Test RMSE = 0.7740\n",
      "Iteration 1, NLL = 158851.0176, Train RMSE = 0.8225, Test RMSE = 0.8729\n",
      "Iteration 2, NLL = 130149.4533, Train RMSE = 0.7216, Test RMSE = 0.8222\n",
      "Iteration 3, NLL = 122943.6390, Train RMSE = 0.6954, Test RMSE = 0.7999\n",
      "Iteration 4, NLL = 119919.5854, Train RMSE = 0.6843, Test RMSE = 0.7898\n",
      "Iteration 5, NLL = 118338.3044, Train RMSE = 0.6786, Test RMSE = 0.7844\n",
      "Iteration 6, NLL = 117399.0539, Train RMSE = 0.6752, Test RMSE = 0.7813\n",
      "Iteration 7, NLL = 116788.1009, Train RMSE = 0.6730, Test RMSE = 0.7793\n",
      "Iteration 8, NLL = 116363.3972, Train RMSE = 0.6715, Test RMSE = 0.7780\n",
      "Iteration 9, NLL = 116053.3262, Train RMSE = 0.6705, Test RMSE = 0.7770\n",
      "Iteration 10, NLL = 115818.3891, Train RMSE = 0.6697, Test RMSE = 0.7762\n",
      "Iteration 11, NLL = 115635.1683, Train RMSE = 0.6690, Test RMSE = 0.7757\n",
      "Iteration 12, NLL = 115488.9747, Train RMSE = 0.6685, Test RMSE = 0.7752\n",
      "Iteration 13, NLL = 115370.1768, Train RMSE = 0.6681, Test RMSE = 0.7748\n",
      "Iteration 14, NLL = 115272.2164, Train RMSE = 0.6678, Test RMSE = 0.7745\n",
      "Iteration 15, NLL = 115190.4718, Train RMSE = 0.6675, Test RMSE = 0.7743\n",
      "Iteration 1, NLL = 158513.8072, Train RMSE = 0.8211, Test RMSE = 0.8730\n",
      "Iteration 2, NLL = 129722.8718, Train RMSE = 0.7195, Test RMSE = 0.8234\n",
      "Iteration 3, NLL = 122539.4189, Train RMSE = 0.6934, Test RMSE = 0.8015\n",
      "Iteration 4, NLL = 119459.0298, Train RMSE = 0.6820, Test RMSE = 0.7913\n",
      "Iteration 5, NLL = 117810.9582, Train RMSE = 0.6759, Test RMSE = 0.7857\n",
      "Iteration 6, NLL = 116814.5733, Train RMSE = 0.6723, Test RMSE = 0.7824\n",
      "Iteration 7, NLL = 116160.4513, Train RMSE = 0.6700, Test RMSE = 0.7802\n",
      "Iteration 8, NLL = 115706.1502, Train RMSE = 0.6684, Test RMSE = 0.7786\n",
      "Iteration 9, NLL = 115376.9420, Train RMSE = 0.6673, Test RMSE = 0.7775\n",
      "Iteration 10, NLL = 115129.8331, Train RMSE = 0.6664, Test RMSE = 0.7767\n",
      "Iteration 11, NLL = 114938.6836, Train RMSE = 0.6657, Test RMSE = 0.7761\n",
      "Iteration 12, NLL = 114787.0255, Train RMSE = 0.6652, Test RMSE = 0.7756\n",
      "Iteration 13, NLL = 114664.1694, Train RMSE = 0.6648, Test RMSE = 0.7752\n",
      "Iteration 14, NLL = 114562.9694, Train RMSE = 0.6645, Test RMSE = 0.7749\n",
      "Iteration 15, NLL = 114478.5029, Train RMSE = 0.6642, Test RMSE = 0.7746\n",
      "Iteration 1, NLL = 158485.1065, Train RMSE = 0.8205, Test RMSE = 0.8748\n",
      "Iteration 2, NLL = 128864.8444, Train RMSE = 0.7156, Test RMSE = 0.8232\n",
      "Iteration 3, NLL = 121513.0042, Train RMSE = 0.6888, Test RMSE = 0.8005\n",
      "Iteration 4, NLL = 118600.4599, Train RMSE = 0.6781, Test RMSE = 0.7910\n",
      "Iteration 5, NLL = 117078.5038, Train RMSE = 0.6725, Test RMSE = 0.7860\n",
      "Iteration 6, NLL = 116147.8004, Train RMSE = 0.6691, Test RMSE = 0.7829\n",
      "Iteration 7, NLL = 115524.6439, Train RMSE = 0.6669, Test RMSE = 0.7808\n",
      "Iteration 8, NLL = 115081.9325, Train RMSE = 0.6654, Test RMSE = 0.7794\n",
      "Iteration 9, NLL = 114753.1437, Train RMSE = 0.6642, Test RMSE = 0.7783\n",
      "Iteration 10, NLL = 114500.1776, Train RMSE = 0.6633, Test RMSE = 0.7775\n",
      "Iteration 11, NLL = 114299.9768, Train RMSE = 0.6627, Test RMSE = 0.7768\n",
      "Iteration 12, NLL = 114137.9832, Train RMSE = 0.6621, Test RMSE = 0.7763\n",
      "Iteration 13, NLL = 114004.6546, Train RMSE = 0.6616, Test RMSE = 0.7758\n",
      "Iteration 14, NLL = 113893.5037, Train RMSE = 0.6613, Test RMSE = 0.7755\n",
      "Iteration 15, NLL = 113799.9470, Train RMSE = 0.6609, Test RMSE = 0.7751\n",
      "Iteration 1, NLL = 157966.8839, Train RMSE = 0.8187, Test RMSE = 0.8737\n",
      "Iteration 2, NLL = 128238.5512, Train RMSE = 0.7126, Test RMSE = 0.8237\n",
      "Iteration 3, NLL = 121196.6760, Train RMSE = 0.6869, Test RMSE = 0.8026\n",
      "Iteration 4, NLL = 118247.2621, Train RMSE = 0.6760, Test RMSE = 0.7930\n",
      "Iteration 5, NLL = 116640.8870, Train RMSE = 0.6701, Test RMSE = 0.7876\n",
      "Iteration 6, NLL = 115644.8798, Train RMSE = 0.6666, Test RMSE = 0.7843\n",
      "Iteration 7, NLL = 114978.8480, Train RMSE = 0.6642, Test RMSE = 0.7820\n",
      "Iteration 8, NLL = 114508.7131, Train RMSE = 0.6625, Test RMSE = 0.7805\n",
      "Iteration 9, NLL = 114162.6653, Train RMSE = 0.6613, Test RMSE = 0.7793\n",
      "Iteration 10, NLL = 113899.4361, Train RMSE = 0.6604, Test RMSE = 0.7785\n",
      "Iteration 11, NLL = 113693.8766, Train RMSE = 0.6597, Test RMSE = 0.7778\n",
      "Iteration 12, NLL = 113529.8587, Train RMSE = 0.6591, Test RMSE = 0.7773\n",
      "Iteration 13, NLL = 113396.6053, Train RMSE = 0.6587, Test RMSE = 0.7768\n",
      "Iteration 14, NLL = 113286.6884, Train RMSE = 0.6583, Test RMSE = 0.7764\n",
      "Iteration 15, NLL = 113194.8627, Train RMSE = 0.6580, Test RMSE = 0.7761\n"
     ]
    }
   ],
   "source": [
    "lam = 0.017684422019387584\n",
    "gamma = 0.06224262766516858\n",
    "tau = 0.29037492246228525\n",
    "\n",
    "Ks = list(range(4, 26))\n",
    "Final_RMSEs = []\n",
    "\n",
    "for k in Ks:\n",
    "    user_bias = np.zeros(n_users)\n",
    "    item_bias = np.zeros(n_items)\n",
    "    user_embedding = np.random.normal(scale=0.1, size=(n_users, k))\n",
    "    item_embedding = np.random.normal(scale=0.1, size=(n_items, k))\n",
    "    feature_embedding = np.random.normal(scale=0.11, size=(n_features, k))\n",
    "    mu_train = train_data[:, 2].mean()\n",
    "\n",
    "    nll_history = []\n",
    "    train_rmse_history = []\n",
    "    test_rmse_history = []\n",
    "    iterations = 15\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        # Update user bias\n",
    "        user_bias = Parallel(n_jobs=-1)(\n",
    "            delayed(update_user_bias)(\n",
    "                user,\n",
    "                mu_train,\n",
    "                data_by_user_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "            )\n",
    "            for user in range(n_users)\n",
    "        )\n",
    "        user_bias = np.array(user_bias)\n",
    "\n",
    "        # Update user embeddings\n",
    "        user_embedding = Parallel(n_jobs=-1)(\n",
    "            delayed(update_user_embedding)(\n",
    "                user,\n",
    "                mu_train,\n",
    "                data_by_user_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "            )\n",
    "            for user in range(n_users)\n",
    "        )\n",
    "        user_embedding = np.vstack(user_embedding)\n",
    "\n",
    "        # Update item bias\n",
    "        item_bias = Parallel(n_jobs=-1)(\n",
    "            delayed(update_item_bias)(\n",
    "                item,\n",
    "                mu_train,\n",
    "                data_by_movie_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "            )\n",
    "            for item in range(n_items)\n",
    "        )\n",
    "        item_bias = np.array(item_bias)\n",
    "\n",
    "        # Update item embeddings\n",
    "        item_embedding = Parallel(n_jobs=-1)(\n",
    "            delayed(update_item_embedding)(\n",
    "                item,\n",
    "                mu_train,\n",
    "                data_by_movie_train,\n",
    "                lam,\n",
    "                gamma,\n",
    "                tau,\n",
    "                item_bias,\n",
    "                user_bias,\n",
    "                item_embedding,\n",
    "                user_embedding,\n",
    "                item_features,\n",
    "                feature_embedding,\n",
    "            )\n",
    "            for item in range(n_items)\n",
    "        )\n",
    "        item_embedding = np.vstack(item_embedding)\n",
    "\n",
    "        # Update feature embeddings\n",
    "        feature_embedding = update_feature_embedding(\n",
    "            item_features, item_embedding, tau, lam=lam\n",
    "        )\n",
    "\n",
    "        # Compute train squared error\n",
    "        uv_train = np.sum(\n",
    "            user_embedding[train_coo.row] * item_embedding[train_coo.col], axis=1\n",
    "        )\n",
    "        pred_train = user_bias[train_coo.row] + item_bias[train_coo.col] + uv_train + mu_train\n",
    "        diff_train = train_coo.data - pred_train\n",
    "        squared_error = np.sum(diff_train**2)\n",
    "\n",
    "        # Regularization terms\n",
    "        reg_user_bias = np.sum(user_bias**2)\n",
    "        reg_item_bias = np.sum(item_bias**2)\n",
    "        reg_user_emb = np.sum(user_embedding**2)\n",
    "\n",
    "        reg_item_feature = 0.0\n",
    "        for n in range(n_items):\n",
    "            feats = item_features[n]\n",
    "            F_n = np.sum(feats)\n",
    "            if F_n == 0:\n",
    "                s_n = np.zeros(k)\n",
    "            else:\n",
    "                s_n = (feats @ feature_embedding) / np.sqrt(F_n)\n",
    "            reg_item_feature += np.sum((item_embedding[n] - s_n) ** 2)\n",
    "\n",
    "        reg_feature_emb = np.sum(feature_embedding**2)\n",
    "\n",
    "        nll = (\n",
    "            (lam / 2) * squared_error\n",
    "            + (gamma / 2) * (reg_user_bias + reg_item_bias)\n",
    "            + (tau / 2) * (reg_user_emb + reg_item_feature + reg_feature_emb)\n",
    "        )\n",
    "        nll_history.append(nll)\n",
    "\n",
    "        # Train RMSE\n",
    "        train_rmse = np.sqrt(squared_error / len(train_coo.data))\n",
    "        train_rmse_history.append(train_rmse)\n",
    "\n",
    "        # Test RMSE\n",
    "        uv_test = np.sum(\n",
    "            user_embedding[test_coo.row] * item_embedding[test_coo.col], axis=1\n",
    "        )\n",
    "        pred_test = user_bias[test_coo.row] + item_bias[test_coo.col] + uv_test + mu_train\n",
    "        diff_test = test_coo.data - pred_test\n",
    "        test_rmse = np.sqrt(np.sum(diff_test**2) / len(test_coo.data))\n",
    "        test_rmse_history.append(test_rmse)\n",
    "\n",
    "        print(\n",
    "            f\"Iteration {iteration + 1}, \"\n",
    "            f\"NLL = {nll:.4f}, \"\n",
    "            f\"Train RMSE = {train_rmse:.4f}, \"\n",
    "            f\"Test RMSE = {test_rmse:.4f}\"\n",
    "        )\n",
    "\n",
    "    Final_RMSEs.append(test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHqCAYAAACZcdjsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACv0klEQVR4nOzdeVxU5f4H8M+ZGXZhEAFBQARcUFEUcN9zyy21srIyNa3UFstuXSu7pj+71i0tK/VauaSmmdlmpVfNNc0FQc1dARcWFVAGlHVmzu8PmpGRbQbmMHOGz/v18iXzzJkz34f5MMzDOed5BFEURRAREREREdWCwtYFEBERERGR/HFgQUREREREtcaBBRERERER1RoHFkREREREVGscWBARERERUa1xYEFERERERLXGgQUREREREdUaBxZERERERFRrHFgQEREREVGtcWBBDmnt2rXo0KEDGjRoAEEQ8M477xjv++ijj9C6dWu4ublBEASsWrXKZnVa26FDh3DfffehUaNGEAQBffv2tXVJ9cY777wDQRCwe/duW5ciub59+0IQBFuXQTK1e/fucu/LNdGsWTM0a9bMKjVJTaPR4IUXXkBoaChUKhUEQcClS5dsXRaR1XFgQXbt0qVLEAShyn8dOnQwecyBAwcwbtw45Ofn4/nnn8fs2bONH7DXrVuHGTNmwN3dHa+88gpmz55d7vHWVlcfODUaDUaMGIGEhAQ8/vjjmD17NiZMmFDlYyZMmABBEHDw4EGr1mJ43ap7fqnU9IPLqlWrTLKlUCjg5eWFsLAwjBw5Ep9++ilu3rwpTdEka3l5ecY/ZPzjH/+odDtDNqdMmWLWfk+ePInx48ejWbNmcHFxgVqtRvPmzfHggw9i0aJFEEXRrP0YMu3m5oacnJwKt8nOzoaLiwsEQYCrq6tZ+5ULw/e97D9XV1eEh4fjmWeekfxD/muvvYbFixejQ4cOePPNNzF79mx4e3tL+pxEtqCydQFE5oiIiMCTTz5Z4X0BAQEmt3/77TcAwOrVq9G1a9cK7/v111/LPU7ujhw5gszMTMyfPx8zZ860dTmy1r9/f/Ts2RMAcPv2baSmpmLfvn34+eefMXv2bCxbtgxjxowxecwLL7yAxx57DE2bNrVFyXVq9erVyM/Pt3UZdmXDhg24c+cOBEHA6tWrMX/+fDg5OdVqn9u3b8fw4cOh1WrRv39/jB49GgCQnJyM/fv344cffsDzzz8Plcq8X+UqlQqFhYVYt24dpk2bVu7+NWvWoLi42Oz9yVFsbCyGDx8OAMjJycHu3bvx5ZdfYtOmTTh8+DCaN28uyfP+9ttvaNWqFX766SdJ9k9kLxz33YMcSvPmzc3+63N6ejqA8gOO6u6TO0fuW10bMGBAucGZTqfDqlWr8OKLL2Ls2LFQq9UYNGiQ8X5fX1/4+vrWdak2UR8GT5Zavnw5XFxc8Oyzz+LTTz/F5s2b8eCDD9Zqn1OnToVOp8OOHTvQr18/k/tEUcS2bdugVCrN3l9ERAREUcSKFSsqHFisXLkS7du3h0ajwbVr12pVu72Ki4sz+V0iiiLGjx+PNWvW4N1338XKlSsled709HT07t1bkn0T2ROeCkUOw3Co2/CLISwszHjI23CKy65duwDcPS3g3vNz9+7dixEjRsDX1xcuLi5o0aIFZs2aVelfZ/ft24fRo0ejcePGcHFxQUhICB588EH88ccfAErPRZ8zZw4AoF+/fpU+b2WuXLmCSZMmISgoCM7OzggODsakSZNw9epVk+0EQcD48eMBABMnTjQ+jzVPv/rhhx8wduxYNG/eHO7u7lCr1ejVqxc2bdpkst2qVasQFhYGAPjqq69MTj0oW4/hA06PHj3g5eUFd3d3xMXFYcWKFeWeu+zpZN9++y1iYmLg5uaGwMBAvPTSSygoKDDZ1vAhbM6cOSbPX5vTHZRKJSZNmoT//ve/0Ol0mDFjhslpKBWd8lb2lLAzZ85g+PDh8Pb2RsOGDTF27FhkZWUBKL02ZuDAgfDy8kLDhg3xzDPP4M6dOxXWYW5Gy54OlpCQgMGDB8PT0xNqtRqjR4+u8HuRkJCAhx9+GE2bNoWLiwsaN26Mbt264b333jPZrrJrLLRaLT766CNER0fDzc0NarUa/fr1w6+//lpuW8PP5KpVq/D777+jZ8+e8PDwQKNGjTB+/HhkZ2dX+lpU5MCBAxg2bBh8fHzg6uqKyMhIvPPOOxX+7BquP8rMzMTTTz8Nf39/uLm5oWvXrjX6mTl9+jQOHjyI4cOHY/r06QBKBxq1cePGDSQlJSEqKqrcoMLQh8GDB1t8rcuECRNw9OhRnDhxwqQ9Pj4eJ06cwMSJEyt9rCWvLwAUFBRg5syZCAkJgaurK6KiovDFF19UWV9KSgomT55szGBgYCAmTJiAy5cvW9RPcwmCgOeffx5A6VFfg5q+P3311VeIjY2Fu7s7+vbtazzdVBRF7Nmzx/heVPY00fz8fLzzzjuIjIyEq6srfHx8MGzYMBw4cMCi5wLu/mwWFRXhzTffRNOmTeHm5obY2Fjs2LEDQOlpey+99BKCgoLg6uqKbt26IT4+vtxz7dq1C08//TRatWqFBg0aoEGDBoiLi8Pnn39e6ffS0p+rvLw8zJ07F+3bt4eHhwfUajU6duyIt99+GyUlJSbb1nU2qGZ4xIIcRrNmzTB79mz8+OOPOH78OKZPn248h7VDhw6YPXs2Vq1ahcuXL2P27NkAYHKO63//+19MmzYNDRs2xIgRI+Dn54cjR47g3Xffxa5du7Br1y44Ozsbt1+8eDFefPFFuLm5YfTo0WjatCnS0tLwxx9/4LvvvkPPnj2Nvzz27NljPE/63uetzIULF9CzZ0/cuHEDI0aMQNu2bXHq1CmsWLECv/zyC/bv3288bD979mwcO3YMP/30E0aOHGm8bsSaFza+8cYbcHZ2Rs+ePREYGIjMzEz8/PPPePjhh/HJJ5/gxRdfBFD6vZ4+fToWLVqE6OhojBo1yrgPQz2iKOLJJ5/EunXr0LJlSzz++ONwdnbG9u3bMWnSJJw+fRoffvhhuRoWL16MLVu2YOTIkejbty+2bt2KTz/9FNnZ2fj6668BlP5ivXTpEr766iv06dPH5AJ2a5zT/OSTT2L27Nk4deoUTp48iXbt2lX7mJSUFHTv3h1xcXGYPHky4uPj8c033+Dq1at4//33MXDgQAwcOBDPPvus8dQMAOU+hFmaUaD0A+MHH3yAvn374rnnnkNiYiJ+/PFH/PXXXzh58qTxXPpjx46he/fuUCqVGDlyJEJDQ5GTk4NTp07hiy++qPb0OlEU8eijj+L7779Hy5Yt8fzzz+POnTv49ttvMXz4cCxatAgvvfRSucdt3rwZv/zyC0aMGIGpU6di7969WL16NZKSkowD9Ops2rQJjz32GJydnfHoo4/C398fO3bswJw5c7Bt2zbs2rULLi4uJo/Jyckxfmh84okncOPGDWzYsAGDBw/G0aNHERUVZdZzA3cHEU899RQiIiLQvXt3/O9//0NaWhqCgoLM3k9ZarUaSqUSGRkZuHPnDjw8PGq0n3uNHz8eb7/9NlauXImPPvrI2L5ixQo4OzvjySefxMcff1zucZa+vnq9Hg888AB27NiBdu3a4fHHH0d2djZeeeWVCgdKQOkAe/Dgwbhz5w5GjBiB5s2b49KlS/j666+xZcsW/PnnnwgPD7fK96EqNX1/+uCDD7Br1y488MADGDhwIFQqFeLi4tCsWTPMmTMHoaGhxt8JhvfooqIi9O/fHwcPHkRMTAxefvllYxa3bduGDRs2VHjkq6LnKuvRRx/FX3/9hQceeAAFBQX4+uuvMXz4cBw4cADPPfccCgsL8fDDDyMzM9OY+5SUFHh5eRn38f777+PixYvo2rUrRo8ejZycHGzduhXPPfcczp07hwULFpSry5Kfq6ysLPTp0wenT59Ghw4dMGXKFOj1epw9exbvv/8+Xn31VeN7tr1kg8wgEtmxlJQUEYAYEREhzp49u8J/W7ZsMXnM+PHjRQBiSkpKuf316dNHrCj2p06dElUqldixY0cxOzvb5L758+eLAMQPP/zQ2HbixAlRqVSKTZo0Kfc8er1eTEtLM96ePXu2CEDctWuXRX2/7777RADismXLTNqXLVsmAhD79+9v0r5y5UoRgLhy5Uqzn8Pwvfrzzz+r3TYpKalcW15entiuXTtRrVaLd+7cMbYbXrfx48dXuK/PP/9cBCBOmjRJLCkpMbYXFRWJI0aMEAGI8fHxxnbD91CtVotnz541tufn54stW7YUBUEw+Z7v2rVLBCDOnj272n6VZfgezp8/v8rtxo0bJwIQly9fXq7Gsq+z4fsAQPz444+N7Xq9Xhw6dKgIQPT29hZ//PFH433FxcVi+/btRScnJ/HatWvGdkszavgeABC/+eabCutfv369sW3GjBkiAPGnn34q19+srCyT2xX9HK1evVoEIPbp00csKioytl+9elX09/cXnZycxOTkZGO74XutUqnEP/74w9iu1WrFvn37mp3L3Nxc0dvbW3RxcRGPHz9ubNfr9eLjjz8uAhD/7//+z+Qxhu/LtGnTRJ1OZ2z/8ssvRQDic889V+3zGhQXF4t+fn5io0aNxOLiYlEU7/6Mzps3r9z2htfFnOcYNWqUCEDs0KGDuGTJEvHYsWPG57AUALFVq1aiKIri0KFDRV9fX+O+CgoKRG9vb/Ghhx4SRVEUQ0NDRRcXF5PH1/T1vf/++0WtVmtsP3HihOjs7Fzu57O4uFhs1qyZ6OnpKR47dszkufft2ycqlUpx+PDhJu2hoaFiaGioWf2v7Puu1+vFJ598UgQgTpgwQRTFmr8/eXh4iCdOnKjw+Q3fu3vNnTtXBCA+8cQTol6vN7YfP35cdHFxERs2bCjm5uaa/VyGn80ePXqIt2/fNrZ/8803xvebMWPGmPTr/fffFwGICxcuNNlX2dfToKSkRBw4cKCoVCrFy5cvl+ujJT9XY8aMEQGIb775ZrnnuXbtmrHGmmSDbIcDC7JrZT+YVfZv+vTpJo+pycDipZdeEgGI+/btK3efTqcT/fz8xNjYWGPbtGnTRADiihUrqu1DTQYWV65cEQGIbdq0MfllI4qlvwhbt24tAhCvXLlibJd6YFGZBQsWiADE3bt3G9uqG1i0b99e9PDwEAsKCsrdd+LECRGA+OqrrxrbDN/Df/3rX+W2N9z3888/G9ukHlj885//FAGI77//frk6KhpYhIeHm/yiFcW7H9T69etXbv+GDxtl92VpRg3fg969e5fb3nDfjBkzjG2GgcW2bduq7LsoVvxzZBgIHzp0qNz2hoFP2Q/4hu/1U089VW57w32ffPJJtbUYvo9Tp04td9+VK1dElUolRkREmLQbPpjl5eWZtJeUlIgqlUqMiYmp9nkNvvvuOxGA+Pzzzxvbbt26Jbq6uorh4eHlfn4tGVhkZmaKw4YNM3m/c3Z2Frt37y4uWrRIzM/PN7vOsgMLQ83fffedKIqiuHbtWhGA+Ouvv4qiWPHAwtLXt1+/fiIA8ejRo+W2nzRpUrmfz++//77CQaDBgw8+KCoUClGj0RjbajKwiI2NNf5R6uWXXxajo6NFAKKPj4944cIFURRr/v70yiuvVPr8lQ0swsPDRScnJ/Hq1avl7nvuuedEAOKaNWvMfi7Dz2bZ92NRLB2wOzk5iQDKDQgMv28qe7++16ZNm0QA4qpVq8r10dyfq2vXromCIIgRERHVDpZrkg2yHZ4KRbIwePBgbN26VbL9G6Zb3bp1q/E81LKcnJxw9uxZ4+3Dhw8DgMnFu9aUmJgIAOjTp0+5c6gFQUDv3r1x5swZHD9+HCEhIZLUcK8bN27gvffew5YtW3D58mWT6xqAuxePVyc/Px9//fUXmjRpUu7cfQDG82rLfr8NYmJiyrUFBwcDQKVTaEpBNHOKT4Po6GgoFKaXtAUGBgJAhdMdG+5LS0sztlmaUQNzv2cPP/wwPv74Y4waNQqPPPIIBg4ciJ49e5p9oXZiYiLc3NzQuXPncvcZTkc7duxYjeur6nnLPkdZISEhiIiIwLlz55CXlwdPT0/jfS1atECDBg1MtlepVGjcuLFFWTKcBjVu3Dhjm7e3N0aMGIGNGzdiz549NV5PxtfXF7/88gvOnz+P//3vfzh8+DAOHjyIAwcO4MCBA/jiiy+wZ88e+Pj4WLTfBx54AL6+vlixYgUeeughrFixAk2aNMHgwYMrfYylr+/x48fh7u5e4evbq1evctegGPJ99uzZCifquHbtGvR6Pc6fP4+4uDgzelmxo0eP4ujRowAAZ2dnBAUF4ZlnnsFbb72F0NDQWr0/VfS9qUpubi6Sk5PRunVrY+bL6tu3L5YtW4Zjx46VmxWxuufq2LGjyW2lUgl/f3/cuXOn3M90Re83QOn1Dx9++CF+/PFHJCUllbvuq6L3fHN/ruLj4yGKIvr161ft7Gl1lQ2yDg4siADj2gTvvvuuWdvn5ORAEATjG7K15ebmAgAaN25c4f2GmZ80Go0kz3+vmzdvolOnTrhy5Qp69OiBAQMGwNvbG0ql0nhtR1FRkVn7unXrFkRRRFpamvHC9opUdPGyWq0u12Y4t1in05nZm9rLyMgAAPj5+Zm1fdnzlg0MdVd1X9mLFy3NqIG537Nu3bph586dmD9/PtavX29cODI2NhYffPBBpefFG+Tm5lY6yK0qr7V9Tc35WTl37hxyc3NNBhYVPa/huc3NUlpaGrZt24YWLVqgS5cuJveNHz8eGzduxPLly2u9UGXLli3RsmVL423DB82TJ09izpw5WLRokUX7c3JywhNPPIHPPvsMBw4cwK5du/DPf/6zyhmmLH19NRpNpdtX9FoZ8m24VqoylU1qYK7nnnsO//3vfyu9vzbvT5VlsDK1eZ+v7rkqe1+p6uet7PtNcXEx+vbti4SEBHTs2BHjxo1Do0aNoFKpjNewVfSeb+7PlWGQYc41SHWVDbIODiyIcPdN+N4PH5Xx9vaGKIrIyMio8cWZ5tRz/fr1Cu83tFf0y0MKy5cvx5UrVzBv3jy89dZbJve99957Fs3Nbqg5Nja2wplI7J1er8fevXsBAJ06daqz57U0ozXRp08f9OnTBwUFBTh06BA2b96MJUuWYNiwYfjrr78QERFRZX22yKstf1ZWrVoFnU6HCxcuVDo706ZNm/DZZ59V+oGrJjp06IBPP/0U9913H3bu3FmjfUyaNAmLFi3CI488AlEU8fTTT1e5vaWvr1qtxo0bN6rc/t79A6UX8xvWmbCF2rw/WTpDV22ya+lzWeqnn35CQkICJk+eXG4SiW+++QZfffVVrfZvuCj73qMkFbGXbJB5ON0sEWD8a6O5K1AbDkNv27at2m0NfwW05C/qhtNj9u7dW+60G1EUsW/fPpPtpJaUlASg9BSKexlqKauqPnt6eqJ169Y4c+aMZKcv1eR7bq41a9bg8uXLaNeuHdq2bWv1/VfG0ozWhpubG/r27YsFCxbgzTffREFBQYWnX5XVsWNHFBQUGE8TLGvPnj0ApMmr4ZSPiqazTEtLQ1JSEsLDw60+GBNFEStXroQgCJg4cSImTZpU7l+XLl1QUFCAdevWWfW5AdR6lqh27dohNjYWaWlp6NmzJ1q0aFHl9pa+vtHR0cjPz0dCQkK57St6zzDk+88//7SkG1ZXF+9PBl5eXggPD8fFixcr/IAt5c9NdSx9z7dUXFwcFAoFdu3aVW5a2XvZSzbIPBxYEAGYNm0aVCoVXnzxxXJrRAClh20N53IDwJQpU6BUKjFr1qxyc2gbjmQYGM5/Tk1NNbuepk2bol+/fsbpZctasWIFTp06hfvuu6/Orq8IDQ0FgHLTf65bt864mnlZDRs2hCAIlfb5pZdeQn5+fqXrNaSkpNRqzYmafM+ro9PpsGLFCkydOhVKpRILFy6U/K+GZVmaUUvt27fPeGpGWYa/mrq5uVX5eMM6Km+88YbJB4W0tDQsXLgQKpUKTzzxRI3rq8zIkSOhVquxcuVKnDp1ytguiqKxlrJrBljL7t27kZSUhN69e2PFihX48ssvy/0zzPdfkzUt7ty5g3fffde41klZWq0W//nPfwDAuEJ8TXz11Vf44Ycfql1bArD89TVcc/LWW2+ZDPD/+usvrFmzptz+R44ciaZNm2LhwoXGI4JllZSUmD39cG1J/f5U1vjx41FSUoI33njD5I9IJ0+exMqVK6FWq02m7K4rlb3n79mzx6y8VKdx48Z46KGHkJSUVOEpZzdu3IBWqwVgX9mg6vFUKJKFixcvVrnytrmrclcmKioKS5YswdSpU9GqVSsMHToUERERxovr9uzZgwkTJhjPzW3Xrh0+/vhjvPTSS2jbti1GjRqF0NBQXLt2DXv37sWwYcOMc8EbFsZ76623cPbsWajVaqjVakydOrXKmpYuXYqePXvimWeewebNm9GmTRucPn0aP//8M/z8/LB06dJa9bms//u//6v0eoG5c+di3LhxeP/99/Hiiy9i165dCA0NxYkTJ7Bjxw48+OCD+P77700e06BBA3Tq1Al79+7FxIkT0aJFCygUCjz++ONo2rQpnnvuORw8eBBfffUV9u/fjwEDBqBJkya4fv06zp49i0OHDmHdunU1XocjMjISTZo0wTfffAN3d3cEBwdDEARMnTrVrFNSduzYgcLCQgClF5unpqZi7969SEtLg4+PD9asWYMBAwbUqLaasjSjllqwYAG2b9+Ofv36ITw8HK6urkhISMDvv/+O5s2bY/To0VU+fty4cfj+++/x008/oX379hg+fLhxnYPs7GwsWLBAknnmvby88MUXX2Ds2LHo0qULHn30Ufj5+eH3339HfHw8OnfujNdee83qz2sYLFR1ClH79u0RExODo0eP4vjx44iOjjbet2vXrkoHPIMGDcLQoUMxa9YsvPPOO+jWrRuio6ONpyNt3boVaWlpCAsLM67JUxNt27Y1+6ibpa/v+PHjsW7dOmzduhUdO3bEkCFDcPPmTaxfvx6DBg3CL7/8YrJ/FxcXfPfddxgyZAj69OmD/v37G9c8uHLlCvbt24dGjRpVeNG0tUn9/lTW66+/jl9//RVr1qzBmTNn0L9/f+PaEiUlJVi9erVkpz5WZcSIEWjWrBn+85//4OTJk4iKisK5c+fwyy+/YNSoUeUWRq2JJUuW4OTJk3j33Xfx22+/4b777oMoijh//jy2bduG69evw9vb266yQWawyVxURGYyZ7rZe2Nck+lmDQ4fPiw+9thjYpMmTUQnJyfR19dXjImJEWfOnCmeOXOm3Pa7du0Shw8fLvr4+IjOzs5icHCw+NBDD4n79+832W7VqlViu3btRBcXFxGA2VMkXrp0SZw4caIYGBgoqlQqMTAwUJw4caJ46dKlctvWZrrZqv4lJiaKoiiKx44dEwcNGiQ2bNhQ9PT0FPv06SPu2LGj0uc9d+6cOHToUNHb21sUBKHCKXc3bNggDhgwQGzYsKHo5OQkBgUFiX379hUXLFggZmZmGrerasreyp7/4MGDYp8+fURPT09jXyrKREX7MvwTBEFs0KCB2KxZM3HEiBHip59+Kt68ebPCx1Y13WxF0zhWNSVuVa+luRmtav8V1bV161bxqaeeElu1aiV6enqKDRo0ENu0aSPOmjXLrHUsRLF0WskPP/zQmHVDTipaG6OqPtZkuuC9e/eKQ4YMEb29vUVnZ2exZcuW4ttvv20yl78BKpn6UxTNm8I0JydHdHNzEz09PU3Wb6nIp59+KgIQX3zxRVEUTdcXqezf9OnTRZ1OJ/7222/i9OnTxdjYWLFx48aiSqUSvby8xLi4OHHOnDliTk6OWd8bQ58N081Wp6LpZkXRstdXFEXxzp074uuvvy4GBQWJLi4uYps2bcRly5ZV+fqmpqaK06dPF1u0aCG6uLiIXl5eYuvWrcXJkyeLv//+e7k6a7uORVWs8f5kUFXmbt++Lb799ttiy5YtRWdnZ9Hb21scMmRIhVNLV/dcVf2Oq+r7VVF9ycnJ4kMPPST6+fmJ7u7uYqdOncRvvvmm0tevJj9XGo1GfPvtt8XIyEjRxcVFVKvVYocOHcR//etf5aahtSQbZDuCKFo4byIREREREdE9eI0FERERERHVGgcWRERERERUaxxYEBERERFRrXFgQUREREREtcaBBRERERER1RoHFkREREREVGtcIK8W9Ho90tPT4enpWacr8BIRERER1QVRFJGXl4cmTZpAoaj6mAQHFrWQnp6OkJAQW5dBRERERCSpq1evIjg4uMptOLCoBU9PTwCl32gvLy8bV1N3tFotEhMT0bFjR6hUjBBZF/NFUmG2SErMF0nF1tnKzc1FSEiI8XNvVZj8WjCc/uTl5VXvBhYeHh7w8vLimydZHfNFUmG2SErMF0nFXrJlzmn/giiKYh3U4pByc3OhVquh0Wjq1cBCFEXodDoolUpeW0JWx3yRVJgtkhLzRVKxdbYs+bzLWaGoRoqLi21dAjkw5oukwmyRlJgvkopcssWBBVlMp9PhxIkT0Ol0ti6FHBDzRVJhtkhKzBdJRU7Z4sCCiIiIiIhqjQMLIiIiIiKqNQ4sqEaUSqWtSyAHxnyRVJgtkhLzRVKRS7Y4K1Qt1NdZoYiIiIiofuCsUCQpURSRk5MDjklJCswXSYXZIikxXyQVOWWLAwuymE6nw9mzZ2UxOwHJD/NFUmG2SErMF0lFTtniwIKIiIiIiGqNAwsiIiIiIqo1la0LoJrR6UUcTrmJG3mF8Pd0RecwHygVdbPMuyAIcHNzs8my8uT4mC+SCrNFUmK+SCpyyhZnhaoFW80KtfVkBuZsPo0MTaGxLVDtitkj2uD+qMA6q4OIiIiIHBtnhXJgW09mYOraBJNBBQBc0xRi6toEbD2ZIXkNer0eN27cgF6vl/y5qP5hvkgqzBZJifkiqcgpWxxYyIhOL2LO5tOo6BCToW3O5tPQ6aU9CKXX65GcnCyLgJP8MF8kFWaLpMR8kVTklC0OLGTkcMrNckcqyhIBZGgKcTjlZt0VRUREREQEDixk5UZe5YOKmmxHRERERGQtHFjIiL+nq1W3qylBEKBWq2UxOwHJD/NFUmG2SErMF0lFTtnirFC1UNezQun0Inq+vxPXNIUVXmchAAhQu+KPf95XZ1PPEhEREZHj4qxQDkqpEDB7RBsApYOIe4kAZo9oI/mgQq/XIzU1VRYXEZH8MF8kFWaLpMR8kVTklC0OLGTm/qhALH0yBgHq8qc79WnpVyfrWMgp4CQ/zBdJhdkiKTFfJBU5ZYsrb8vQ/VGBGNgmAIdTbuJy9h3866dTKNbpceTSTWgKSqB2c7J1iURERERUz/CIhUwpFQK6RTTCY52b4pFOwQCA/GIdNsZftXFlRERERFQfcWDhACZ0b2b8+qs/L0m+QJ5CoYCfnx8UCsaHrI/5IqkwWyQl5oukIqds2X+FVK3m/p7o1cIXAHD1ZgF2nr0h6fMpFApERETIIuAkP8wXSYXZIikxXyQVOWXL/iskszzdI8z49cr9KZI+l16vR1JSkiwuIiL5Yb5IKswWSYn5IqnIKVscWDiIPi39EObrAQA4kJSNc9fyJHsuvV6PzMxMWQSc5If5IqkwWyQl5oukIqdscWDhIBQKAeO7hRpvrzog7VELIiIiIqKyOLBwIA/FBqOBS+kMwj8kpuHWnWIbV0RERERE9QUHFg7E09UJY+JKp54tLNHjmyPSTD2rUCgQHBwsi4uISH6YL5IKs0VSYr5IKnLKlv1XSBYZ360ZBKH06zV/XoJWZ/3z8eQUcJIf5oukwmyRlJgvkoqcsmX/FZJFmvl64L5W/gCAdE0htp2+bvXn0Ol0OHPmDHQ6ndX3TcR8kVSYLZIS80VSkVO2OLBwQBN6NDN+vWr/JavvXxRFaDQaiKK0C/FR/cR8kVSYLZIS80VSkVO2OLBwQD2b+6K5fwMAwOFLN3EyTWPjioiIiIjI0XFg4YAEQcCE7s2Mt1cduGSzWoiIiIiofuDAwkE9GBMEL9fSqWd/PpaOrNtFVtu3QqFAeHi4LC4iIvlhvkgqzBZJifkiqcgpW/ZfIdWIu7MKj3VuCgAo1umx/tAVq+1boVDA399fFgEn+WG+SCrMFkmJ+SKpyClb9l8h1di4rqFQGKaePXgZxVrrTD2r0+lw/PhxWcxOQPLDfJFUmC2SEvNFUpFTtjiwcGAhPu4Y2KYxAOBGXhG2nMywyn5FUURBQYEsZicg+WG+SCrMFkmJ+SKpyClbHFg4uAndw4xfr5Rg6lkiIiIiIoADC4fXNdwHkQGeAIBjV3OQeOWWjSsiIiIiIkfEgYWDEwQBE8sumGeFqWeVSiUiIyOhVCprvS+iezFfJBVmi6TEfJFU5JQtDizqgZEdgtDQ3QkA8OuJDFzPLazV/gRBgLe3NwRBsEZ5RCaYL5IKs0VSYr5IKnLKFgcW9YCrkxJj/556VqsX8fXBy7Xan1arxZEjR6DVaq1RHpEJ5oukwmyRlJgvkoqcssWBRT0xrlsolH/PPfv1oSso0tZuyjI5THlG8sV8kVSYLZIS80VSkUu2OLCoJwLVbrg/KgAAkH2nGJuPW2fqWSIiIiIigAOLeuXpMhdxr9yfIov5kImIiIhIHjiwqEdimjZEuyA1AOBUei7iL9ds6lmlUon27dvLYnYCkh/mi6TCbJGUmC+SipyyxYFFPVJu6tlaLJjn7Oxc+4KIKsF8kVSYLZIS80VSkUu2OLCoZ4a1D4RvAxcAwNZT15CeU2DxPnQ6HeLj42VzIRHJC/NFUmG2SErMF0lFTtniwKKecVEp8USX0qlndXoRa2o59SwREREREcCBRb30RNemcFKWTj27/vAVFBTb/wiYiIiIiOwbBxb1kL+nK4a3bwIAyMkvwU/H0mxcERERERHJnV0MLJYsWYKwsDC4uroiNjYW+/btq3TbCRMmQBCEcv/atm1r3KakpARz585FREQEXF1dER0dja1bt5rs55133im3j4CAAMn6aG8mdG9m/Hrl/ksWTT2rVCoRFxcni9kJSH6YL5IKs0VSYr5IKnLKls0HFhs2bMDLL7+Mt956C4mJiejVqxeGDBmCK1euVLj9okWLkJGRYfx39epV+Pj4YMyYMcZtZs2ahWXLluHTTz/F6dOnMWXKFIwePRqJiYkm+2rbtq3Jvv766y9J+2pPokO8EdPUGwBw7noe/kzOtujxxcXFElRFVIr5IqkwWyQl5oukIpds2XxgsXDhQkyaNAmTJ09G69at8fHHHyMkJARLly6tcHu1Wo2AgADjv/j4eNy6dQsTJ040brNmzRq8+eabGDp0KMLDwzF16lQMHjwYCxYsMNmXSqUy2Zefn5+kfbU3E3qEGb9eacHUszqdDidOnJDF7AQkP8wXSYXZIikxXyQVOWVLZcsnLy4uxtGjRzFz5kyT9kGDBuHAgQNm7WP58uUYMGAAQkNDjW1FRUVwdXU12c7NzQ1//PGHSduFCxfQpEkTuLi4oEuXLvj3v/+N8PDwSp+rqKgIRUVFxtu5ubkAAK1WC61WCwBQKBRQKBTQ6/XQ6/XGbQ3tOp3O5LSjytqVSiUEQTDut2w7gHLhqqxdpVJBFEWTdkEQoFQqMbiNPxp7uuB6XhF2nLmOS5l5aObnWWnthnZDrYZt7KlP99ZeWbucXqf61idDPTqdzmH65Iivkxz7BMDsvsqlT474Osm1T4avRVE02Y+c+wQ43uskxz6V/b1oiz6V/bo6Nh1YZGVlQafToXHjxibtjRs3xrVr16p9fEZGBrZs2YJ169aZtA8ePBgLFy5E7969ERERgd9//x0//fSTyTe7S5cuWL16NVq2bInr169j3rx56N69O06dOoVGjRpV+Hzz58/HnDlzyrUnJibCw8MDAODn54eIiAikpKQgMzPTuE1wcDCCg4Nx/vx5aDQaY3t4eDj8/f1x8uRJFBTcXVMiMjIS3t7eSExMNKm7ffv2cHZ2Rnx8vEkNcXFxKC4uxokTJ4xtSqUSnTp1gkajwdmzZ43tbm5uiI6OhubWTfQNVmDDGUAUgU+2HMfCp3oiPT0dqampxu3v7ZMoisjJyUFGRgZCQ0Ptqk9ZWVlITk42tqvVarRu3braPtnz61Tf+nT16lXk5OQgISEB/v7+DtEnR3yd5Nintm3bori4GAkJCcaBhtz75Iivk1z7pFCUngSSm5uLCxcuOESfHPF1kmOfDJ+7EhIS0KlTpzrvk1qthrkE0ZKrdq0sPT0dQUFBOHDgALp162Zsf/fdd7FmzRqTb05F5s+fjwULFiA9Pd1kRcLMzEw888wz2Lx5MwRBQEREBAYMGICVK1ciPz+/wn3duXMHEREReP311zFjxowKt6noiEVISAiys7Ph5eUFQH6j8sy8QvT6YA+KtXp4uqpw8I3+cHNSVHvE4tixY+jYsSOcnZ3trk/29pcG9smyPpWUlODYsWPo0KEDnJycHKJPjvg6ybFPoigiISEB0dHRxsfKvU+O+DrJtU86XenpKh06dDAOXOXeJ8DxXic59snwuatDhw7Gz7t12afbt2+jYcOG0Gg0xs+7lbHpwKK4uBju7u7YuHEjRo8ebWyfPn06jh07hj179lT6WFEU0bJlSwwfPhwfffRRhdsUFhYiOzsbTZo0wcyZM/HLL7/g1KlTle5z4MCBaN68eaXXd9wrNzcXarXarG+0PXtt43FsPFo6Yv2/kW0xrlsz2xZERERERHbBks+7Nr1429nZGbGxsdi+fbtJ+/bt29G9e/cqH7tnzx5cvHgRkyZNqnQbV1dXBAUFQavVYtOmTRg5cmSl2xYVFeHMmTMIDAy0rBMOYEKPZsavVx24BL2+6rGm4ZCcDcek5MCYL5IKs0VSYr5IKnLKls1nhZoxYwa+/PJLrFixAmfOnMErr7yCK1euYMqUKQCAN954A0899VS5xy1fvhxdunRBVFRUufsOHTqE77//HsnJydi3bx/uv/9+6PV6vP7668Zt/vGPf2DPnj1ISUnBoUOH8PDDDyM3Nxfjx4+XrrN2qm0TNTqH+QAAkjLvYN/FrCq31+l0OHv2bLnDcETWwHyRVJgtkhLzRVKRU7ZsevE2ADz66KPIzs7G3LlzkZGRgaioKPz222/GWZ4yMjLKrWmh0WiwadMmLFq0qMJ9FhYWYtasWUhOTkaDBg0wdOhQrFmzBt7e3sZtUlNTMXbsWGRlZcHPzw9du3bFwYMHTWaXqk+e7tEMh1NuAgBW7U9Bn5b1a+pdIiIiIqodmw8sAGDatGmYNm1ahfetWrWqXJtara70ImwA6NOnD06fPl3lc37zzTcW1ejoBrRujCBvN6TlFGDXuUwkZ95GuF8DW5dFRERERDJh81OhyD6olAo81e3u0ZrVf16udFtBEODm5mYy6wWRtTBfJBVmi6TEfJFU5JQtm84KJXeOMiuUQU5+MbrO/x2FJXp4OCvx55v94eXqZOuyiIiIiMhGZDMrFNkXb3dnPBgTDAC4U6zDd/GpFW6n1+tx48YNi1ZiJDIX80VSYbZISswXSUVO2eLAgkxM6N7M+PVXf16CroKpZ/V6PZKTk2URcJIf5oukwmyRlJgvkoqcssWBBZlo2dgTPZv7AgAuZ+dj97kbNq6IiIiIiOSAAwsqp+xRi5X7L9msDiIiIiKSDw4sqJz7Iv0R2sgdAPDHxSxcuJ5ncr8gCFCr1bKYnYDkh/kiqTBbJCXmi6Qip2xxYEHlKBQCnurWzHh75YFLJvcrlUq0bt0aSqWybgujeoH5IqkwWyQl5oukIqdscWBBFRoTFwwP59IAf5+QCk1+ifE+vV6P1NRUWVxERPLDfJFUmC2SEvNFUpFTtjiwoAp5uTrh4djSqWcLS/T45sgV431yCjjJD/NFUmG2SErMF0lFTtniwIIqNb7MRdyr/7wMrc7+A01EREREtsGBBVUq3K8B+rbyAwCk5RRgx5nrNq6IiIiIiOwVBxZUpYk9woxfG6aeVSgU8PPzg0LB+JD1MV8kFWaLpMR8kVTklC37r5BsqldzX4T7eQAADqXcxKl0DRQKBSIiImQRcJIf5oukwmyRlJgvkoqcsmX/FZJNKRSCyYJ5Xx24BL1ej6SkJFlcRETyw3yRVJgtkhLzRVKRU7Y4sKBqPRQTDE8XFQDgx2PpyMwrRGZmpiwCTvKj1+uZL5IEs0VSYr5IKnLKFgcWVC0PFxUe6RQCACjW6rHhSKqNKyIiIiIie2PxwEKpVOLw4cMV3nf06FFZrApIlhvfrRkMK8mvPHAJe68U4mDyTej0om0LIyIiIiK7YPHAQhQr/yCp1+shGD59kkNp2sgd7ZqoAQC38kuwJOEOnlxxBD3f34mtJzNsXB05EoVCgeDgYFlcpEbywmyRlJgvkoqcslWjCisbPBw9ehRqtbpWBZF92noyAyfSNOXar2kKMXVtAgcXZDVyegMleWG2SErMF0lFTtkyq8JFixYhPDwc4eHhEAQBo0aNMt42/AsMDMTzzz+PAQMGSF0z1TGdXsSczacrvM9w/GrO5tM8LYqsQqfT4cyZM9DpdLYuhRwMs0VSYr5IKnLKlsqcjfz9/dG2bVsAwKVLlxAeHg5vb2+TbVxcXNCuXTtMnz7d6kWSbR1OuYkMTWGl94sAMjSFOJxyE90iGtVdYeSQRFGERqOp8rRLoppgtkhKzBdJRU7ZMmtgMXbsWIwdOxYA0K9fPyxduhSRkZGSFkb240Ze5YOKmmxHRERERI7HrIFFWbt27ZKiDrJj/p6uVt2OiIiIiBxPja4CyczMxBtvvIFu3bqhRYsWOHXqFABg2bJlSExMtGqBZHudw3wQqHZFZfN9CQAC1a7oHOZTl2WRg1IoFAgPD5fFRWokL8wWSYn5IqnIKVsWV5iSkoL27dvjk08+gSAISE5ORlFREQDgxIkT+OSTT6xeJNmWUiFg9og2AFDh4EIEMHtEGygVnGqYak+hUMDf318Wb6AkL8wWSYn5IqnIKVsWV/j666+jYcOGuHDhAvbu3WtyIUnPnj2xf/9+qxZI9uH+qEAsfTIGAerypzuN7NAE90cF2qAqckQ6nQ7Hjx+XxewXJC/MFkmJ+SKpyClbFl9j8fvvv2Pp0qVo0qRJuQ4GBgYiPT3dasWRfbk/KhAD2wTgz4uZ2H74FL46mQ8A+CutdKYCLo5I1iCKIgoKCmQx+wXJC7NFUmK+SCpyypbFRywKCwvh41PxufR37tyRxWEaqjmlQkDXcB8Mae6GTs0aAgCSM+/g2NUc2xZGRERERDZl8SigVatW2LFjR4X37d27F1FRUbUuiuThwY5NjF9vSki1YSVEREREZGsWDyyeeeYZLFq0CIsWLcKtW7cAAMXFxfjuu++wZMkSPPfcc1YvkuyLUqlEZGQkhrVvAlen0ghtPp6BIq39n/tH9s+QL6VSaetSyMEwWyQl5oukIqdsCWINTth69tln8eWXX0KhUECv10OhUEAURTzzzDP473//K0Wddik3NxdqtRoajQZeXl62Lscmpn+TiJ+OlV5Xs/SJGAxpx4u4iYiIiByFJZ93a3RBxOeff44DBw7gjTfewOTJk/H6669j37599WpQUZ9ptVocOXIEWq0WD8UEG9t5OhRZQ9l8EVkTs0VSYr5IKnLKlsWzQhl07doVXbt2tWYtJCOGGcF6NPdFYy8XXM8twu5zmci6XQTfBi42ro7kTg5T6pE8MVskJeaLpCKXbNVqCqfi4mL897//xQsvvID58+fj+vXr1qqLZEKpEDCqYxAAQKsX8fMxTjdMREREVB+ZNbB4//330blzZ5M2rVaLHj164Pnnn8eSJUvw1ltvISYmhutY1EMP83QoIiIionrPrIHF1q1b0alTJ5O2zz//HEePHsUjjzyCY8eOYd26dbhz5w7+/e9/S1Io2Q+lUon27dsbZydo0dgT7YPVAIBT6bk4ey3XluWRzN2bLyJrYbZISswXSUVO2TJrYHH+/Hl0797dpO2HH36AWq3GihUr0L59ezz22GOYMWMGtm3bJkmhZF+cnZ1Nbpe9iPv7hLS6LocczL35IrIWZoukxHyRVOSSLbMGFtnZ2QgJCTHe1uv1OHDgAPr06QM3Nzdje9euXXH16lXrV0l2RafTIT4+3uRCohHRTeCkFAAAPySmQavT26o8krmK8kVkDcwWSYn5IqnIKVtmDSwaNWqEmzdvGm+fPHkSBQUFiIuLM9nO1dUVKlWNJ5oiGfPxcEa/Vv4AgMy8Iuy7mGXjioiIiIioLpk1sIiKisL69euNt3/44QcIgoC+ffuabJeUlISAgACrFkjy8VAsT4ciIiIiqq/MOrwwY8YMDBkyBJmZmQgICMB3332HqKgo9OzZ02S7LVu2oEOHDlLUSTLQr5U/Gro74VZ+CbaduobcwhJ4uTrZuiwiIiIiqgNmHbEYPHgwlixZguTkZGzevBl9+/bFDz/8YLLNtWvX8Pvvv2Pw4MGSFEr2Q6lUIi4urtzsBM4qBR6IbgIAKNLq8euJDFuURzJXWb6IaovZIikxXyQVOWVLEEVRtHURcpWbmwu1Wg2NRgMvLy9bl1NnRFFEQUEB3NzcIAiCyX0nUnPwwGf7AQBxoQ3x3dTuFe2CqFJV5YuoNpgtkhLzRVKxdbYs+bxbq5W3qX7S6XQ4ceJEhbMTtAtSo4V/AwBA/OVbuJx9p67LI5mrKl9EtcFskZSYL5KKnLLFgQVZlSAIJhdxb+JF3ERERET1AgcWZHWjOgRB8feRuu8TUqHX82w7IiIiIkfHgQXVSFUXEAWoXdGjuS8AIPVWAQ5fulnptkQVkcMFaiRPzBZJifkiqcglW7x4uxbq68Xb5vjpWBqmf3MMAPBIXDD+83C0bQsiIiIiIovx4m2SlCiKyMnJQVVj0kFtAtDApXSZlN/+uoaCYvu/4Ijsgzn5IqoJZoukxHyRVOSUrRoPLG7cuIEjR45g79695f6RY9PpdDh79myVsxO4OSsxtF3pKuy3i7T436lrdVUeyZw5+SKqCWaLpMR8kVTklC2zVt4uKyMjA+PGjcOuXbvK3SeKIgRBkEXHSXoPxQTj2/hUAMCmhFSM6hhk44qIiIiISCoWDyxeeOEFJCYm4v3330f79u3h4uIiRV3kADo180GIjxuu3izA/otZuKYpRIDa1dZlEREREZEELB5Y7NmzBx9++CEmTpwoRT0kA4IgmLX6o0IhYHTHYHzy+wXoReCHxDRM7RtRR1WSXJmbLyJLMVskJeaLpCKnbFl8jYUgCAgJCZGiFpIJpVKJ6Ohos6Y+eyjm7ulPmxJSZXHhEdmWJfkisgSzRVJivkgqcsqWxQOLMWPG4JdffpGiFpIJvV6PGzduQK/XV7ttaCMPdGrWEABw8cZt/JWmkbo8kjlL8kVkCWaLpMR8kVTklC2LT4V65JFH8Mwzz0Cv12PEiBFo1KhRuW1iYmKsUhzZJ71ej+TkZPj4+EChqH5s+lBMMI5cugUA2HQ0Fe2DvSWukOTM0nwRmYvZIikxXyQVOWXL4oHFfffdBwD47LPPsHjxYpP7OCsUVWRo+0DM/vkUirR6/Hw8HW8NawNnlX3/YBARERGRZSweWKxcuVKKOsiBebk6YVDbAGw+no5b+SXYde4GBrcNsHVZRERERGRFFg8sxo8fL0UdJCOCIECtVls0O8FDMUHYfDwdQOnpUBxYUGVqki8iczBbJCXmi6Qip2wJYi2m6Tl//jyys7Ph6+uLFi1aWLMuWcjNzYVarYZGo4GXl5ety7FrWp0e3d/biRt5RXBSCjj05gD4eDjbuiwiIiIiqoIln3drdKL7xo0bERoaitatW6Nnz56IjIxEaGgovvvuuxoVTPKi1+uRmppq0ewEKqXCuPJ2iU7Ez8fSpCqPZK4m+SIyB7NFUmK+SCpyypbFA4vffvsNjz32GNRqNd577z2sXr0a8+fPh1qtxmOPPYYtW7ZIUSfZkZoG/KGYYOPX3ydyYEEVk9MbKMkLs0VSYr5IKnLKlsUDi3fffReDBg3CsWPH8Nprr+GJJ57A66+/juPHj2PAgAGYN2+exUUsWbIEYWFhcHV1RWxsLPbt21fpthMmTIAgCOX+tW3b1rhNSUkJ5s6di4iICLi6uiI6Ohpbt26t1fNS7bUK8ERUUOkhtBOpGly4nmfjioiIiIjIWiweWBw7dgzTpk0rN4+uIAiYNm0ajh8/btH+NmzYgJdffhlvvfUWEhMT0atXLwwZMgRXrlypcPtFixYhIyPD+O/q1avw8fHBmDFjjNvMmjULy5Ytw6efforTp09jypQpGD16NBITE2v8vGQdD3a8e9Tiu4RUG1ZCRERERNZk8cBCqVSiuLi4wvtKSkosXrhj4cKFmDRpEiZPnozWrVvj448/RkhICJYuXVrh9mq1GgEBAcZ/8fHxuHXrFiZOnGjcZs2aNXjzzTcxdOhQhIeHY+rUqRg8eDAWLFhQ4+eluxQKBfz8/Gq0SMvIDk2gUpTOavBjYhp0+hrPHUAOqjb5IqoKs0VSYr5IKnLKlsUVdurUCf/5z39QUFBg0l5UVIQPP/wQXbp0MXtfxcXFOHr0KAYNGmTSPmjQIBw4cMCsfSxfvhwDBgxAaGioSS2urq4m27m5ueGPP/6w2vPWZwqFAhERETUKeKMGLujbyh8AcD23CPsvZlm7PJK52uSLqCrMFkmJ+SKpyClbFq9jMWfOHPTv3x/h4eEYM2YMAgICkJGRge+//x7Z2dnYuXOn2fvKysqCTqdD48aNTdobN26Ma9euVfv4jIwMbNmyBevWrTNpHzx4MBYuXIjevXsjIiICv//+O3766SfjiuA1fd6ioiIUFRUZb+fm5gIAtFottFotgNIXX6FQQK/Xm1xkY2jX6XQoO8NvZe1KpRKCIBj3W7YdQLnVzStrV6lUEEXRpF0QBCiVynI1VtZ+b5/0ej0uX76MsLAwqFQqi/s0ukMgdpy5DgD4Lv4qerXwtXmfqqtdjq+TXPuk1Wpx+fJlhIaGQqVSOUSfHPF1kmOfBEFAcnIymjZtavwFLfc+OeLrJNc+6fV6XL16Fc2aNTPZt5z7BDje6yTHPhk+d4WGhsLJyanO+2TRLKBmb/m3nj17Ytu2bZg5cyYWL14MURShUCjQpUsXrF+/Ht27d7d0l+UW/BBF0axFQFatWgVvb2+MGjXKpH3RokV45plnEBkZCUEQEBERgYkTJ5ZbNdzS550/fz7mzJlTrj0xMREeHh4AAD8/P0RERCAlJQWZmZnGbYKDgxEcHIzz589Do9EY28PDw+Hv74+TJ0+aHAWKjIyEt7c3EhMTTULSvn17ODs7Iz4+3qSGuLg4FBcX48SJE8Y2pVKJTp06QaPR4OzZs8Z2Nzc3REdHIysrC8nJycZ2tVqN1q1bIz09Hampd69/uLdPoigiJycHzs7OCA0NtbhPDQvS4OEk4E6JiK2nMpCZ0xw+nu427ZMjvk5y7dPVq1eRk5ODrKws+Pv7O0SfHPF1kmOf2rZti7S0NGRmZhrf6+XeJ0d8neTaJ8MHMm9vb1y4cMEh+uSIr5Mc+2T43JWVlYVOnTrVeZ/UajXMVasF8vLz83Hr1i00bNgQ7u7uFj++uLgY7u7u2LhxI0aPHm1snz59Oo4dO4Y9e/ZU+lhRFNGyZUsMHz4cH330UYXbFBYWIjs7G02aNMHMmTPxyy+/4NSpUzV+3oqOWISEhCA7O9u4YEh9GJXrdDokJCQgNjYWzs7ONerT7M2n8fWhqwCA9x9qh0fiQhz2Lw3sk2V9KikpQUJCAmJiYuDk5OQQfXLE10mOfRJFEUeOHEFMTIzxsXLvkyO+TnLtk06nQ2JiImJjY03+SCnnPgGO9zrJsU+Gz10xMTFwdnau8z7dvn0bDRs2NGuBPIuPWJTl7u5eowGFgbOzM2JjY7F9+3aTD/jbt2/HyJEjq3zsnj17cPHiRUyaNKnSbVxdXREUFISSkhJs2rQJjzzySK2e18XFBS4uLuXaVSoVVCrTb6XhxbmXISjmtt+735q0C4JQYXtlNZrTLgiC8eua9GlMXFPjwGJTQhoe7dTU5n0yp3a5vU5lyaVPhjdZpVJp3EbufXLE10mOfdJqtcZsmfuebe99qqpG9ol9qqxGS9vZJ9v3yfDeZRi01mWfKtqmMmYNLFavXo1hw4ahUaNGWL16dbXbP/XUU2YXMGPGDIwbNw5xcXHo1q0bPv/8c1y5cgVTpkwBALzxxhtIS0sr97zLly9Hly5dEBUVVW6fhw4dQlpaGjp06IC0tDS888470Ov1eP31181+XqqcQqFAcHCwRUG7V3SwGhF+HkjKvIPDKTdx9WY+QnxqPkglx2GNfBFVhNkiKTFfJBU5ZcusgcWECRNw8OBBNGrUCBMmTKhyW0EQLBpYPProo8jOzsbcuXORkZGBqKgo/Pbbb8ZZnjIyMsqtLaHRaLBp0yYsWrSown0WFhZi1qxZSE5ORoMGDTB06FCsWbMG3t7eZj8vVc4Q8NoQBAEPxgTjg/+dAwBsSkjFywNaWqM8kjlr5IuoIswWSYn5IqnIKVtmXWNx+fJlBAYGwtnZGZcvX652p/Xlw3lubi7UarVZ55w5Ep1Oh/Pnz6Nly5aVHtIzR4amAN3f2wlRBJr6uGPPa33NumifHJu18kV0L2aLpMR8kVRsnS1LPu+adcSi7EChvgwaqHKiKEKj0cCMMWmVAtVu6BHhiz8uZuHKzXzEX76FTs18rFQlyZW18kV0L2aLpMR8kVTklC2LT9YKDw/H8ePHK7zv5MmTCA8Pr3VRVH88FBtk/HrT0dQqtiQiIiIie2bxwOLSpUsmU66WVVhYaNapUkQGg9sGwMO59LDerycyUFiiq+YRRERERGSPanR5eWXnwScnJ8PT07NWBZH9UygUCA8Pt8rsBO7OKgxpFwgAyCvSYtvp67XeJ8mbNfNFVBazRVJivkgqcsqWWddYfPXVV/jqq6+Mt6dOnVru4o2CggIcP34cffr0sW6FZHcUCgX8/f2ttr+HYoLx3d+nQW06mooHoptYbd8kP9bOF5EBs0VSYr5IKnLKlllDn/z8fGRmZiIzMxOCICAnJ8d42/CvpKQEjz76KJYtWyZ1zWRjOp0Ox48fL7fqY011CfNBkLcbAGDfhUxczy20yn5JnqydLyIDZoukxHyRVOSULbOOWEydOhVTp04FAISFhWHTpk2Ijo6WtDCyX6IooqCgwGqzEygUAh6MCcKnOy9CLwI/HUvDs70jrLJvkh9r54vIgNkiKTFfJBU5Zcvik7VSUlI4qCCrezDm7sIvm46myeKHh4iIiIjuMuuIRWUyMzNRUFBQrr1p06a12S3VQ2G+HogNbYijl2/h3PU8nErPRVSQ2tZlEREREZGZajSwmDdvHj755BNkZ2dXeL8czgGjmlMqlYiMjLT66o8PxgTh6OVbAIDvjqZyYFFPSZUvImaLpMR8kVTklC2LT4VasWIF3nvvPbz00ksQRRFvvvkm3njjDQQHB6NFixb48ssvpaiT7IggCPD29q502uGaGt6+CZxVpZH8+Xg6SnR6q+6f5EGqfBExWyQl5oukIqdsWTywWLx4sXEwAQCjR4/GvHnzcPbsWXh6eiIrK8vqRZJ90Wq1OHLkCLRarVX3q3ZzwsA2jQEAN+8UY/e5TKvun+RBqnwRMVskJeaLpCKnbFk8sLh48SK6du1qXKSjuLgYAODm5oZXX30Vn3/+uXUrJLsk1eluD5tcxJ0qyXOQ/ePplCQVZoukxHyRVOSSLYsHFipV6WUZgiDAy8sLqal3P/z5+voiLS3NetVRvdOrhS98G7gAAH4/ex05+cU2roiIiIiIzGHxwKJFixa4evUqAKBTp0744osvUFJSAp1Oh88//xzNmjWzdo1Uj6iUCozqULrydolOxObj6TauiIiIiIjMYfHAYujQodi7dy8A4I033sDOnTvh7e0NHx8fbNq0Cf/85z+tXiTZF6VSifbt20s2O8FDsXdPh/ougUfA6hup80X1F7NFUmK+SCpyypYg1nIlsiNHjuCbb76BIAgYNmwY+vXrZ63a7F5ubi7UajU0Gg28vLxsXU6dEUUROp0OSqVSshkKhi7ah9MZuQCAHTP6oLl/A0meh+xPXeSL6idmi6TEfJFUbJ0tSz7vWnzE4l6dOnXCggUL8OGHH9arQUV9ptPpEB8fL+mFRA/GBBm//j6BF3HXJ3WRL6qfmC2SEvNFUpFTtmo9sCCSwsgOQVAqSkflPySmQaev1YE1IiIiIpKYWStvh4WFWXToJTk5ucYFEQGAn6cL+rb0w+9nbyBDU4g/k7LRs4WvrcsiIiIiokqYNbDo06ePycBi586duHbtGrp3746AgABcu3YNBw4cQGBgIO677z7JiqX65aHYYPx+9gYAYFNCKgcWRERERHbMrIHFqlWrjF+vWbMG+/fvx4ULF9C0aVNj++XLlzFw4ED06dPH6kWSfVEqlYiLi5N8doL7Iv3h5apCbqEWW09ew/+N0qKBi1mRJRmrq3xR/cNskZSYL5KKnLJl8TUW7733HubMmWMyqACA0NBQzJ49G++9957ViiP7ZVhxXUquTkqMiC5d06KgRIctf2VI/pxkH+oiX1Q/MVskJeaLpCKXbFk8sEhKSoJara7wvoYNG+LSpUu1rYnsnE6nw4kTJ+pkdoKya1ps4uxQ9UJd5ovqF2aLpMR8kVTklC2LBxbNmjXD8uXLK7zviy++QGhoaK2LIjLoGOKNMF8PAMDB5JtIvZVv44qIiIiIqCIWn7A+c+ZMPP300+jcuTPGjh1rvHh7/fr1OHr0KL788ksp6qR6ShAEPBQThA+3nQcAfLLjAnq08IW/pys6h/kYp6QlIiIiItuyeGAxYcIEAMCsWbPw6quvGtsDAwPxxRdfYOLEiVYrjuxXXV5ANDom2Diw+PZoKr49WnpKVKDaFbNHtMH9UYF1VgvVDTlcoEbyxGyRlJgvkopcsiWIolijlcdEUcS5c+eQnZ2NRo0aoVWrVvVuCXtLljinmtt6MgNT1iaUazekbemTMRxcEBEREUnAks+7NV55WxAEREZGokePHoiMjKx3g4r6TBRF5OTkoIZjUovo9CLmbD5dcR1//z9n82muzO1A6jJfVL8wWyQl5oukIqdsmXUq1N69exETE4MGDRpg79691W7fu3fvWhdG9kun0+Hs2bOIi4uDSiXtuhKHU24iQ1NY6f0igAxNIQ6n3ES3iEaS1kJ1oy7zRfULs0VSYr5IKnLKllnV9e3bFwcPHkTnzp3Rt2/fSo9OiKIIQRBkMR0WycONvMoHFTXZjoiIiIikYdbAYteuXWjTpo3xa6K64u/patXtiIiIiEgaZg0s+vTpU+HXVD8JggA3N7c6ua6mc5gPAtWuuKYpRGVnFgaqS6eeJcdQl/mi+oXZIikxXyQVOWWrxrNCEWeFqitbT2Zg6t+zQlUU1s8e74jh7ZvUbVFERERE9YAln3fNOmIxd+5cs59cEAS8/fbbZm9P8qPX65GVlQVfX18oFDWeWMxs90cFYumTMZiz+XSFF3In3bgjeQ1Ud+o6X1R/MFskJeaLpCKnbJk1sHjnnXfM3iEHFo5Pr9cjOTkZPj4+dRbw+6MCMbBNAA6n3MSNvELkFWrxr59OQi8Cn+68gP6t/REVpK6TWkhatsgX1Q/MFkmJ+SKpyClbZg0s9Hq91HUQVUupEEymlL2eW4hPd16EVi/i1W+P4+cXe8BFJY+VKYmIiIgcjX0Pe4iq8OJ9LdA6sPRcv3PX8/Dxjgs2roiIiIio/uLAgiwmCALUarXNZydwVimw8JFoOClL61i2JwkJV27ZtCaqPXvJFzkeZoukxHyRVOSUrRrNCrV371588sknOHPmDAoKCkx3KAhISkqyWoH2jLNC2YfFuy7ig/+dAwCE+3rg15d6wc2Zp0QRERER1ZYln3ctPmLxxx9/oH///tBoNDhz5gwiIyMRFBSEK1euQKVSoXfv3jUunORBr9cjNTXVbq69ea53OKJDvAEAyVl38J//nbVtQVQr9pYvchzMFkmJ+SKpyClbFg8sZs+ejYkTJ2Lr1q0AgHnz5mHfvn1ISEjA7du38eCDD1q9SLIv9hZwlVKBBWOi4aIqjfPK/ZfwZ1K2jauimrK3fJHjYLZISswXSUVO2bJ4YHHy5EmMHj3aeJ6XTqcDALRv3x5vv/22RWteEFlLc/8GeG1wK+Pt1747jttFWhtWRERERFS/WDywyM/PR4MGDaBQKODi4oKsrCzjfZGRkTh9+rRVCyQy19M9wtC5mQ8AIPVWAd799YyNKyIiIiKqPyweWDRt2hTXr18HALRp0wa//vqr8b49e/agUaNGlT2UHIRCoYCfn5/dLdKiUAj4YEx7uP994fb6w1ew53ymjasiS9lrvkj+mC2SEvNFUpFTtiyusG/fvti9ezcA4JlnnsGSJUvQv39/DB06FPPmzcPYsWOtXSPZGYVCgYiICLsMeGgjD7w5tLXx9j+/OwFNfokNKyJL2XO+SN6YLZIS80VSkVO2zKowM/PuX33nzJmDl156CQAwZcoUfPjhh8jJycGNGzcwa9YszJs3T5pKyW7o9XokJSXZ7UVET3Rpil4tfAEA13ILMWfzKRtXRJaw93yRfDFbJCXmi6Qip2yZNbAICgrCww8/jC1btqBRo0Zo2bKl8b4ZM2bg6NGjiI+PxzvvvAMnJyfJiiX7oNfrkZmZabcBFwQB7z/UHp4uKgDA94lp+N+pazauisxl7/ki+WK2SErMF0lFTtkya2AxZswYbNmyBcOHD0dISAhmzZpVbxbBI3lq4u2G2Q+0Nd5+64e/kH27yIYVERERETk2swYWX3/9NTIyMrB48WIEBQXh3//+N1q2bIl+/fph7dq1KCwslLpOIos9FBOEAa39AQBZt4vx9k8nUYOF5omIiIjIDGZfBeLl5YUpU6bg0KFDOHXqFF555RWcPXsWTz31FAICAjB16lQcOXJEylrJTigUCgQHB9v9RUSCIODfD7aDt3vp6Xm//XUNm09k2Lgqqo5c8kXyw2yRlJgvkoqcsiWItfgTrk6nw6+//ooVK1Zgy5Yt0Gq1aNu2LU6cOGHNGu1Wbm4u1Go1NBoNvLy8bF0OVWLz8XS8uD4RAKB2c8L2V3rD38vVxlURERER2T9LPu/WauijVCrxwAMPYNmyZXjhhRcAAKdOcQYeR6fT6XDmzBnjquv2bkR0EwxrHwgA0BSU4I3v/+IpUXZMbvki+WC2SErMF0lFTtmq8cBCp9Phhx9+wAMPPICQkBB89NFHaNeuHT7++GMrlkf2SBRFaDQaWX04/7+RUfBt4AIA+P3sDWw8mmrjiqgycswXyQOzRVJivkgqcsqWytIHnDp1CitWrMDatWuRlZUFLy8vTJ48GZMmTUJsbKwUNRLVmo+HM+Y/2A7PrI4HAMzdfBo9mvsiyNvNxpUREREROQazBha5ublYt24dVqxYgaNHjwIAevfujUmTJuHhhx+GqyvPVyf7N7BNYzwUE4xNCam4XaTFP787gdVPd4ZCIdi6NCIiIiLZM2tgERAQgKKiIgQGBmLmzJl4+umnERERIXVtZKcUCgXCw8NlMTvBvf41og0OJGUhQ1OIPy5m4etDlzGuWzNbl0VlyDlfZN+YLZIS80VSkVO2zJoV6sEHH8SkSZMwZMgQWXSqrnBWKHnaez4TT604DABwc1Jiy/ReaObrYeOqiIiIiOyP1WeF+v777zFs2DAOKghA6YX7x48fl8XsBBXp3dIPT3RpCgAoKNHhte+OQ6e3/wui6gu554vsF7NFUmK+SCpyyhZHCmQxURRRUFAgi9kJKvPm0NYI8Sm9cPvIpVtY8UeKjSsiA0fIF9knZoukxHyRVOSULQ4sqF7ycFHhg4ejIfx93fYH287h4o082xZFREREJGMcWFC91TW8EZ7uEQYAKNbqMePb49Dq9DauioiIiEieOLAgiymVSkRGRkKpVNq6lFp7bXArhPuVXrh9IlWDpbuTbFwROVK+yL4wWyQl5oukIqds2cXAYsmSJQgLC4OrqytiY2Oxb9++SredMGECBEEo969t27Ym23388cdo1aoV3NzcEBISgldeeQWFhYXG+995551y+wgICJCsj45EEAR4e3tDEOS//oOrkxILxkTDsJTFJzsv4FS6xrZF1XOOlC+yL8wWSYn5IqnIKVsWr7wNlF5EcuTIEVy+fBkFBQXl7n/qqafM3teGDRvw8ssvY8mSJejRoweWLVuGIUOG4PTp02jatGm57RctWoT33nvPeFur1SI6Ohpjxowxtn399deYOXMmVqxYge7du+P8+fOYMGECAOCjjz4ybte2bVvs2LHDeFsOI0F7oNVqkZiYiI4dO0KlqlGE7ErHpg0xtW8EFu9KQolOxKvfHsdPL/SAi4p5sAVHyxfZD2aLpMR8kVTklC2Lqzt//jweeOABXLhwocKr0wVBsGhgsXDhQkyaNAmTJ08GUHqk4X//+x+WLl2K+fPnl9terVZDrVYbb//444+4desWJk6caGz7888/0aNHDzz++OMAgGbNmmHs2LE4fPiwyb5UKhWPUtSQHKY8s8RL/Vvg9zM3cPZaHs5ey8Mnv1/Aa4MjbV1WveVo+SL7wWyRlJgvkopcsmXxqVDPP/88CgsLsWHDBpw9exYpKSkm/5KTk83eV3FxMY4ePYpBgwaZtA8aNAgHDhwwax/Lly/HgAEDEBoaamzr2bMnjh49ahxIJCcn47fffsOwYcNMHnvhwgU0adIEYWFheOyxxyyqnRyLi0qJBY9EQ/X3OVFLdych8cotG1dFREREJB8WH7E4fPgwvvjiCzz88MO1fvKsrCzodDo0btzYpL1x48a4du1atY/PyMjAli1bsG7dOpP2xx57DJmZmejZsydEUYRWq8XUqVMxc+ZM4zZdunTB6tWr0bJlS1y/fh3z5s1D9+7dcerUKTRq1KjC5ysqKkJRUZHxdm5uLoDSQ1RarRZA6bLrCoUCer0eev3dGYYM7TqdzuRIT2XtSqUSgiAY91u2HSg/cq2sXaVSQRRFk3ZBEKBUKsvVWFn7vX0y1GrYxhH6BACt/D3w4n3N8dGOC9CLwKvfHsfPz3eDq5NStn0q2y6X18lQj06nc5g+OeLrJMc+ATC7r3LpkyO+TnLtk+Frw+cOR+gT4Hivkxz7VPb3oi36VPbr6lg8sGjQoEG1y3lb6t6LUURRNOsClVWrVsHb2xujRo0yad+9ezfeffddLFmyBF26dMHFixcxffp0BAYG4u233wYADBkyxLh9u3bt0K1bN0REROCrr77CjBkzKny++fPnY86cOeXaExMT4eFROrOQn58fIiIikJKSgszMTOM2wcHBCA4Oxvnz56HR3L04ODw8HP7+/jh58qTJ9SqRkZHw9vZGYmKiSUjat28PZ2dnxMfHm9QQFxeH4uJinDhxwtimVCrRqVMnaDQanD171tju5uaG6OhoZGVlmRylUavVaN26NdLT05Gammpsr6hPer0e169fR0hIiMP0CQBGtgjC72fVOJGqQXLWHfxz7R8Y185D1n2SY/b0ej0SExMdqk+O+DrJrU/t27dHcHAwEhMTHaZPjvg6yblP7du3x+3bt3Hu3DmH6ZMjvk5y7JPh96It+lT2EoTqCKKFy/jNmjULV65cwerVqy15WIWKi4vh7u6OjRs3YvTo0cb26dOn49ixY9izZ0+ljxVFES1btsTw4cNNLsgGgF69eqFr16744IMPjG1r167Fs88+i9u3b0OhqPgMsIEDB6J58+ZYunRphfdXdMQiJCQE2dnZxsFWfRiVG45WqFQqKJVKh+hT2fakzDsY9ukfKNbqIQjAmglxUCgVyLxdDF8PJ8SFNoTy71Om5NInOWVPp9NBr9dDoVBAqVQ6RJ8c8XWSY58UCgVKSkqMMwE6Qp8c8XWSa58MfxQ11OMIfQIc73WSY58Mn7sUCoXx4u267NPt27fRsGFDaDSaag8uWHzEIioqCuvXr8cDDzyAESNGVHja0IMPPmjWvpydnREbG4vt27ebDCy2b9+OkSNHVvnYPXv24OLFi5g0aVK5+/Lz88sNHpRKJURRrHQ59KKiIpw5cwa9evWq9DldXFzg4uJSrl2lUpW7St/w4tzLEBRz2yu7+t+SdkEQKmyvrMbq2g2zE8TFxVVZu5z6VFaLxp74x6CW+PdvZyGKwPhV8dCViU2g2hWzR7TB/VGBsukTIJ/XqexfZQzbyL1Pjvg6ybFPWq0WCQkJiIuLM/s92977VFWN7FPd9kmr1SI+Pr7CfNWkdnvok4EjvU4GcupT2c9dhj+K1GWfKtqmMhYPLAwzLaWkpOCXX34pd78gCOVGUVWZMWMGxo0bh7i4OHTr1g2ff/45rly5gilTpgAA3njjDaSlpZU7QrJ8+XJ06dIFUVFR5fY5YsQILFy4EB07djSeCvX222/jgQceML6g//jHPzBixAg0bdoUN27cwLx585Cbm4vx48ebXTs5rkk9w7HhyFUkZd4xGVQAwDVNIaauTcDSJ2NMBhdERERE9ZnFA4tdu3ZZtYBHH30U2dnZmDt3LjIyMhAVFYXffvvNOMtTRkYGrly5YvIYjUaDTZs2YdGiRRXuc9asWRAEAbNmzUJaWhr8/PwwYsQIvPvuu8ZtUlNTMXbsWGRlZcHPzw9du3bFwYMHTWaXovott0BbYbsIQAAwZ/NpDGwTYDwtioiIiKg+s3hg0adPH6sXMW3aNEybNq3C+1atWlWuTa1WIz8/v9L9qVQqzJ49G7Nnz650m2+++cbiOqn+OJxyE5m3iyq9XwSQoSnE4ZSb6BZR8SxiRERERPVJjZfvy8vLw59//ons7Gz4+vqia9eu8PT0tGZtZKeUSiXi4uIqPU/QEdzIK7TqdmS++pAvsg1mi6TEfJFU5JQtixfIA4APP/wQTZo0wZAhQ/DEE09g8ODBaNKkCRYuXGjt+shOFRcX27oESfl7upq1nZOyRj9CVA1HzxfZDrNFUmK+SCpyyZbFn4pWr16N119/Hb1798Y333yDffv2YcOGDejTpw9ee+01rFmzRoo6yY7odDqcOHHCoov05aZzmA8C1a6o7uqJGRuO4eMd55FfXPH1GGS5+pAvsg1mi6TEfJFU5JQtiwcWH330ER5//HH8+uuvGDNmDHr06IExY8bgl19+wdixY8utKUEkR0qFgNkj2gBAlYOLQq0eH++4gH4f7sZ3R1Oh11u0LAwRERGRw7B4YHH27Fk8+eSTFd735JNP4syZM7Uuisge3B8ViKVPxiBAbXpaVKDaFR883B4Tujczzgh1PbcI/9h4HA8s/gN/JmXbolwiIiIim7L44m03NzfcvHmzwvtu3rwJNze3WhdF9k8OFxBZw/1RgRjYJgCHU27iRl4h/D1d0TnMB0qFgDEAnuwaive2nMGOMzcAACfTcjH2i4MY2KYx3hgSiXC/BrbtgEzVl3xR3WO2SErMF0lFLtkSxMqWoq7EyJEjcebMGezevRtNmjQxtl+7dg19+/ZFZGQkfvzxR2vXaZdyc3OhVqvNWuKcHNuBi1mY9+sZnM7INbapFALGdQvF9P4t4O3ubMPqiIiIiGrGks+7Fg8sTp06he7du6OkpAT9+/dHYGAgMjIysHPnTjg5OeHAgQNo06ZNrTogF/V1YCGKIjQaDdRqtXFpeQJ0ehGbElLx4f/O4Ube3TUw1G5OeKl/C4zrGgpnFWeRqg7zRVJhtkhKzBdJxdbZsuTzrsWfctq2bYsjR45g5MiROHLkCFauXIkjR45g1KhROHz4cL0ZVNRnOp0OZ8+elcXsBHVJqRDwSFwIdv2jL6b3bwFXp9IfL01BCf7vl9MY9NEebD15DRaO5esd5oukwmyRlJgvkoqcslWjBfJatmyJ9evXW7sWIofg4aLCKwNbYmznpvjgf+fwfWIqRBG4lJ2PKWuPonOYD94e1gbtgtW2LpWIiIjIanheBpFEAtSuWPBINDa/0BNdw32M7YdTbmLEZ39gxrfHkKEpsGGFRERERNbDgQVZTBAEuLm58RxSM0UFqbH+ma74fFwswnw9jO3fJ6Sh34e7sXDbOdwp4gJ7BswXSYXZIikxXyQVOWXL4ou36a76evE21VyxVo+vD13GxzsuQFNQYmz383TBa4Na4aHYYOPaGDq9WOE0t0RERER1RdJZoeiu+jqw0Ov1yMrKgq+vLxQKHvSqiZz8Yny68yJW/3kJJbq7P4KRAZ54e3gb5BWWYM7m08jQFBrvC1S7YvaINrg/KtAWJdcZ5oukwmyRlJgvkoqtsyXprFBEer0eycnJ0Ov1ti5FtrzdnfH28DbY9kofDG7b2Nh+9loenvjyEKasTTAZVADANU0hpq5NwNaTGXVdbp1ivkgqzBZJifkiqcgpWxxYENlQmK8Hlo2Lw4Znu6JdUNWzRBmOa8zZfBo6PQ80EhERkX0xa2Dx9NNPIyUlxaRt3bp1yMnJMWk7c+YMevfubbXiiOqLLuGN8NPzPTCtb0SV24kAMjSFOJxys24KIyIiIjKTWQOLVatWITMz03hbp9Nh3LhxSE5ONtkuNzcX+/fvt26FZHcEQeDKohJQKAS0CvA0a9sbeYXVbyRTzBdJhdkiKTFfJBU5ZatGC+QB4OrB9ZhSqUTr1q1tXYZD8vd0tep2csR8kVSYLZIS80VSkVO2eI0FWUyv1yM1NVUWFxHJTecwHwSqXVHZ3yQElM4O1TnMp5It5I/5IqkwWyQl5oukIqdscWBBFpNTwOVGqRAwe0QbAKhwcCECmD2ijUOvZ8F8kVSYLZIS80VSkVO2OLAgsjP3RwVi6ZMxCFCXP92pfZCXw69jQURERPJk9jUWCxYsQOPGpfPti6IIQRDwwQcfwM/Pz7jN9evXrV8hUT10f1QgBrYJwOGUm0jPyce7v53BzTslOJGWi1PpGrRtUvXUtERERER1zeyBxcaNG8u1bdiwoVybHK5Yp9pRKBTw8/PjyqISUyoEdItoBKARbhfpMPvnUwCAxbsuYskTsbYtTkLMF0mF2SIpMV8kFTllSxA5vVONWbLEOVFtFJbo0PP9Xci6XQRBALa93BstGps3NS0RERFRTVnyedf+hz5kd/R6PZKSkmRxEZGjcHVS4rne4QAAUSw9auGomC+SCrNFUmK+SCpyylatBxZbt27Fhx9+iHXr1qGw0HEX7aK79Ho9MjMzZRFwR/J4l6Zo6O4EAPj5eDpSsu7YuCJpMF8kFWaLpMR8kVTklC2zBhbLly/HyJEjy7WPHDkSw4YNw+uvv45x48ahU6dOyMnJsXaNRATAw0WFyb1Kj1roRWCJAx+1ICIiIvkxa2CxceNGNGzY0KRt7dq12Lx5M/r06YMff/wR7777Li5cuID//Oc/khRKRMBT3ULh5Vo658IPiWm4ejPfxhURERERlTJrYHHq1Cncd999Jm3ffvst3N3dsWnTJjzwwAOYOXMmXnrpJfz888+SFEr2Q6FQIDg4WBazEzgaT1cnTOgRBgDQ6kX8d0+SjSuyPuaLpMJskZSYL5KKnLJlVoVZWVkIDQ01aduzZw969eplciSjX79+SElJsW6FZHfkFHBH9HSPZvBwVgIANsan4prGsa5tYr5IKswWSYn5IqnIKVtmVejl5YW8vDzj7XPnziEvLw9xcXEm2zVo0ACcvdbx6XQ6nDlzBjqdztal1Eve7s4Y160ZAKBYp8eyvY511IL5IqkwWyQl5oukIqdsmTWwaNWqFX799Vfj7d9++w2CIKB3794m2129etW4Ojc5LlEUodFoOIi0ocm9wuDqVPrju/7wFWTmFdm4IuthvkgqzBZJifkiqcgpW2atvD1t2jQ8/vjjKCoqQkBAABYvXoymTZuWu+5ix44daNu2rSSFEtFdvg1c8HjnUKzYn4LCEj2+/CMZbwxpbeuyiIiIqB4z64jFY489htdffx3r16/He++9h8aNG2Pjxo1QKpXGbW7evImNGzeif//+khVLRHc91ycczsrSH+G1f17GrTvFNq6IiIiI6jNBtOC4SmFhIe7cuYNGjRqVu0+r1RqX+nZycrJqkfbKkiXOHYler0dWVhZ8fX1lcSGRI5v1419Ye/AKAOCl+5pjxqBWNq6o9pgvkgqzRVJivkgqts6WJZ93LRpYkKn6OrAg+5F6Kx99P9gNrV6Ep6sK+2feBy/X+jGwJyIiIulZ8nnXrGHPlStXLPpHjk2n0+H48eOymJ3A0QU3dMeDMUEAgLxCLVYfuGTbgqyA+SKpMFskJeaLpCKnbJl18XazZs0gCILZO5VDx6nmRFFEQUGBLGYnqA+m9W2O746mQi8Cy/9IwcQeYfBwMetH2y4xXyQVZoukxHyRVOSULbM/fXh5eeHRRx9FixYtpKyHiCzUzNcDD0Q3wY/H0nErvwRfH7qMZ3tH2LosIiIiqmfMGli88847+Oqrr/D555+jZ8+emDRpEsaMGQN3d3ep6yMiMzzfrzl+Op4OUQQ+35uCp7o1g6uTsvoHEhEREVmJWddY/Otf/0JSUhK2b9+Opk2bYurUqQgMDMSzzz6LQ4cOSV0j2RmlUonIyEiT6YbJtlo09sSQqAAAQNbtInxzWL7XOjFfJBVmi6TEfJFU5JQti+as6t+/P9auXYuMjAzMnz8fCQkJ6N69O9q2bYv169dLVSPZGUEQ4O3tbdF1NyS9F/rdPU1x2d5kFGnlea0T80VSYbZISswXSUVO2arRZLhqtRrTpk3DoUOH8M477+DcuXPYuHGjtWsjO6XVanHkyBFotVpbl0JltGnihQGt/QEAGZpCbDqaZuOKaob5IqkwWyQl5oukIqds1Whgce7cOfzzn/9ESEgI5s6diwEDBmDq1KnWro3sGGf+sk8v3Hf3qMWS3RdRotPbsJqaY75IKswWSYn5IqnIJVtmDyzu3LmD5cuXo0ePHmjdujW+++47TJ06FSkpKdi6dSsGDhwoZZ1EZIYOId7o1cIXAJB6qwA/HUu3cUVERERUX5g1sHj66acREBCA6dOnIzw8HL///juSkpLw9ttvIzg4WOoaicgCL/Uvc9Ri10Xo9PY/7zURERHJnyCasdqGQqGAl5cXRo4cCbVaXfUOBQGLFi2yWoH2zJIlzh2JYaEWNzc3WVxIVB89uuxPHEq5CQD4ZGxHPBDdxMYVmY/5IqkwWyQl5oukYutsWfJ51+yBhbkEQZDNeWC1VZ8HFjqdDkqlkm+edmr/xSw88WXpVNAtGzfA1um9oVDI47VivkgqzBZJifkiqdg6W5Z83jVrxKDX683+V18GFfWZTqdDfHw8X2s71j2iETo29QYAnL9+G9tOX7dtQRZgvkgqzBZJifkiqcgpWzWaFYqI7JsgCHipzAxRn+68ADMOThIRERHVmFUHFpcvX8bkyZOtuUsiqqG+rfwQFVR6yPJUei52n8u0cUVERETkyCwaWFy+fBmHDx9GVlaWSXtaWhqmTp2KVq1a4auvvrJqgURUM4IgmKzG/QmPWhAREZGEzBpY5OXl4f7770d4eDi6deuGoKAg/Otf/wIAvP/++2jZsiWWLVuGAQMG4NixY1LWS3ZAqVQiLi4OSqXS1qVQNQa1aYxWjT0BAIlXcnAgKdvGFVWP+SKpMFskJeaLpCKnbKnM2Wju3LnYtm0b+vXrh7i4OCQnJ2P+/PlITk7GunXr0LZtWyxevBi9e/eWul6yE8XFxXBzc7N1GVQNhULA8/c1x0vrEwEAn/x+AT2a+9q4quoxXyQVZoukxHyRVOSSLbOOWPzwww+YPHkyfv/9d7z//vvYuHEjPv30U6xbtw79+vXD0aNHOaioR3Q6HU6cOCGL2QkIGNYuEOG+HgCAQyk3cfjv9S3sFfNFUmG2SErMF0lFTtkya2Bx9epVPPzwwyZtjzzyCADg1VdfhbOzs/UrIyKrUCoETOvX3Hj7050XbFgNEREROSqzBhYlJSXw9vY2aTOswB0QEGD1oojIukZ2aIIQn9JDqPsuZOHY1RzbFkREREQOx+xZoSpb6Y+rS9ZPcriAiO5yUiowtc/doxaf2flRC+aLpMJskZSYL5KKXLIliGbMP6lQKODu7g6FwnQccvv27XLtgiBAo9FYv1I7ZMkS50S2VqTVoe8Hu5GhKQQA/PpST7RtorZxVURERGTPLPm8a9asUOPHj7dKYeQYRFGERqOBWq3mESsZcVEp8VzvcLyz+TQAYPGui1jyRKyNqyqP+SKpMFskJeaLpCKnbJk1sFi5cqXUdZCM6HQ6nD17FnFxcVCpzIoQ2YnHOjfFZ7uSkHW7CFtOXsOF63lo8fc6F/aC+SKpMFskJeaLpCKnbFm08jYRyZurkxLP9g4DAIhi6VELIiIiImvgwIKonnmiSygaujsBAH4+no5LWXdsXBERERE5ArsYWCxZsgRhYWFwdXVFbGws9u3bV+m2EyZMgCAI5f61bdvWZLuPP/4YrVq1gpubG0JCQvDKK6+gsLCwxs9LdwmCADc3N7s/z48q5uGiwqSepUct9CKwZLd9HbVgvkgqzBZJifkiqcgpWzYfWGzYsAEvv/wy3nrrLSQmJqJXr14YMmQIrly5UuH2ixYtQkZGhvHf1atX4ePjgzFjxhi3+frrrzFz5kzMnj0bZ86cwfLly7Fhwwa88cYbNX5eukupVCI6Olo2U59ReU91bwZP19LzNL9PSMPVm/k2rugu5oukwmyRlJgvkoqcsmXzgcXChQsxadIkTJ48Ga1bt8bHH3+MkJAQLF26tMLt1Wo1AgICjP/i4+Nx69YtTJw40bjNn3/+iR49euDxxx9Hs2bNMGjQIIwdOxbx8fE1fl66S6/X48aNG9Dr9bYuhWrIy9UJE7s3AwBo9SKW7U2ybUFlMF8kFWaLpMR8kVTklC2bXlpeXFyMo0ePYubMmSbtgwYNwoEDB8zax/LlyzFgwACEhoYa23r27Im1a9fi8OHD6Ny5M5KTk/Hbb78Zp82t6fMWFRWhqKjIeDs3NxcAoNVqodVqAZSu+aFQKKDX600CYGjX6XQou3RIZe1KpRKCIBj3W7YdKJ0hwJx2lUoFURRN2gVBgFKpLFdjZe339kmn0yEpKQne3t5wdnZ2iD5VV7sj9ml891As/yMFd4p12HDkKqb0DkOAl6vN+1RSUoKkpCSo1Wo4OTnV+9eJfbJen0RRNGbL8Fi598kRXye59kmn0yE5ORkNGzY0q3Y59AlwvNdJjn0yfO5Sq9Vwdnau8z5ZMqCx6cAiKysLOp0OjRs3Nmlv3Lgxrl27Vu3jMzIysGXLFqxbt86k/bHHHkNmZiZ69uwJURSh1WoxdepU40Cips87f/58zJkzp1x7YmIiPDw8AAB+fn6IiIhASkoKMjMzjdsEBwcjODgY58+fN1lAMDw8HP7+/jh58iQKCgqM7ZGRkfD29kZiYqJJSNq3bw9nZ2eToy8AEBcXh+LiYpw4ccLYplQq0alTJ2g0Gpw9e9bY7ubmhujoaGRlZSE5OdnYrlar0bp1a6SnpyM1NdXYfm+fRFFETk4OMjIyEBoa6hB9csTXyZw+PdktFMv2JKNEJ+LdTYcwvp2Hzft09epV5OTkICEhAf7+/nyd2Cer9alt27YoLi5GQkKC8VxluffJEV8nufbJsFhwbm4uLly44BB9csTXSY59MnzuSkhIQKdOneq8T2q1+YvpmrXydlhYmNkXjAiCgKQk806rSE9PR1BQEA4cOIBu3boZ2999912sWbPG5JtTkfnz52PBggVIT083juAAYPfu3Xjssccwb948dOnSBRcvXsT06dPxzDPP4O23367x81Z0xCIkJATZ2dnGlQjrw6hcp9MhISEBsbGxPGIh8z7dzC9Bz/d3orBED1cnBfa82hv+anebH7FISEhATEwMj1iwT1btkyiKOHLkCGJiYnjEgn2S5IhFYmIiYmNjTT4zyblPgOO9TnLsk+FzV0xMjE2OWNy+fRsNGza03srbffr0keRKdF9fXyiVynJHCW7cuFHuaMK9RFHEihUrMG7cOJNBBQC8/fbbGDduHCZPngwAaNeuHe7cuYNnn30Wb731Vo2f18XFBS4uLuXaVSpVuQVLDC/OvQxBMbe9soVQLGkXBKHC9spqrK5dEAR4e3sba3aEPt2rvvTJt4ELxnZuipX7L6GwRI+Vf17FzCGRFtdeWXtN+qRSqeDt7Q2VSmXcpr6/TlW1s0/m90mn0xmzde9zy7VPVdXIPtVtnwRBgFqthkKhqHD/cuyTgSO9TgZy6pPhc5fh68q2l6pPFW1TGbMGFqtWrTJ7h5ZwdnZGbGwstm/fjtGjRxvbt2/fjpEjR1b52D179uDixYuYNGlSufvy8/PLfROUSiVEUYQoirV6Xir9XrZu3drWZZCVPNc7Al8fvIJinR5r/ryE53qHo6GHc/UPlAjzRVJhtkhKzBdJRU7ZsvmsUDNmzMCXX36JFStW4MyZM3jllVdw5coVTJkyBQDwxhtv4Kmnnir3uOXLl6NLly6Iiooqd9+IESOwdOlSfPPNN0hJScH27dvx9ttv44EHHjCOFKt7XqqcXq9HamqqRRfzkP0KULtiTFwwAOBOsQ4rD1yyaT3MF0mF2SIpMV8kFTllq8YXb2s0Gpw/f97kAhSD3r17m72fRx99FNnZ2Zg7dy4yMjIQFRWF3377zTjLU0ZGRrm1JTQaDTZt2oRFixZVuM9Zs2ZBEATMmjULaWlp8PPzw4gRI/Duu++a/bxUOUPAAwICLDo8RvZrSp8IbDhyFVq9iJX7UzC5Vxi8XJ1sUgvzRVJhtkhKzBdJRU7ZMuvi7bK0Wi2mTJmC1atXl7twxKCydkeTm5sLtVpt1sUsjkSr1SI+Ph5xcXGVnhNI8vPaxuPYeLR0dojXBrfC8/2a26QO5oukwmyRlJgvkoqts2XJ512Lhz0fffQRNm/ejBUrVkAURXz22WdYtmwZ4uLi0KJFC2zZsqXGhROR7Uzr1xyKv+doWLr7Ir6Nv4o/k7Kh01v0twciIiKqpyweWKxZswZvvfUWxo4dCwDo0qULJk+ejEOHDiE0NBS7du2yepFkXxQKBfz8/Oz+cBxZJszXA7GhDQEAt4t0eP27Exj7xUH0fH8ntp7MqLM6mC+SCrNFUmK+SCpyypbFFSYnJyM6OtrYucLCQuN9U6ZMwddff2296sguKRQKREREyCLgZL6tJzNw5NKtcu3XNIWYujahzgYXzBdJhdkiKTFfJBU5ZcviCj08PFBcXAxBEODj44PLly8b73Nzc0N2drZVCyT7o9frkZSUJIvZCcg8Or2IOZtPV3if+Pe/t388hbzCEslrYb5IKswWSYn5IqnIKVsWXwESGRmJlJQUAED37t2xcOFC9OrVC87OzvjPf/6DVq1aWb1Isi96vR6ZmZkIDQ2VxeiZqnc45SYyNIVVbpN5uwjt3tkGP08XNGvkjqY+HqX/N3JHs0YeCG3kDm/32q1/odOL+PNiFg6dSEUXnSe6NfeDUmH9xTmpfuJ7F0mJ+SKpyClbFg8sHn30UZw/fx4AMGfOHPTu3ds4RauTkxO+//5761ZIRJK7kVf1oKKszLwiZOYVVXjalNrNCaGN3BHayAOhPu7Gr5s1coefp4txxdCKbD2ZgTmbTxsHOJ/GH0Gg2hWzR7TB/VGBlneKiIiI6pTFA4tp06YZv+7YsSNOnz6NH374AQqFAgMHDuQRCyIZ8vd0NWu7lo0b4FZ+CTLziiq8X1NQghOpGpxI1ZS7z81JidBG7mhaZsAR+vfRjhOpOXhhXSLunX/KcH3H0idjOLggIiKyc7WeDDckJAQvvfSSNWohmVAoFAgODrb7w3Fkvs5hPghUu+KaprDch3sAEFC6QveW6b2hVAi4U6TFlZv5uJx9B5ez83EpOx9Xbt7Bpax8ZGgKUNEMtQUlOpy9loez1/LMrkv8+7nnbD6NgW0CeFoU1Qrfu0hKzBdJRU7ZsniBvLIyMzMrXHm7adOmtSpKLurrAnnkmLaezMDUtQkAYDK4MHyUN/eoQZFWh9RbBbiSnY9Lfw88rtws/Tr1ZgGKdTW7+Gz9M13RLaJRjR5LRERENWPJ512Lj1jk5eXhlVdewfr1602mmi2rvqy8XV/pdDqcP38eLVu2hFKptHU5ZCX3RwVi6ZMxJtc5AKVHKiy5zsFFpUSEXwNE+DUod59OLyJDYxh05OPyzTv4Mym7wlOn7mXJdSBEFeF7F0mJ+SKpyClbFg8sXn75Zaxbtw6TJk1C+/bt4eLiIkVdZMdEUYRGo0EtDnaRnbo/KhAD2wTgcMpN3MgrhL+nKzqH+VjtFCSlQkBwQ3cEN3RH9+albX8mZWPsFwerfay514EQVYbvXSQl5oukIqdsWTyw+PXXX/Hee+9h+vTpUtRDRDamVAh1espRddd3AIC/pws6h/nUWU1ERERkOYuvAiksLES7du2kqIWI6iGlQsDsEW0A3L2e414CgJz84jqriYiIiCxn8cBi6NCh2LdvnxS1kEwoFAqEh4fLYnYCkgfD9R0BatPTnQynYF3PK8LTq47gTpHWFuWRg+B7F0mJ+SKpyClbFs8Kdfr0aTz88MOYPHkyRowYgUaNyp8y4eNTP05Z4KxQRNal04sm13c08XbFo8sO4lpu6YXbvVr4Yvn4TnBW2f+bKxERkSOw5POuxQMLw2ipqhV068usUPV1YKHT6XDy5ElERUXZ/ewEJD/35uvctTyM+e8B5BaWHq14ILoJPn60AxRc04IsxPcukhLzRVKxdbYknW72X//6V5WDCnJ8oiiioKBAFrMTkPzcm69WAZ5YMaETnvjyEIq0evx8PB2NGjjjX8Pb8L2ILML3LpIS80VSkVO2LB5YvPPOOxKUQURUubhmPlj8eAyeW3sUOr2IlfsvwbeBC57v19zWpREREdHfeKIyEcnCgDaNMf/BuzPSffC/c9hw5IoNKyIiIqKyzDpisXr1agwbNgyNGjXC6tWrq93+qaeeqnVhZL+USiUiIyN5DilJoqp8PRIXguzbxXh/61kAwBvf/wUfDxcMbNO4rsskGeJ7F0mJ+SKpyClbZl28rVAocPDgQXTu3Lnaqa4EQeDF20QkGVEUMe/XM1j+RwoAwEWlwJpJXbiAHhERkQQs+bxr1qlQKSkp6Nixo/Hrqv4lJyfXvgdk17RaLY4cOQKtlmsKkPVVly9BEPDW0NYY1aEJAKBIq8ekr47g7LXcuiyTZIjvXSQl5oukIqdsmXUq1OLFi/HSSy8hODgYoaGhAAC9Xi+LhTpIGvXlqBTZRnX5UigE/OfhaNzML8He85nIK9TiqeWHsWlqd4T4uNdRlSRHfO8iKTFfJBW5ZMuskcGCBQuQnp5uvK3T6eDk5ISEhATJCiMiqoqzSoGlT8QgOsQbAHAjrwjjVxxG9u0i2xZGRERUT5k1sKjoMgw5zKVLRI7Nw0WFlRM6IdzPAwCQnHUHT686gjtF9n+4mIiIyNHwXCaymFKpRPv27WUxOwHJj6X58vFwxuqnOyPAyxUAcDxVgylrj6JYq5eyTJIhvneRlJgvkoqcssWBBdWIs7OzrUsgB2ZpvoIbuuOrpzvDy7X0srF9F7Lwj43HodfzyCqZ4nsXSYn5IqnIJVtmDyzOnTuHhIQE4z8AOHv2rElb2fvIcel0OsTHx8vmQiKSl5rmq1WAJ1ZM6AQXVenb2s/H0zH3l9M8bZOM+N5FUmK+SCpyypZZs0IBwIQJE8q1jRs3zuS2KIr1ah0LIrIvcc18sPjxGDy39ih0ehGrDlyCn6cLnu/X3NalEREROTyzBhYrV66Uug4iIqsY0KYx5j/YDq9/dwIA8MH/zsG3gTMe7dTUxpURERE5NrMGFuPHj5e6DiIiq3kkLgTZt4vx/tazAIA3vv8LDd2dMahtgI0rIyIiclyCyBOQa8ySJc4diSiK0Ol0UCqVEATB1uWQg7FWvkRRxLxfz2D5HykAABeVAmsmdUHnMB9rlUoyw/cukhLzRVKxdbYs+bzLWaGoRoqLi21dAjkwa+RLEAS8NbQ1RnVoAgAo0uox6asjOHstt9b7JvniexdJifkiqcglWxxYkMV0Oh1OnDjBi/RJEtbMl0Ih4D8PR6N3Sz8AQF6hFk8tP4yrN/NrvW+SH753kZSYL5KKnLLFgQUROTRnlQJLn4hBdIg3AOBGXhHGrziM7NtFti2MiIjIwXBgQUQOz8NFhZUTOiHczwMAkJx1BxNXHcHtIq2NKyMiInIcHFhQjchhWXmSLyny5ePhjNVPd0aAlysA4ESqBlPXHkWxVm/15yL7xfcukhLzRVKRS7Y4K1Qt1NdZoYjk7Ny1PIz57wHkFpYerXggugkWjIlG/OVbuJFXCH9PV3QO84FSwVldiIiILPm8y4FFLdTXgYUoitBoNFCr1ZxSj6yuLvIVf+kmnvjyEIr+Plrh7qxEfvHdi+IC1a6YPaIN7o8KlOT5yTb43kVSYr5IKrbOFqebJUnpdDqcPXtWFrMTkPzURb7imvlg8eMxMByUKDuoAIBrmkJMXZuArSczJKuB6h7fu0hKzBdJRU7Z4sCCiOqlfpH+8HR1qvA+w2HcOZtPQ6fnQV0iIiJzcGBBRPXS4ZSb0BSUVHq/CCBDU4jDKTfrrigiIiIZ48CCLCYIAtzc3HgOKUmirvJ1I6/QqtuR/eN7F0mJ+SKpyClbKlsXQPKjVCoRHR1t6zLIQdVVvvw9Xc3azsfdWeJKqK7wvYukxHyRVOSULR6xIIvp9XrcuHEDej3n/yfrq6t8dQ7zQaDaFdX9/Wfer6dxMk0jaS1UN/jeRVJivkgqcsoWBxZkMb1ej+TkZFkEnOSnrvKlVAiYPaINAFQ5uDh3/TZGLd6PhdvPczE9meN7F0mJ+SKpyClbHFgQUb11f1Qglj4ZgwC16WlRgWpXvDkkEpEBngAArV7EJ79fwAOf/cGjF0REVGd0ehEHk29if2oRDibftPuZCnmNBRHVa/dHBWJgmwAcTrlZbuXtCT3C8Nmui1iy6yK0ehFnr+Vh1OL9mNavOV7o1xzOKv5thoiIpLH1ZAbmbD6NDE3pJCKfxh+x+wVc+VuRLCYIAlcWJcnYIl9KhYBuEY0wskMQukU0gvLvlfOcVQrMGNgSPz7fo9zRi5GL9+NUOo9eyAnfu0hKzBdZ09aTGZi6NsE4qDCw9wVcBVEU7fuYih2zZIlzIpK3Yq0en+28gMW7k4yHolUKAc/3a47nefSCiIisRKcX0fP9neUGFQYCgAC1K/74533GP4RJyZLPu/xNSBbT6/VITU2VxUVEJD/2mi9nlQIzBrXCT/ccvVjEoxeyYa/ZIsfAfJG1HE65WemgArDvBVw5sCCL8c2TpGTv+YoKUuPnF3rixfuaG/9SdCYjFyM/24+Pd5xHic4+6yb7zxbJG/NF1nCnSIsNR66Yta09LuDKgQURkYWcVQq8OqgVfpzWA60a3z168fGOCxj52X6cTs+1cYVERCQnuYUl+GznBfR8fyd+PJZu1mPMXei1LnFgQURUQ+2C1fj5xR54od/doxenM3LxwGd/YNGOCzx6QUREVbp1pxgLt51Dj/d24sNt53Erv6TaxwgonRa9c5iP9AVaiAMLsphCoYCfnx8UCsaHrE9u+XJRKfGPwa3ww7TuJkcvPtpxHqMW78eZDB69sBdyyxbJC/NFlriRV4j5v51Bj/d34pOdF5FXqAUAKARgdMcg/Gt4Gwgov4Cr4fbsEW3q5MJtS3FWqFrgrFBEVFaRVodPf7+IpXvuzhzlpBTw4n0tMLVvBJyU/MBBRFSfpecU4PO9yVh/+AqKtHePaqsUAh6KCcbUvhFo5usBoPw6FgBsso6FJZ93ObCohfo6sNDr9UhJSUFYWBj/MkNW5wj5OpGag39sPI7z128b29o28cKHY6LROrD+vFfYG0fIFtkv5ouqciU7H0v3XMR3R1NRorv70dtZpcBjnULwXJ8IBHm7lXucTi/iUHIWTiWlom1EMLqE+9b5kQpON0uS0uv1yMzM5MwXJAlHyFf7YG9sfrEnnu8XYfwFcCq99NqLT36/e+2FTi/iz6Rs/HQsDX8mZRuPcpA0HCFbZL+YL6rIxRt5mLHhGPot2I31h68aBxVuTko80ysMf7zeD3NHRlU4qABKF3Dt3Kwh2qmL0LlZQ7s8/aksla0LICJyRC4qJV4bHInBbQPw6rfHceHGbZToRCzcfh7bTl/DqI5BWL4vxeaHuImIyPpOp+di8a6L+O1kBsqeG+TposL47s3wdM8w+Hg4265AiXBgQUQkofbB3vjlpZ5YtOMC/rsnCXoROJmWi5Np5S/qvqYpxNS1CVj6ZAwHF0REMnTsag4+23kBO87cMGn3dnfC0z3CML57M6jdnGxUnfQ4sCCLKRQKBAcH8xxSkoQj5stFpcTr9xuOXhzDxcw7FW4nonTGjzmbT2NgmwC7P+QtN46YLbIfzFf9djjlJj7deQH7LmSZtPs2cMYzvcLxRNdQNHCp2cduOWWLF2/XQn29eJuIam7P+RsYv+JItdutf6YrukU0qoOKiIioOjq9iMMpN3EjrxD+nqVrSCgE4I+LWfh050UcTrlpsn2Alyum9AnHY52bwtVJaaOqrcOSz7s8YkEW0+l0OH/+PFq2bAmlUt4/LGR/HD1fOWYsfgQAF67ncWBhZY6eLbIt5stxVTTta0N3J3i5OeFydr7JtiE+bpjWtzkejAmCi8o6OZBTtuzimMqSJUsQFhYGV1dXxMbGYt++fZVuO2HCBAiCUO5f27Ztjdv07du3wm2GDRtm3Oadd94pd39AQICk/XQUoihCo9GAB7tICo6eL39PV7O2m/3zKUxceRhbT2agWMtZZqzB0bNFtsV8OaatJzMwdW2CyaACAG7ll5gMKsL9PLBgTDR2vtoXYzs3tdqgApBXtmx+xGLDhg14+eWXsWTJEvTo0QPLli3DkCFDcPr0aTRt2rTc9osWLcJ7771nvK3VahEdHY0xY8YY277//nsUFxcbb2dnZ5fbBgDatm2LHTt2GG/b+yiQiOSvc5gPAtWuuKYpRFW/IkQAu85lYte5TDTycMbojkF4pFMIWv69ujcREUlLpxfxr59OVflerVIIWPhINIa1b8Lr4mAHA4uFCxdi0qRJmDx5MgDg448/xv/+9z8sXboU8+fPL7e9Wq2GWq023v7xxx9x69YtTJw40djm4+Nj8phvvvkG7u7u5QYWKpWKRymIqE4pFQJmj2iDqWsTIAAmv7AMt4e2C8SxK7eQ/vdfyLLvFOPLP1Lw5R8piA7xxqNxIRgRHQhPV8edWYSIqK6JooiUrDs4cukmDqXcxL4LmcjMK67yMVq9CD9PVw4q/mbTgUVxcTGOHj2KmTNnmrQPGjQIBw4cMGsfy5cvx4ABAxAaGlrlNo899hg8PDxM2i9cuIAmTZrAxcUFXbp0wb///W+Eh4db3pF6RqFQIDw8XBazE5D81Id83R8ViKVPxpQ7ZzegzDoWOr2I/Rez8G38VWw7dR3Ffy+qd/xqDo5fzcHcX05haLtAPBIXgi5hPhAE/lKrTn3IFtkO8yU/Or2IMxm5OHLpJo5cuonDKbeQdbvI4v3cyCusfqNakFO2bDqwyMrKgk6nQ+PGjU3aGzdujGvXrlX7+IyMDGzZsgXr1q2rdJvDhw/j5MmTWL58uUl7ly5dsHr1arRs2RLXr1/HvHnz0L17d5w6dQqNGlV8wWRRURGKiu4GLje3dB56rVYLrVYLoPTFVygU0Ov1JqtvGtp1Op3JOXKVtSuVSgiCYNxv2Xag9EIec9pVKhVEUTRpFwQBSqWyXI2VtVfUp7JHhRylT1XVzj7VbZ98fHyM9ztKn+6tcVCbxujX0hdHLt1CZl4RGqtd0SXcFxD1xjq7hzdEz+aNoCnQ4ofEVGyMT8WZa3kAgMISPb5PSMP3CWkIbeSOhzoG4cGYJghq6GGzPsnhdfL19TWp0xH65Iivk1z75O/vD1EUTfYj9z450utUpNXjZJoGR69qcDj5JuIv38LtItN9mfRdIUCrr/66hkbuTuU+B1q7T4bfi4Y/ItXl62TJavI2PxUKQLm/tImiaNZf31atWgVvb2+MGjWq0m2WL1+OqKgodO7c2aR9yJAhxq/btWuHbt26ISIiAl999RVmzJhR4b7mz5+POXPmlGtPTEw0Hg3x8/NDREQEUlJSkJmZadwmODgYwcHBOH/+PDQajbE9PDwc/v7+OHnyJAoKCoztkZGR8Pb2RmJioklI2rdvD2dnZ8THx5vUEBcXh+LiYpw4ccLYplQq0alTJ2g0Gpw9e9bY7ubmhujoaGRlZSE5OdnYrlar0bp1a6SnpyM1NdXYfm+fRFFEXl4eIiMj0bRpU4fokyO+TnLt09WrV5GXlwdPT0/4+/s7RJ+qep1UAAIBBLsGQ6kQcOZMxX2K9cxD6y5OuKRRY9flQvyZrkVeUWlfLmfnY+GOC/hoxwX0au6DRzs1hfrOFajKHJpn9twQFRWFI0eOmPyOkXufHPF1kmufFAoFXFxcEBISgvPnzztEn+zxdfL0UmPttkPIztfC20WB1r4qdIiOrrBPrdt1wJHkLPx65DzOZpfg4i0tSqr4jOzpokK7QA80dS9BZCMVwtRKvLozF9n5ugqvsxAA+HqooMhOQvzN5Br3qbrXyfC5y9PTE506darz16nsJQjVsek6FsXFxXB3d8fGjRsxevRoY/v06dNx7Ngx7Nmzp9LHiqKIli1bYvjw4fjoo48q3CY/Px+BgYGYO3cupk+fXm09AwcORPPmzbF06dIK76/oiEVISAiys7ON8/ray6jcQIoRrE6nQ0JCAmJjY+Hs7OwQfaqudvap7vpUUlKChIQExMTEwMnJySH6JMXrVKIHtp2+jg2Hr+BAsun86QDg4+GMUR0C8XBMMFo2blCuTzq9iCOXbiE7vwT+ni6ICVEbzxF21OyJoogjR44gJibG+Fi598kRXye59kmn0yExMRGxsbEmfxyVc58A+3qdtp/JxNxf7jmF1MsFs0e0xf1RAcjMLcDRy7dw5PItHLl0C6cz8qCr4ohDIw9nxDVriM7NGqJTMx+0DfKGANGkxm2nr+P5dccAlL8mDgAWP94Rg9r417hP5rxOhs9dMTExcHZ2NraXJeXrdPv2bTRs2ND+17FwdnZGbGwstm/fbjKw2L59O0aOHFnlY/fs2YOLFy9i0qRJlW7z7bffoqioCE8++WS1tRQVFeHMmTPo1atXpdu4uLjAxcWlXLtKpYJKZfqtNLw49zIExdz2e/dbk3ZBECpsr6xGc9oFQTB+7Sh9Kot9sl2fDG+ySqXSuI3c+yTF66QCMLJDEEZ2CMLVm/nYeDQV38VfNV7wffNOMVbsv4wV+y8bL/geHh0IL1enCudkDyxzfYet+mTsm0Svk1arNWbL3Pdse+9TVTWyT+xTZTVa2m4Pfdp6MgPTvk4od+TgWm4Rpn6dgEAvV2TkVn2tQ3BDN3QO80HnZj7oHOaDMF+PCs6QEUxqHNo+CEsViiqviauudnPaq3s9DO9dhnrr8nWqaJvK2PxUqBkzZmDcuHGIi4tDt27d8Pnnn+PKlSuYMmUKAOCNN95AWloaVq9ebfK45cuXo0uXLoiKiqp038uXL8eoUaMqvGbiH//4B0aMGIGmTZvixo0bmDdvHnJzczF+/HjrdpCISGIhPu6YMbAlpvdvUe0F39HB3jiUUv4IxzVNIaauTcDSJ2Mq/UVJRGQLOr2I2T9XPe1rRYOKlo0boNPfg4jSqb7davT890cFYmCbgHIrb3MmqPJsPrB49NFHkZ2djblz5yIjIwNRUVH47bffjLM8ZWRk4MqVKyaP0Wg02LRpExYtWlTpfs+fP48//vgD27Ztq/D+1NRUjB07FllZWfDz80PXrl1x8ODBKmeXolJKpRKRkZGVjrqJaoP5qjmlQkDvln7o3dIPOfnF+DExDRviU3Emo3SiicISfYWDCqD0EL8AYM7m0xjYJsAhf2EyWyQl5st6sm8X4VR6Lk6l5+JkugZHL93C9dzqZ2uK8PPAfZH+6NTMB52a+aChh7PValIqBHSLqHhyH6nJKVs2vcZC7nJzc6FWq80654yIyFZOpmnwbfxVfBd/FflVXbn4t/XPdLXZL1Aiqj9EUUS6phCn0jQ4mZ6L0+kanErPLbfKtbkWPdYBIzsEWblKsuTzrs2PWJD8aLVaJCYmomPHjpWeE0hUU8yX9UUFqREVpEb7YDX+sfFEtdvvOHMdMaHecFHZ/1/HLMFskZTqU750etHi04L0ehEp2XdKj0Skaf4+IqHBrfySap/PSSGgxIxpX/09Xc3ug5zIKVv2XR3ZrXtnIyCyJuZLGkHe7mZtt/yPFGyMv4ph7ZtgVIcm6NTMBwoHOTWK2SIp1Yd8mTP5Q7FWjws38kwGEWcycnGnuPrvj6eLCm2aeKFtEzWigkr/b9bIHX0/3I1rmsJKp30NUJcOcByVXLLFgQURUT1RevGia6W/nMvKLdRi/eErWH/4CoK83TCyQxOM6hiElo0966RWIjnR6UUcTL6JQ6lF0PrcRLfmfg55ndLWkxmYurb8zEwZmkJMWZuAHhGNoCkswflrt42TR1TFt4EL2jbxMg4g2jbxQkhD9wr/kDF7RBtMXZsAARVP+zp7RBuH/J7LDQcWRET1hFIhVPvL+fl+EUjLKcTWk9dQUFL6F7K0nAIs2Z2EJbuT0CbQC6M6NsED0UEIUDvmaQdElrj3L/ifxh+pcPpmKdTklCRLlej0uJ5biNRbBZi56a8q/yixPym70vuCG7qVDiKaqNE2qPR/fy/z30PujwrE0idjLJ72leoWL96uhfp68bYoiigoKICbm5tZK6QTWYL5kp45pzLcKdJi++nr+PFYGvZdyCq3yJQgAN3CG2FUxyDcHxUAL1enOu1DTTBbZG2V/QXfkC4pp282dz2aquj1IjJvFyE9pwAZmkLj/xmaAqTnlP5/I68IlnxSFABE+De4O4ho4oU2Tbzg7W6dGZrqYjBlb2z93mXJ510OLGqhPg8sdDqdyUItRNbCfNUNS345Z90uwi/H0/HDsXQcv5pT7n5nlQIDWzfGqI5B6NPSD84q8xdTqkvMFlmTTi+i5/s7K53ByHDe/x//vM/qH3zNGdAMbhuAW/klFQ4WMnIKka4pwDVNIbRmXBRtiQ8ebo8xcSFW3Wd9Z+v3Lg4s6kh9HVhotVrEx8cjLi7O7mcnIPlhvuxbStYd/JiYhp+OpeFSdn65+73dnTCsXSBGdQxCbNOG5c6VtuVfG5ktsqY/k7Ix9ouD1W6nEABXJyWcVQo4KxVwUirgolKU3i7T5nxPm/M9bYbHqRQCFu+6iNxCbaXPqVQIUCmAIm3tPuL5ebqgidq1dGE5QcTWk9erfQynq7Y+W793cbpZIiKSRJivB14Z2BIvD2iBY1dz8NOxdGw+no7sO8UAgJz8Enx96Aq+PnQFwQ1LL/oe3TEIzf3/v707D4uq3v8A/j7AMKyyw4DsqLiAgLmBC5rhkmJmKum1IJenm2al2aaVWD7dm+bN7u1mu6ZdEzOU+pm5JNrN3eu+xBUUxAVBkE1kne/vD5y5jgwwMAzDwPv1PPPonPOdc75zzsfj+cx3OfYt0nXDFHXErhvtXUl5Fb7/T7ZOZZUCKKusQZkOMyK1lBqlQGNjpx1tZPB0sIaXgxW8HK3h6WgFLwdreN5779HJSqP1UdVC05FnZqLGMbEgIqImkyQJEb5OiPB1wuKxPfB7+i1sPXENO8/dVA/6vnr7Lv6ZmoF/pmbAx8ka2bfv1tlOTlE5nvv2uEH7ohtTR02m2qus/DtYeyAT3x+7itKK+lsM7ufvagNLczNUVitrXzUCldU1qKypfd/CPZHUPDrJEazopG5xUCcOjlbwdLCCjWXTbgF1mfyBMzMREwsiItKLzNwMw4PdMTzYHXcqqrHzfA62nLiO3y/mqW+atCUVQO3NiQRg6U/nEdNT0a5uSurrB9/ek6n21kIjhMDBjHx8vT8Tv/5xU+eBzKpf8H9dMKzB71+jFOqko6KmBpXVSlTViPsSkRpUVgt1InL2WhE++vVio/tfFRfR4l2SODMTNYZjLPTQUcdYGHsQEbVvjK/2I7ekHP936gb+dSgLGbfuNFr+25n9Mbirm8Hq05qxVaMUiPrrr7hZXFFvGU8DDew1pvbUQlNeVYOUk9ewZn8m/sgp0VgntzDDxD61XfyW/d95ANp/wTdE8qhrlyRDxlZ7Sx7bOmP/v8jB262kIycWnLKRDIXx1f6knLyGFzeebLSclcwMI7p7IDrYDcO6uTVpjntdGDq2qmqUOJVdiP3p+dh+9kadm1FtfJys0dvbEUHudujibocubnYIdLOFlcxc7/q09s2fMadebUk5ReVYfygTGw5fwe2yKo11ik5WeCrSD1P7+8LZtnb6VGMkU6pjDbReQkPGY+z/F5lYtJKOmlgYe3YCat8YX+2PrrPnPKiXVycMC3bDsGB3RPg4wsJcv2lsWzq2lEqBtJsl2J9+Cwcy8nH4Uj7utMAAXUmqfZhYF7d7yYa7HYLu/V3XZwG09s2uMadebSknrtzGmv2Z+PnMjTpTsPbxdcQzgwIwOkQBmZY4rFEKHEzPw+HTFzCgd49WefJ2e2odooYZ+/9FzgpFRERtRv8AZ3g6WNXbdQOoba2wNDfTmELz3PVinLtejH+mZqCTlQWGdKttyYgOdoO7vXGe+p1dUIbf029hf/otHMzIV8+G1VzmZqgze48QQHbBXWQX3EVqWp7GOlc7S3WSEXRf4uHpYKX+JbOlxnbUKAWK71ah8G4VbpdVorCsEoVlVbhdVoWiskrcLqtdXnS3Ctm3y+pNKoDaX9VvFJUj9Y9cPNLTQ5dD0yqqapTYfjYHa/ZfxokrhRrrLMwkjO3tiWcGBSDcx7HB7ZibSRgY6AyLAjn6BrZOt6DRIZ6I6alglyRqU5hYEBGRQekym8yquHA80sMDp64WYm9aHlLTcnH2WrG6XHF5NbadvoFtp28AAEI6d8Kwbu4Y3t0N4T5OBruZulVagQMZ+TiQfgv7M24hu0D7IHQAcLWTIyrIBYO6uGBAgAumfnGo0X7wqQuH4VrhXaTnliI9txQZuaXIyKv9u7bWj1ullbhVWoDDlws0lttamiPI3Q6BrrbYfSFX6z5VyxZtOYu7lTUoqajG7TtVKLyrShhq/ywsq0Th3SoU3a1q0hOXdTFr3TH4udggtLMDwrwdEepd+2Rm+1Z+cvvtO5XYcOQK1h/MQk6xZkLkbGuJaf19MX2gHxQOxklgdWVuJvGZEdSmsCuUHjpyV6gTJ04gIiKCXVWoxTG+2q+mdt3ILSnHb/+9hdS0XPz7v3n1PhDMwVqGIV1dMTzYHUO7ucHNXl6njKqrytGzaegXElxvV5XSimocvpSP/en5OJBxq8FxEnZyCwwMdEZUkCsGdXFFNw87jf7P+vSDF0Igp7j8fwnHvWQjPfcObpXWPyDcWMwkNHvaVEkCAl1t0dvbsTbh8HFAT08HWFs2bZyJLmNK0nJKsGb/ZWw5cQ0V1ZpNRd0V9nhmkD8eC+/crDEuvHaRoRg7tjjGopV01MSCiKi5mjuguLpGiZPZhUhNy8XetDycu15cb9nQzg7qsRnhPo7YdT6n3oRmeHd3nLhSeK9FIh8nswtRU88dsqW5Gfr4OWJQkCuiurgizNuh0XEfhugHX1RWhfS8knsJxx118pF9u6xFWhg6WVnAydYSjtYyONpYwsmm9k9HGxkcrWW162xq1zvZWMLRVgZrC3MMXZHaYHc3G0tzBHvY4fyNkjo39Q8yk4BuHvYI7eyA3t4OCPV2RA9Pe8gttN/wN3ScR/ZUYM8fuVhz4DL2p+drfE6SgBHdPTBjsD8iA104YQSRFkwsWklHTSyEECgqKoKDgwMvwtTiGF+ki9zicuz9bx72peXht4t5KKmnNcPG0rzBJx7LzCVU1Wj/b1CSapOU2hYJF/T1c27yr+hA683OVF5Vg60nruH15DONlp0+0BcRPk61ycK9pMHJxhIO1rJm103XFpqqGiUu3izFmWuFOH21CGeuFeHCjeJ6z4OKzFxCsMIeoZ0da5ONzg4IVtjj1ws3652NSgBws7NEXqnmWBg7uQWm9PVBfJQf/Fxsm/V9H8RrFxmKsWOLiUUr6aiJhbFnJ6D2jfFFTVVdo8TxK4XYe6814/yN+lszGhPoZotB9xKJgYEuOs/C1FYY+xkHzW2hqaiuwX9zSnH6WiFOZxfh9LUi/PdmSb2tRyoy89rv0FhSouLnYoOEKH9Mesi7xcd18NpFhmLs2OKsUERE1GFYmJuhf4Az+gc449XR3XGzuBz70vLww/FsHL58u9HPD+nqignhnRHVxQWeDtatUGPD0WWg/JLYngYb7N7cmYrkFuYI9XZAqLcD/jSgdll5VQ3O3yjGmatF91o2CnExt1Sju5euCUWIVyfMj+mG4cHuMOOsSUQGw8SCiIjaFY9OVpjSzwdymZlOicWkh7zxWHjnVqhZ6xgd4onV0/vUaTlQtNIzDlpqpiIrmTn6+Dqhj6+Tetmdimqcu16M01cLceZaEQ5m5CO3pPHB7LOHBmJEj7YzzS1Re8XEgppMkiQ+FZkMhvFFLUXXZ10Y65kYhtRen3FgK7dQt04Buj98sTXOMa9dZCimFFtMLKjJzM3NERYWZuxqUDvF+KKW0tiD+VTjDVQ3qe1NR3jGQVs6x7x2kaGYUmw1PE8ekRZKpRK5ublQKhueLpCoORhf1FJU4w2A/40vUGmN8QZkeG3pHPPaRYZiSrHFxIKaTKlU4tKlSyYR4GR6GF/UklTjDR58grLCwarBB9SR6Wgr55jXLjIUU4otdoUiIqJ2TTXe4GB6Hg6fvoABvXvU++RtMk3tdUwJkalhYkFERO2euZmEgYHOsCiQo28gbzjbo44wpoSorWNXKGoySZL4ZFEyGMYXGQpjiwyJ8UWGYkqxxSdv66GjPnmbiIiIiDqGptzvssWCmkypVOLq1asmMYiITA/jiwyFsUWGxPgiQzGl2GJiQU1mSgFOpofxRYbC2CJDYnyRoZhSbDGxICIiIiIivTGxICIiIiIivTGxoCYzMzODm5sbzMwYPtTyGF9kKIwtMiTGFxmKKcUWZ4XSA2eFIiIiIqL2jLNCkUEplUpkZGSYxCAiMj2MLzIUxhYZEuOLDMWUYouJBTWZUqlEXl6eSQQ4mR7GFxkKY4sMifFFhmJKscXEgoiIiIiI9GZh7AqYMtXwlOLiYiPXpHVVV1fjzp07KC4uhoUFQ4haFuOLDIWxRYbE+CJDMXZsqe5zdRmWzcjXQ0lJCQDAx8fHyDUhIiIiIjKckpISODg4NFiGs0LpQalU4vr167C3t4ckScauTqspLi6Gj48PsrOzORsWtTjGFxkKY4sMifFFhmLs2BJCoKSkBF5eXo1OecsWCz2YmZnB29vb2NUwmk6dOvHiSQbD+CJDYWyRITG+yFCMGVuNtVSocPA2ERERERHpjYkFERERERHpjYkFNZlcLseSJUsgl8uNXRVqhxhfZCiMLTIkxhcZiinFFgdvExERERGR3thiQUREREREemNiQUREREREemNiQUREREREemNiQTpLTEyEJEkaL4VCYexqkQn67bffEBsbCy8vL0iShK1bt2qsF0IgMTERXl5esLa2xrBhw3Du3DnjVJZMTmPxlZCQUOdaNnDgQONUlkzKX/7yF/Tr1w/29vZwd3fHhAkTkJaWplGG1y9qDl1iyxSuXUwsqEl69eqFGzduqF9nzpwxdpXIBN25cwdhYWH4+OOPta5fvnw5/va3v+Hjjz/G0aNHoVAoEBMTg5KSklauKZmixuILAEaPHq1xLfv5559bsYZkqvbt24e5c+fi0KFD2LVrF6qrqzFy5EjcuXNHXYbXL2oOXWILaPvXLj55m5rEwsKCrRSktzFjxmDMmDFa1wkhsGrVKixevBgTJ04EAHzzzTfw8PDAhg0b8Oyzz7ZmVckENRRfKnK5nNcyarJffvlF4/2aNWvg7u6O//znPxg6dCivX9RsjcWWSlu/drHFgprk4sWL8PLyQkBAAJ588klcunTJ2FWiduby5cvIycnByJEj1cvkcjmio6Nx4MABI9aM2pO9e/fC3d0d3bp1w+zZs5Gbm2vsKpEJKioqAgA4OzsD4PWLWs6DsaXS1q9dTCxIZwMGDMC6deuwY8cOfPHFF8jJyUFUVBTy8/ONXTVqR3JycgAAHh4eGss9PDzU64j0MWbMGPzrX//Cnj17sHLlShw9ehQPP/wwKioqjF01MiFCCCxYsACDBw9GSEgIAF6/qGVoiy3ANK5d7ApFOru/a0FoaCgiIyMRFBSEb775BgsWLDBizag9kiRJ470Qos4youaIi4tT/z0kJAR9+/aFn58ftm3bpu6+QtSY559/HqdPn8bvv/9eZx2vX6SP+mLLFK5dbLGgZrO1tUVoaCguXrxo7KpQO6LqO/rgr3u5ubl1fgUkagmenp7w8/PjtYx0Nm/ePPz4449ITU2Ft7e3ejmvX6Sv+mJLm7Z47WJiQc1WUVGBCxcuwNPT09hVoXYkICAACoUCu3btUi+rrKzEvn37EBUVZcSaUXuVn5+P7OxsXsuoUUIIPP/880hOTsaePXsQEBCgsZ7XL2quxmJLm7Z47WJXKNLZwoULERsbC19fX+Tm5mLZsmUoLi5GfHy8satGJqa0tBTp6enq95cvX8bJkyfh7OwMX19fvPTSS3jvvffQtWtXdO3aFe+99x5sbGwwbdo0I9aaTEVD8eXs7IzExEQ88cQT8PT0RGZmJhYtWgRXV1c8/vjjRqw1mYK5c+diw4YNSElJgb29vbplwsHBAdbW1pAkidcvapbGYqu0tNQ0rl2CSEdxcXHC09NTyGQy4eXlJSZOnCjOnTtn7GqRCUpNTRUA6rzi4+OFEEIolUqxZMkSoVAohFwuF0OHDhVnzpwxbqXJZDQUX2VlZWLkyJHCzc1NyGQy4evrK+Lj48WVK1eMXW0yAdriCoBYs2aNugyvX9QcjcWWqVy7JCGEaM1EhoiIiIiI2h+OsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSAiIiIiIr0xsSCidmnt2rWQJAlWVlbIysqqs37YsGEICQkxQs2AvXv3QpIkbN682Sj7b6rMzEyMHTsWzs7OkCQJL730Ur1l/f39MW7cuBbZ74EDB5CYmIjCwsIW2V5DysrKkJiYiL179+pUPjMzE5IkaX317du3xev3ySefYO3atS2+3Zbw73//G3K5XOPfWX3/vn7++WfY2NggMjISt2/fRlVVFYKCgrBq1apWrDERGYqFsStARGRIFRUVePPNN7F+/XpjV8VkzZ8/H4cPH8bXX38NhUIBT0/PVtnvgQMHsHTpUiQkJMDR0dGg+yorK8PSpUsB1N4U62revHmYNm2axjI7O7uWrBqA2sTC1dUVCQkJLb5tfQgh8NJLL2H27Nnw8/NrsOx3332H+Ph4REdHY+vWrbC1tQUAvP3225g/fz6eeuopuLi4tEa1ichA2GJBRO3a6NGjsWHDBpw6dcrYVWl1d+/ehRBC7+2cPXsW/fv3x4QJEzBw4MBGbyA7El9fXwwcOFDjZayWsKaqqalBRUWFXtv45ZdfcPz4ccybN6/BcqtXr8b06dMRGxuLbdu2qZMKAJg6dSokScJnn32mV12IyPiYWBBRu/bqq6/CxcUFr732WoPlVF1btHU3kSQJiYmJ6veJiYmQJAmnT5/G5MmT4eDgAGdnZyxYsADV1dVIS0vD6NGjYW9vD39/fyxfvlzrPsvLy7FgwQIoFApYW1sjOjoaJ06cqFPu2LFjGD9+PJydnWFlZYWIiAhs2rRJo4yq69fOnTsxY8YMuLm5wcbGpsEbxytXrmD69Olwd3eHXC5Hjx49sHLlSiiVSgD/67KVnp6O7du3q7v6ZGZmNngsG7Nr1y489thj8Pb2hpWVFbp06YJnn30Wt27dUpdJTEzEK6+8AgAICAhQ7/v+rkpJSUmIjIyEra0t7OzsMGrUqDrHLyEhAXZ2dkhPT8ejjz4KOzs7+Pj44OWXX1Yfm8zMTLi5uQEAli5dqt6XPq0D5eXlePnllxEeHq6Oj8jISKSkpNQpq1Qq8Y9//APh4eGwtraGo6MjBg4ciB9//BFAbfeyc+fOYd++feq6+fv7qz/f2HlUfUdJkrB8+XIsW7YMAQEBkMvlSE1NhVKpxLJlyxAcHKzef+/evfHRRx81+j1Xr16Nfv36ITg4uN4y7733HubMmYOEhARs2rQJlpaWGustLS0RFxeHzz//vEUSYSIyHiYWRNSu2dvb480338SOHTuwZ8+eFt32lClTEBYWhh9++AGzZ8/Ghx9+iPnz52PChAkYO3YstmzZgocffhivvfYakpOT63x+0aJFuHTpEr788kt8+eWXuH79OoYNG4ZLly6py6SmpmLQoEEoLCzEp59+ipSUFISHhyMuLk5rEjRjxgzIZDKsX78emzdvhkwm01r3vLw8REVFYefOnXj33Xfx448/4pFHHsHChQvx/PPPAwD69OmDgwcPQqFQYNCgQTh48CAOHjyod1eojIwMREZGYvXq1di5cyfefvttHD58GIMHD0ZVVRUAYNasWepfwZOTk9X77tOnD4Dam9WpU6eiZ8+e2LRpE9avX4+SkhIMGTIE58+f19hfVVUVxo8fjxEjRiAlJQUzZszAhx9+iPfffx8A4OnpiV9++QUAMHPmTPW+3nrrrUa/i1KpRHV1tcZLCIGKigoUFBRg4cKF2Lp1K7777jsMHjwYEydOxLp16zS2kZCQgBdffBH9+vVDUlISNm7ciPHjx6sTuC1btiAwMBARERHqum3ZsgWAbufxfn//+9+xZ88efPDBB9i+fTu6d++O5cuXIzExEVOnTsW2bduQlJSEmTNnNjq2pbKyErt378bw4cPrLfPKK69g8eLFePnll/HVV1/B3Nxca7lhw4YhKysLZ8+ebXCfRNTGCSKidmjNmjUCgDh69KioqKgQgYGBom/fvkKpVAohhIiOjha9evVSl798+bIAINasWVNnWwDEkiVL1O+XLFkiAIiVK1dqlAsPDxcARHJysnpZVVWVcHNzExMnTlQvS01NFQBEnz591PURQojMzEwhk8nErFmz1Mu6d+8uIiIiRFVVlca+xo0bJzw9PUVNTY3G93366ad1Oj6vv/66ACAOHz6ssfy5554TkiSJtLQ09TI/Pz8xduxYnbbblLJCCKFUKkVVVZXIysoSAERKSop63YoVKwQAcfnyZY3PXLlyRVhYWIh58+ZpLC8pKREKhUJMmTJFvSw+Pl4AEJs2bdIo++ijj4rg4GD1+7y8vDrnuSGqeNH22rVrV53y1dXVoqqqSsycOVNERESol//2228CgFi8eHGD++vVq5eIjo6us1zX86iqb1BQkKisrNQoO27cOBEeHq7T977f4cOHBQCxcePGOuuio6PVx2PatGmNbuvixYsCgFi9enWT60FEbQdbLIio3bO0tMSyZctw7NixOl2I9PHg7Ec9evSAJEkYM2aMepmFhQW6dOmidWaqadOmQZIk9Xs/Pz9ERUUhNTUVAJCeno4//vgDf/rTnwBA41fxRx99FDdu3EBaWprGNp944gmd6r5nzx707NkT/fv311iekJAAIUSLt+7cLzc3F3/+85/h4+MDCwsLyGQy9biNCxcuNPr5HTt2oLq6Gk8//bTGMbGyskJ0dHSdmZ0kSUJsbKzGst69e2s9J0314osv4ujRoxqvAQMGAAC+//57DBo0CHZ2durv+dVXX2l8x+3btwMA5s6d26z9N/U8jh8/vk4rVv/+/XHq1CnMmTMHO3bsQHFxsU77vn79OgDA3d1d63pfX1+EhYVh8+bNWruA3U+1jWvXrum0byJqm5hYEFGH8OSTT6JPnz5YvHixuruNvpydnTXeW1pawsbGBlZWVnWWl5eX1/m8QqHQuiw/Px8AcPPmTQDAwoULIZPJNF5z5swBAI1xCQB07qaUn5+vtayXl5d6vSEolUqMHDkSycnJePXVV/Hrr7/iyJEjOHToEIDaAeeNUR2Xfv361TkuSUlJdY6JtnMil8u1npOm8vb2Rt++fTVe9vb2SE5OxpQpU9C5c2d8++23OHjwII4ePYoZM2Zo7DcvLw/m5uZaY0EXTT2P2sq+8cYb+OCDD3Do0CGMGTMGLi4uGDFiBI4dO9bgvlXn6sFjq2Jvb489e/agV69emDx5MrZu3VrvtlTb0OX8E1HbxelmiahDkCQJ77//PmJiYvD555/XWa+6sXlwsLOhbrABICcnR+sy1ZSbrq6uAGpv/CZOnKh1Gw8Omr2/BaQhLi4uuHHjRp3lql+hVftuaWfPnsWpU6ewdu1axMfHq5enp6frvA1V3TZv3txmZ6j69ttvERAQgKSkJI1z8mB8ubm5oaamBjk5Oc0au9LU86gtPiwsLLBgwQIsWLAAhYWF2L17NxYtWoRRo0YhOzsbNjY2Wvet2nZBQUG99XN2dsbu3bsRExODKVOmYOPGjVpjWbUNQ8UdEbUOtlgQUYfxyCOPICYmBu+88w5KS0s11nl4eMDKygqnT5/WWN5YFw59fPfddxqz4GRlZeHAgQPq5ygEBweja9euOHXqVJ1fxe//dbw5RowYgfPnz+P48eMay9etWwdJkhockKsP1Y2tXC7XWK5tqlFVmQd/xR41ahQsLCyQkZFR73Fpqvr21VySJMHS0lLjRj4nJ6dOPKm6za1evbrR+mmrW0ufR0dHR0yaNAlz585FQUFBgzOA9ejRA0DtYPyGqJKL3r17Iy4uDj/88EOdMqoJC3r27Nmk+hJR28IWCyLqUN5//3089NBDyM3NRa9evdTLJUnC9OnT8fXXXyMoKAhhYWE4cuQINmzYYLC65Obm4vHHH8fs2bNRVFSEJUuWwMrKCm+88Ya6zGeffYYxY8Zg1KhRSEhIQOfOnVFQUIALFy7g+PHj+P7775u17/nz52PdunUYO3Ys3nnnHfj5+WHbtm345JNP8Nxzz6Fbt27N/l45OTlanyru7++PsLAwBAUF4fXXX4cQAs7Ozvjpp5+wa9euOuVDQ0MBAB999BHi4+Mhk8kQHBwMf39/vPPOO1i8eDEuXbqE0aNHw8nJCTdv3sSRI0dga2urftidruzt7eHn54eUlBSMGDECzs7OcHV11ZjWtSnGjRuH5ORkzJkzB5MmTUJ2djbeffddeHp64uLFi+pyQ4YMwVNPPYVly5bh5s2bGDduHORyOU6cOAEbGxv1zFihoaHYuHEjkpKSEBgYCCsrK4SGhrbIeYyNjUVISAj69u0LNzc3ZGVlYdWqVfDz80PXrl3r/Zy3tzcCAwNx6NAhvPDCCw3uw8nJSd1y8eSTT2LDhg2YPHmyev2hQ4dgbm6OoUOHNlpfImrDjDt2nIjIMO6fFepB06ZNEwA0ZoUSQoiioiIxa9Ys4eHhIWxtbUVsbKzIzMysd1aovLw8jc/Hx8cLW1vbOvt7cAYq1axQ69evFy+88IJwc3MTcrlcDBkyRBw7dqzO50+dOiWmTJki3N3dhUwmEwqFQjz88MPi008/1en71icrK0tMmzZNuLi4CJlMJoKDg8WKFSvUM02pNHVWKNQzW1J8fLwQQojz58+LmJgYYW9vL5ycnMTkyZPFlStXtM7K9MYbbwgvLy9hZmYmAIjU1FT1uq1bt4rhw4eLTp06CblcLvz8/MSkSZPE7t271WXqOyeqc3i/3bt3i4iICCGXyzXqq41qlqUVK1bUW+avf/2r8Pf3F3K5XPTo0UN88cUXWvdbU1MjPvzwQxESEiIsLS2Fg4ODiIyMFD/99JO6TGZmphg5cqSwt7cXAISfn596nS7nsaH6rly5UkRFRQlXV1dhaWkpfH19xcyZM0VmZma9303lrbfeEk5OTqK8vFxj+YMxr1JYWCj69+8vLCwsRFJSknr5kCFDRGxsbKP7I6K2TRKCT6MhIiKiprt+/ToCAgKwbt06xMXFNWsbGRkZ6Nq1K3bs2IGYmJgWriERtSYmFkRERNRsr732GrZv346TJ0/CzKzpQzefeeYZXL16VWt3OCIyLRxjQURERM325ptvwsbGBteuXYOPj0+TPltdXY2goCCNcUVEZLrYYkFERERERHrjdLNERERERKQ3JhZERERERKQ3JhZERERERKQ3JhZERERERKQ3JhZERERERKQ3JhZERERERKQ3JhZERERERKQ3JhZERERERKS3/wdd2WVx7lW0jAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0.7731925718353914\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Ks, Final_RMSEs, marker='o', linewidth=2, markersize=6)\n",
    "plt.title(\n",
    "    \"Effect of Latent Dimension on ALS Model Performance\", fontsize=14\n",
    ")\n",
    "plt.xlabel(\"Number of Latent Factors (K)\", fontsize=12)\n",
    "plt.ylabel(\"Final RMSE on Validation Set\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.argmin(Final_RMSEs))\n",
    "print(np.min(Final_RMSEs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04897455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gmedw\\AppData\\Local\\Temp\\ipykernel_24872\\1604762600.py:93: RuntimeWarning: divide by zero encountered in divide\n",
      "  scaling = np.where(F_n > 0, 1.0 / np.sqrt(F_n), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, NLL = 161672.7987, Train RMSE = 0.8335, Test RMSE = 0.8717\n"
     ]
    }
   ],
   "source": [
    "lam = 0.017684422019387584\n",
    "gamma = 0.06224262766516858\n",
    "tau = 0.29037492246228525\n",
    "\n",
    "k = 15\n",
    "\n",
    "user_bias = np.zeros((n_users))\n",
    "item_bias = np.zeros((n_items))\n",
    "user_embedding = np.random.normal(scale=0.1, size=(n_users, k))\n",
    "item_embedding = np.random.normal(scale=0.1, size=(n_items, k))\n",
    "feature_embedding = np.random.normal(scale=0.11, size=(n_features, k))\n",
    "mu_train = train_data[:, 2].mean()\n",
    "\n",
    "nll = 0\n",
    "nll_history = []\n",
    "train_rmse_history = []\n",
    "test_rmse_history = []\n",
    "iterations = 3\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # user bias update\n",
    "    user_bias = Parallel(n_jobs=-1)(\n",
    "        delayed(update_user_bias)(\n",
    "            user,\n",
    "            mu_train,\n",
    "            data_by_user_train,\n",
    "            lam,\n",
    "            gamma,\n",
    "            tau,\n",
    "            item_bias,\n",
    "            user_bias,\n",
    "            item_embedding,\n",
    "            user_embedding,\n",
    "        )\n",
    "        for user in range(n_users)\n",
    "    )\n",
    "    user_bias = np.array(user_bias)\n",
    "\n",
    "    # user embedding update\n",
    "    user_embedding = Parallel(n_jobs=-1)(\n",
    "        delayed(update_user_embedding)(\n",
    "            user,\n",
    "            mu_train,\n",
    "            data_by_user_train,\n",
    "            lam,\n",
    "            gamma,\n",
    "            tau,\n",
    "            item_bias,\n",
    "            user_bias,\n",
    "            item_embedding,\n",
    "            user_embedding,\n",
    "        )\n",
    "        for user in range(n_users)\n",
    "    )\n",
    "    user_embedding = np.vstack(user_embedding)\n",
    "\n",
    "    # item bias update\n",
    "    item_bias = Parallel(n_jobs=-1)(\n",
    "        delayed(update_item_bias)(\n",
    "            item,\n",
    "            mu_train,\n",
    "            data_by_movie_train,\n",
    "            lam,\n",
    "            gamma,\n",
    "            tau,\n",
    "            item_bias,\n",
    "            user_bias,\n",
    "            item_embedding,\n",
    "            user_embedding,\n",
    "        )\n",
    "        for item in range(n_items)\n",
    "    )\n",
    "    item_bias = np.array(item_bias)\n",
    "\n",
    "    # item embedding update\n",
    "    item_embedding = Parallel(n_jobs=-1)(\n",
    "        delayed(update_item_embedding)(\n",
    "            item,\n",
    "            mu_train,\n",
    "            data_by_movie_train,\n",
    "            lam,\n",
    "            gamma,\n",
    "            tau,\n",
    "            item_bias,\n",
    "            user_bias,\n",
    "            item_embedding,\n",
    "            user_embedding,\n",
    "            item_features,\n",
    "            feature_embedding,\n",
    "        )\n",
    "        for item in range(n_items)\n",
    "    )\n",
    "    item_embedding = np.vstack(item_embedding)\n",
    "\n",
    "    # feature embedding update\n",
    "    feature_embedding = update_feature_embedding(\n",
    "        item_features, item_embedding, tau, lam=lam\n",
    "    )\n",
    "\n",
    "    # compute train RMSE\n",
    "    uv_train = np.sum(\n",
    "        user_embedding[train_coo.row] * item_embedding[train_coo.col], axis=1\n",
    "    )\n",
    "    pred_train = user_bias[train_coo.row] + item_bias[train_coo.col] + uv_train + mu_train\n",
    "    diff_train = train_coo.data - pred_train\n",
    "    squared_error = np.sum(diff_train**2)\n",
    "\n",
    "    reg_user_bias = np.sum(user_bias**2)\n",
    "    reg_item_bias = np.sum(item_bias**2)\n",
    "    reg_user_emb = np.sum(user_embedding**2)\n",
    "\n",
    "    reg_item_feature = 0.0\n",
    "    for n in range(n_items):\n",
    "        feats = item_features[n]\n",
    "        F_n = np.sum(feats)\n",
    "        s_n = np.zeros(k) if F_n == 0 else (feats @ feature_embedding) / np.sqrt(F_n)\n",
    "        reg_item_feature += np.sum((item_embedding[n] - s_n) ** 2)\n",
    "\n",
    "    reg_feature_emb = np.sum(feature_embedding**2)\n",
    "\n",
    "    nll = (\n",
    "        (lam / 2) * squared_error\n",
    "        + (gamma / 2) * (reg_user_bias + reg_item_bias)\n",
    "        + (tau / 2) * (reg_user_emb + reg_item_feature + reg_feature_emb)\n",
    "    )\n",
    "    nll_history.append(nll)\n",
    "\n",
    "    train_rmse = np.sqrt(squared_error / len(train_coo.data))\n",
    "    train_rmse_history.append(train_rmse)\n",
    "\n",
    "    # compute test RMSE\n",
    "    uv_test = np.sum(user_embedding[test_coo.row] * item_embedding[test_coo.col], axis=1)\n",
    "    pred_test = user_bias[test_coo.row] + item_bias[test_coo.col] + uv_test + mu_train\n",
    "    diff_test = test_coo.data - pred_test\n",
    "    test_rmse = np.sqrt(np.sum(diff_test**2) / len(test_coo.data))\n",
    "    test_rmse_history.append(test_rmse)\n",
    "\n",
    "    print(\n",
    "        f\"Iteration {iteration + 1}, NLL = {nll:.4f}, \"\n",
    "        f\"Train RMSE = {train_rmse:.4f}, Test RMSE = {test_rmse:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6471246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"all_data.npz\",\n",
    "#          user_bias=user_bias,\n",
    "#          item_bias=item_bias,\n",
    "#          user_embedding=user_embedding,\n",
    "#          item_embedding=item_embedding,\n",
    "#          feature_embedding=feature_embedding,\n",
    "#          nll_history=nll_history,\n",
    "#          train_rmse_history=train_rmse_history,\n",
    "#          test_rmse_history=test_rmse_history,\n",
    "#          item_id_map=item_id_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgmpy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
